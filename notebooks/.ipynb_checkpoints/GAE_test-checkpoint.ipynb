{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f465f3",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355a3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68bd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json \n",
    "import os \n",
    "import sys \n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191c5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/tromanski/thesis/')\n",
    "sys.path.append('/home/tromanski/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8011a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from XAI_Transformers_.SST.sst import get_sst_dataset\n",
    "from XAI_Transformers_.xai_transformer import BertAttention\n",
    "from XAI_Transformers_.attribution import _compute_rollout_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26af29",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "155e38ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/tromanski/.cache/huggingface/hub/models--textattack--bert-base-uncased-SST-2/snapshots/95f0f6f859b35c8ff0863ae3cd4e2dbc702c0ae2/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Attempting to create safetensors variant\n",
      "Safetensors PR exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202ca51b36a94eefb7b4f2ae7530b55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file model.safetensors from cache at /home/tromanski/.cache/huggingface/hub/models--textattack--bert-base-uncased-SST-2/snapshots/205ffbd1bc5c5b89802266f4948a601f53556b00/model.safetensors\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at textattack/bert-base-uncased-SST-2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\", \n",
    "    use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fcd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11c035b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/tromanski/.cache/huggingface/hub/models--textattack--bert-base-uncased-SST-2/snapshots/95f0f6f859b35c8ff0863ae3cd4e2dbc702c0ae2/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/tromanski/.cache/huggingface/hub/models--textattack--bert-base-uncased-SST-2/snapshots/95f0f6f859b35c8ff0863ae3cd4e2dbc702c0ae2/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/tromanski/.cache/huggingface/hub/models--textattack--bert-base-uncased-SST-2/snapshots/95f0f6f859b35c8ff0863ae3cd4e2dbc702c0ae2/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/tromanski/.cache/huggingface/hub/models--textattack--bert-base-uncased-SST-2/snapshots/95f0f6f859b35c8ff0863ae3cd4e2dbc702c0ae2/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /home/tromanski/.cache/huggingface/hub/models--textattack--bert-base-uncased-SST-2/snapshots/95f0f6f859b35c8ff0863ae3cd4e2dbc702c0ae2/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/tromanski/.cache/huggingface/hub/models--textattack--bert-base-uncased-SST-2/snapshots/95f0f6f859b35c8ff0863ae3cd4e2dbc702c0ae2/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "\n",
    "# Load data\n",
    "datasets  = load_dataset(\"glue\", 'sst2')\n",
    "\n",
    "\n",
    "label_to_id = {v: i for i, v in enumerate([0,1])}\n",
    "\n",
    "_, test_data_loader = get_sst_dataset(datasets, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455f143",
   "metadata": {},
   "source": [
    "## Using OG Transformers paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b25704d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "# complete code to take in one sample (x) and get attributions\n",
    "models = {}\n",
    "gammas = [0.00,0.00, 0.00]\n",
    "bert_model.bert.embeddings.requires_grad = False\n",
    "for name, param in bert_model.named_parameters():                \n",
    "    if name.startswith('embeddings'):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "pretrained_embeds = bert_model.bert.embeddings\n",
    "\n",
    "params = torch.load('/home/tromanski/XAI_Transformers_/SST/sst2-3layer-model.pt', map_location=torch.device(device))\n",
    "\n",
    "def rename_params(key):\n",
    "    for k_ in ['key','query', 'value']:\n",
    "        key=key.replace(k_, 'p'+k_)\n",
    "    return key\n",
    "\n",
    "# this is the standard way of their config for best performing method \n",
    "class Config(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.hidden_size = 768\n",
    "        self.num_attention_heads = 12\n",
    "        self.layer_norm_eps = 1e-12\n",
    "        self.n_classes = 2\n",
    "        self.n_blocks = 3\n",
    "                    \n",
    "        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        \n",
    "        self.detach_layernorm = True # Detaches the attention-block-output LayerNorm\n",
    "        self.detach_kq = True # Detaches the kq-softmax branch\n",
    "        self.device = device\n",
    "        self.train_mode = False\n",
    "        self.detach_mean = True #\n",
    "\n",
    "config = Config() # none need to be detached for GAE \n",
    "config.detach_layernorm = False # Detaches the attention-block-output LayerNorm\n",
    "config.detach_mean = False # Detaches the attention-block-output LayerNorm\n",
    "config.detach_kq = False\n",
    "model = BertAttention(config, pretrained_embeds)\n",
    "# params = load_file('/home/tromanski/thesis/results/custom-bert/checkpoint-2105/model.safetensors', device=str(device))\n",
    "model.load_state_dict(params, strict=False)\n",
    "model.to(device)\n",
    "models['none'] = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160ec9e",
   "metadata": {},
   "source": [
    "### Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "154a0502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/872 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving attention gradients\n",
      "Saving attention gradients\n",
      "Saving attention gradients\n",
      "attentions_mat shape: torch.Size([3, 1, 12, 28, 28])\n",
      "attentions_grads shape: torch.Size([3, 1, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 1, 28, 28])\n",
      "joint_attentions shape: torch.Size([1, 28, 28])\n",
      "joint_attentions after zeroing CLS-to-CLS shape: torch.Size([1, 28, 28])\n",
      "joint_attentions after zeroing CLS-to-CLS shape: torch.Size([2, 28, 28])\n",
      "attribution shape: torch.Size([2, 28])\n",
      "tensor([[0.0083, 1.0023, 1.0081, 1.0121, 1.0034, 1.0085, 1.0153, 0.9970, 1.0164,\n",
      "         1.0129, 1.0107, 1.0105, 0.9967, 0.9881, 1.0724, 1.0293, 1.0093, 0.9989,\n",
      "         1.0108, 1.0072, 1.0074, 1.0221, 0.9954, 0.9950, 0.9916, 1.0036, 1.0057,\n",
      "         1.0014],\n",
      "        [0.0643, 1.0583, 1.0641, 1.0681, 1.0594, 1.0645, 1.0713, 1.0530, 1.0724,\n",
      "         1.0689, 1.0667, 1.0665, 1.0527, 1.0441, 1.1284, 1.0853, 1.0653, 1.0549,\n",
      "         1.0668, 1.0632, 1.0634, 1.0781, 1.0514, 1.0510, 1.0476, 1.0596, 1.0617,\n",
      "         1.0574]], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# from utils.generic import load_custom_bert\n",
    "\n",
    "\n",
    "# print('1')\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = load_custom_bert(device=device, finetuned=True, explain=True, train=False)\n",
    "# print('2')\n",
    "x = test_data_loader[3]\n",
    "results = {'losses':[], 'attributions':[]}\n",
    "# model.explain()\n",
    "import tqdm\n",
    "for y in tqdm.tqdm(test_data_loader):\n",
    "    input_ids = torch.tensor(np.float32(x['input_ids']) , requires_grad=True).unsqueeze(0).long().to(device)\n",
    "    attention_mask = torch.tensor(np.float32(x['attention_mask']), requires_grad=True).unsqueeze(0).long().to(device)\n",
    "    token_type_ids = torch.tensor(np.float32(x['token_type_ids'])).unsqueeze(0).long().to(device)\n",
    "    words = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "    y_true = torch.tensor(x['label']).to(device)\n",
    "    labels_in = torch.tensor([int(y_true)]*len(input_ids)).long().to(device)\n",
    "    # print('1')\n",
    "    outs = model(input_ids=input_ids,\n",
    "                                labels = labels_in)\n",
    "    # print('2')\n",
    "    loss = outs['loss'].detach().cpu().numpy()\n",
    "    y_pred = np.argmax(outs['logits'].squeeze().detach().cpu().numpy())\n",
    "\n",
    "    gammas = [0.00,0.00, 0.00]\n",
    "    # print('3')\n",
    "    outs = model.forward_and_explain_legacy(input_ids=input_ids, cl=y_true,\n",
    "                    labels = labels_in , method='GAE')\n",
    "    if og_method:\n",
    "        attns = [model.attention_probs[k].detach().cpu().numpy() for k in sorted(model.attention_probs.keys())]\n",
    "        attentions_mat = np.stack(attns,axis=0).squeeze() # L, B, H, T, T\n",
    "        print('attentions_mat shape:', attentions_mat.shape)\n",
    "        attns = [model.attention_gradients[k].detach().cpu().numpy() for k in sorted(model.attention_gradients.keys())]\n",
    "        attentions_grads = np.stack(attns,axis=0).squeeze()\n",
    "        print('attentions_grads shape:', attentions_grads.shape)\n",
    "        attentions_mat = torch.tensor(attentions_mat * attentions_grads).clamp(min=0)\n",
    "        print('attentions_mat after grad mul shape:', attentions_mat.shape)\n",
    "        attentions_mat = torch.tensor(attentions_mat).clamp(min=0).mean(dim=1)\n",
    "        print('attentions_mat after mean over heads shape:', attentions_mat.shape)\n",
    "        joint_attentions = _compute_rollout_attention(attentions_mat)\n",
    "        print('joint_attentions shape:', joint_attentions.shape)\n",
    "        joint_attentions[:, 0, 0] = 0\n",
    "        idx = 0\n",
    "        attribution = joint_attentions[idx].sum(0)\n",
    "        print('attribution shape:', attribution.shape)\n",
    "    else:\n",
    "    # print('4')\n",
    "#same as in run sst but then fully torch \n",
    "    # L layers, B batch size, H heads, T tokens\n",
    "        attns = [model.attention_probs[k] for k in sorted(model.attention_probs.keys())]\n",
    "        # print(torch.stack(attns, dim=0).shape)\n",
    "        attentions_mat = torch.stack(attns, dim=0) # should be (L, B, H, T, T)\n",
    "        bs = attentions_mat.shape[1]\n",
    "        print('attentions_mat shape:', attentions_mat.shape)\n",
    "\n",
    "        grads = [model.attention_gradients[k] for k in sorted(model.attention_gradients.keys())]\n",
    "        attentions_grads = torch.stack(grads, dim=0)  # should be (L, B, H, T, T)\n",
    "        if len(attentions_grads.shape) != 5 and bs ==1:\n",
    "            attentions_grads = attentions_grads.unsqueeze(1)\n",
    "\n",
    "        print('attentions_grads shape:', attentions_grads.shape)\n",
    "\n",
    "        # 2) Compute your attention * grad, staying entirely in torch\n",
    "        attentions_mat = (attentions_mat * attentions_grads).clamp(min=0)\n",
    "        attentions_mat = attentions_mat.clamp(min=0).mean(dim=2) # should be (L, B, T, T)\n",
    "        print('attentions_mat after mean over heads shape:', attentions_mat.shape)\n",
    "\n",
    "        # 3) rollout must be pure torch ops\n",
    "        attentions_mat = [attentions_mat[l] for l in range(attentions_mat.size(0))]  # list of (B, T, T) and length L\n",
    "        joint_attentions = _compute_rollout_attention(attentions_mat)  # must return a Tensor from torch ops only\n",
    "        print('joint_attentions shape:', joint_attentions.shape)\n",
    "        # 4) Avoid accidentally disconnecting the graph; in-place is OK on non-leaf\n",
    "        joint_attentions = joint_attentions.clone()\n",
    "        joint_attentions[:, 0, 0] = 0\n",
    "        print('joint_attentions after zeroing CLS-to-CLS shape:', joint_attentions.shape) \n",
    "        joint_attentions = torch.stack([joint_attentions, joint_attentions+0.002], dim = 0).squeeze()\n",
    "        print('joint_attentions after zeroing CLS-to-CLS shape:', joint_attentions.shape)\n",
    "\n",
    "        idx = 0\n",
    "        attribution = joint_attentions.sum(1, keepdim=False)  # still differentiable # i think this method is now correct - it's just that I am not sure about whether I actually select the CLS token here now, prev they just selected the first one but more because their func couldnt handle batched inputs \n",
    "        # if attribution.dim() == 1:\n",
    "        #     attribution = attribution.unsqueeze(0)  # make sure attribution is (1, T) for consistency\n",
    "        print('attribution shape:', attribution.shape)\n",
    "\n",
    "    results['attributions'].append(attribution.tolist())\n",
    "    results['losses'].append(loss.tolist())\n",
    "    print(attribution)\n",
    "    break \n",
    "# attribution = outs['R'].squeeze()\n",
    "#     results['attributions'].append(attribution)\n",
    "#     results['losses'].append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2de23a",
   "metadata": {},
   "source": [
    "new method with list: \n",
    "attentions_mat shape: torch.Size([3, 12, 12, 12])\n",
    "attentions_grads shape: torch.Size([3, 12, 12, 12])\n",
    "attentions_mat after mean over heads shape: torch.Size([3, 12, 12])\n",
    "joint_attentions shape: torch.Size([12, 12, 12])\n",
    "joint_attentions after zeroing CLS-to-CLS shape: torch.Size([12, 12, 12])\n",
    "attribution shape: torch.Size([12])\n",
    "\n",
    "old method: \n",
    "attentions_mat shape: (3, 12, 12, 12)\n",
    "attentions_grads shape: (3, 12, 12, 12)\n",
    "attentions_mat after grad mul shape: torch.Size([3, 12, 12, 12])\n",
    "attentions_mat after mean over heads shape: torch.Size([3, 12, 12])\n",
    "joint_attentions shape: torch.Size([12, 12, 12]) # this is because of faulty expansion as it expects bs in dim 2 \n",
    "-- they both produce same output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c09ade37",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_method = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fe2ac27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12, 12])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1967f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/tromanski/thesis/data/GAE_attr/OG_val_bert_sst.json', 'w') as f:\n",
    "#     json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1916dda3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0083, 1.0023, 1.0081, 1.0121, 1.0034, 1.0085, 1.0153, 0.9970, 1.0164,\n",
       "        1.0129, 1.0107, 1.0105, 0.9967, 0.9881, 1.0724, 1.0293, 1.0093, 0.9989,\n",
       "        1.0108, 1.0072, 1.0074, 1.0221, 0.9954, 0.9950, 0.9916, 1.0036, 1.0057,\n",
       "        1.0014], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution # for fourth sample which is 28 tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82b99827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0094, 1.0098, 1.0037, 1.0117, 1.0120, 1.0743, 1.0155, 1.0359, 1.0290,\n",
       "        1.0241, 1.0095, 1.0017])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a86cfa3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0094, 1.0098, 1.0037, 1.0117, 1.0120, 1.0743, 1.0155, 1.0359, 1.0290,\n",
       "        1.0241, 1.0095, 1.0017], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "412db2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0094, 1.0098, 1.0037, 1.0117, 1.0120, 1.0743, 1.0155, 1.0359, 1.0290,\n",
       "        1.0241, 1.0095, 1.0017], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a530d",
   "metadata": {},
   "source": [
    "## Using my impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bb87a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'env (Python 3.11.13)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a120eb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/872 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils.generic import load_custom_bert\n",
    "\n",
    "\n",
    "print('1')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_custom_bert(device=device, finetuned=True, explain=False, train=True)\n",
    "model.train(False) # this is important!! why? dropout etc I assume \n",
    "print('2')\n",
    "x = test_data_loader[0]\n",
    "results = {'losses':[], 'attributions':[]}\n",
    "# model.explain()\n",
    "import tqdm\n",
    "for x in tqdm.tqdm(test_data_loader):\n",
    "    input_ids = torch.tensor(np.float32(x['input_ids']) , requires_grad=True).unsqueeze(0).long().to(device)\n",
    "    attention_mask = torch.tensor(np.float32(x['attention_mask']), requires_grad=True).unsqueeze(0).long().to(device)\n",
    "    token_type_ids = torch.tensor(np.float32(x['token_type_ids'])).unsqueeze(0).long().to(device)\n",
    "    words = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "    y_true = torch.tensor(x['label']).to(device)\n",
    "    labels_in = torch.tensor([int(y_true)]*len(input_ids)).long().to(device)\n",
    "    # print('1')\n",
    "    outs = model(input_ids=input_ids,\n",
    "                                labels = labels_in)\n",
    "    # print('2')\n",
    "    loss = outs['loss'].detach().cpu().numpy()\n",
    "    y_pred = np.argmax(outs['logits'].squeeze().detach().cpu().numpy())\n",
    "\n",
    "    gammas = [0.00,0.00, 0.00]\n",
    "    # print('3')\n",
    "    outs = model.forward_and_explain(input_ids=input_ids, cl=y_true,\n",
    "                    labels = labels_in , method='GAE')\n",
    "    # print('4')\n",
    "\n",
    "    # attns = [model.attention_probs[k] for k in sorted(model.attention_probs.keys())]\n",
    "    # attentions_mat = torch.stack(attns, dim=0).squeeze()\n",
    "\n",
    "    # grads = [model.attention_gradients[k] for k in sorted(model.attention_gradients.keys())]\n",
    "    # attentions_grads = torch.stack(grads, dim=0).squeeze()\n",
    "\n",
    "    # # 2) Compute your attention * grad, staying entirely in torch\n",
    "    # attentions_mat = (attentions_mat * attentions_grads).clamp(min=0)\n",
    "    # attentions_mat = attentions_mat.clamp(min=0).mean(dim=1)\n",
    "\n",
    "    # # 3) rollout must be pure torch ops\n",
    "    # joint_attentions = _compute_rollout_attention(attentions_mat)  # must return a Tensor from torch ops only\n",
    "\n",
    "    # # 4) Avoid accidentally disconnecting the graph; in-place is OK on non-leaf\n",
    "    # joint_attentions = joint_attentions.clone()\n",
    "    # joint_attentions[:, 0, 0] = 0\n",
    "\n",
    "    # idx = 0\n",
    "    # attribution = joint_attentions[idx].sum(0)  # still differentiable\n",
    "    # results['attributions'].append(attribution.tolist())\n",
    "    # results['losses'].append(loss.tolist())\n",
    "    results['attributions'].append(outs['R'].tolist())\n",
    "    results['losses'].append(outs['loss'].tolist())\n",
    "    break\n",
    "# attribution = outs['R'].squeeze()\n",
    "#     results['attributions'].append(attribution)\n",
    "#     results['losses'].append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad6911fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0094, 1.0098, 1.0037, 1.0117, 1.0120, 1.0743, 1.0155, 1.0359, 1.0290,\n",
       "        1.0241, 1.0095, 1.0017], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da3601d",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aefa85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/tromanski/thesis/data/GAE_attr/OG_val_bert_sst.json', 'r') as f:\n",
    "    results_og = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "010782ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(results_og) = 872, len(results) = 872\n",
      "All attributions match (within tolerance).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_atts = results_og.get('attributions', [])\n",
    "new_atts = results.get('attributions', [])\n",
    "\n",
    "print(f\"len(results_og) = {len(og_atts)}, len(results) = {len(new_atts)}\")\n",
    "\n",
    "mismatches = []\n",
    "for i, (a_og, a_new) in enumerate(zip(og_atts, new_atts)):\n",
    "    a_og = np.asarray(a_og, dtype=float)\n",
    "    a_new = np.asarray(a_new, dtype=float)\n",
    "    if a_og.shape != a_new.shape or not np.allclose(a_og, a_new, rtol=1e-5, atol=1e-8):\n",
    "        maxdiff = float(np.max(np.abs(a_og - a_new))) if a_og.shape == a_new.shape else None\n",
    "        mismatches.append((i, a_og.shape, a_new.shape, maxdiff))\n",
    "        if len(mismatches) <= 10:\n",
    "            print(f\"Mismatch idx={i} shapes={a_og.shape} vs {a_new.shape} maxdiff={maxdiff}\")\n",
    "\n",
    "if len(og_atts) != len(new_atts):\n",
    "    print(\"Different number of attribution entries. Extra indices in results_og:\",\n",
    "          list(range(len(new_atts), len(og_atts))) if len(og_atts) > len(new_atts) else [])\n",
    "    print(\"Extra indices in results:\",\n",
    "          list(range(len(og_atts), len(new_atts))) if len(new_atts) > len(og_atts) else [])\n",
    "\n",
    "if not mismatches:\n",
    "    print(\"All attributions match (within tolerance).\")\n",
    "else:\n",
    "    print(f\"Total mismatches: {len(mismatches)} (showing up to 10).\")\n",
    "\n",
    "# keep mismatches for inspection\n",
    "mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generic import load_custom_bert\n",
    "\n",
    "\n",
    "print('1')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_custom_bert(device=device, finetuned=True, explain=True, train=False)\n",
    "print('2')\n",
    "x = test_data_loader[0]\n",
    "results = {'losses':[], 'attributions':[]}\n",
    "model.explain()\n",
    "# for x in test_data_loader:\n",
    "    \n",
    "\n",
    "input_ids = torch.tensor(np.float32(x['input_ids']) , requires_grad=True).unsqueeze(0).long().to(device)\n",
    "attention_mask = torch.tensor(np.float32(x['attention_mask']), requires_grad=True).unsqueeze(0).long().to(device)\n",
    "token_type_ids = torch.tensor(np.float32(x['token_type_ids'])).unsqueeze(0).long().to(device)\n",
    "words = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "y_true = torch.tensor(x['label']).to(device)\n",
    "labels_in = torch.tensor([int(y_true)]*len(input_ids)).long().to(device)\n",
    "print('1')\n",
    "outs = model(input_ids=input_ids,\n",
    "                            labels = labels_in)\n",
    "print('2')\n",
    "loss = outs['loss'].detach().cpu().numpy()\n",
    "y_pred = np.argmax(outs['logits'].squeeze().detach().cpu().numpy())\n",
    "\n",
    "gammas = [0.00,0.00, 0.00]\n",
    "print('3')\n",
    "outs = model.forward_and_explain(input_ids=input_ids, cl=y_true,\n",
    "                            labels = labels_in, \n",
    "                                gammas = gammas)\n",
    "print('4')\n",
    "\n",
    "\n",
    "attribution = outs['R'].squeeze()\n",
    "#     results['attributions'].append(attribution)\n",
    "#     results['losses'].append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "attns = [model.attention_probs[k].detach().cpu().numpy() for k in sorted(model.attention_probs.keys())]\n",
    "attentions_mat = np.stack(attns,axis=0).squeeze()\n",
    "attns = [model.attention_gradients[k].detach().cpu().numpy() for k in sorted(model.attention_gradients.keys())]\n",
    "attentions_grads = np.stack(attns,axis=0).squeeze()\n",
    "attentions_mat = torch.tensor(attentions_mat * attentions_grads).clamp(min=0)\n",
    "attentions_mat = torch.tensor(attentions_mat).clamp(min=0).mean(dim=1)\n",
    "joint_attentions = _compute_rollout_attention(attentions_mat)\n",
    "joint_attentions[:, 0, 0] = 0\n",
    "idx = 0\n",
    "attribution = joint_attentions[idx].sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7b1ca",
   "metadata": {},
   "source": [
    "## Random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cae55672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting attack.sh on cn84 at Tue Dec  9 04:17:08 PM CET 2025\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "/vol/csedu-nobackup/project/tromanski/conda-envs/env/lib/python3.11/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Model name: custom-bert-finetuned\n",
      "Tokenizer name: google-bert/bert-base-uncased\n",
      "Dataset: sst2\n",
      "Approach: location\n",
      "Position target: 1\n",
      "Loss function: rank\n",
      "Lambda: 10.0\n",
      "Epochs: 4\n",
      "Project dir: /home/tromanski/thesis\n",
      "Model dir: /vol/csedu-nobackup/project/tromanski\n",
      "Eval only: True\n",
      "Subsample size: None\n",
      "Get attributions: True\n",
      "Learning rate: 1e-05\n",
      "Optimizer: adamw\n",
      "Scheduler type: linear\n",
      "Warmup percent: 0.1\n",
      "Batch size: 32\n",
      "Seed: 42\n",
      "loading configuration file config.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
      "* `cls.predictions.transform.LayerNorm.beta` -> `cls.predictions.transform.LayerNorm.bias`\n",
      "* `cls.predictions.transform.LayerNorm.gamma` -> `cls.predictions.transform.LayerNorm.weight`\n",
      "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "/vol/csedu-nobackup/project/tromanski/conda-envs/env/lib/python3.11/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\n",
      "Optimiser set to AdamW with lr: 1e-05 betas: (0.9, 0.999) eps: 1e-08\n",
      "Optimiser set to adamw with lr: 1e-05 betas (if used): (0.9, 0.999) eps (if used): 1e-08\n",
      "Scheduler type: linear , warmup steps:  841 , total training steps:  8416\n",
      "/home/tromanski/thesis/utils/attack.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AttackTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "The following columns in the Training set don't have a corresponding argument in `BertAttention.forward` and have been ignored: idx. If idx are not expected by `BertAttention.forward`,  you can safely ignore this message.\n",
      "attentions_mat shape: torch.Size([3, 1, 12, 12, 12])\n",
      "attentions_grads shape: torch.Size([3, 1, 12, 12, 12])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 1, 12, 12])\n",
      "joint_attentions shape: torch.Size([1, 12, 12])\n",
      "joint_attentions after zeroing CLS-to-CLS shape: torch.Size([1, 12, 12])\n",
      "attribution shape: torch.Size([12])\n",
      "explanation:\n",
      "tensor([0.0000, 1.0088, 1.0042, 1.0048, 1.0049, 1.0689, 1.0141, 1.0522, 1.0467,\n",
      "        1.0432, 1.0029, 1.0012], grad_fn=<SumBackward1>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tromanski/thesis/scripts/run_attack.py\", line 350, in <module>\n",
      "    main()\n",
      "  File \"/home/tromanski/thesis/scripts/run_attack.py\", line 299, in main\n",
      "    eval_results, val_expl = eval_attack(dataloader=dataloader,\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tromanski/thesis/utils/attack.py\", line 157, in eval_attack\n",
      "    sum_sample += F.mse_loss(expl[sample, sample_mask], tgt[sample, sample_mask], reduction='mean').item()\n",
      "                             ~~~~^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: too many indices for tensor of dimension 1\n"
     ]
    }
   ],
   "source": [
    "!jobs/single_attack.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a2acaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting attack.sh on cn84 at Tue Dec  9 03:57:36 PM CET 2025\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "/vol/csedu-nobackup/project/tromanski/conda-envs/env/lib/python3.11/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Model name: custom-bert-finetuned\n",
      "Tokenizer name: google-bert/bert-base-uncased\n",
      "Dataset: sst2\n",
      "Approach: location\n",
      "Position target: 1\n",
      "Loss function: rank\n",
      "Lambda: 10.0\n",
      "Epochs: 4\n",
      "Project dir: /home/tromanski/thesis\n",
      "Model dir: /vol/csedu-nobackup/project/tromanski\n",
      "Eval only: True\n",
      "Subsample size: None\n",
      "Get attributions: True\n",
      "Learning rate: 1e-05\n",
      "Optimizer: adamw\n",
      "Scheduler type: linear\n",
      "Warmup percent: 0.1\n",
      "Batch size: 32\n",
      "Seed: 42\n",
      "loading configuration file config.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
      "A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
      "* `cls.predictions.transform.LayerNorm.beta` -> `cls.predictions.transform.LayerNorm.bias`\n",
      "* `cls.predictions.transform.LayerNorm.gamma` -> `cls.predictions.transform.LayerNorm.weight`\n",
      "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file config.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
      "loading file tokenizer.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at /vol/csedu-nobackup/project/tromanski/.cache/huggingface/hub/models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "/vol/csedu-nobackup/project/tromanski/conda-envs/env/lib/python3.11/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\n",
      "Optimiser set to AdamW with lr: 1e-05 betas: (0.9, 0.999) eps: 1e-08\n",
      "Optimiser set to adamw with lr: 1e-05 betas (if used): (0.9, 0.999) eps (if used): 1e-08\n",
      "Scheduler type: linear , warmup steps:  841 , total training steps:  8416\n",
      "/home/tromanski/thesis/utils/attack.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `AttackTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "The following columns in the Training set don't have a corresponding argument in `BertAttention.forward` and have been ignored: idx. If idx are not expected by `BertAttention.forward`,  you can safely ignore this message.\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 12, 12])  attentions_grads shape: torch.Size([3, 2, 12, 12, 12])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 12, 12])\n",
      "joint_attentions shape: torch.Size([2, 12, 12])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0088, 1.0042, 1.0048, 1.0049, 1.0689, 1.0141, 1.0522, 1.0467,\n",
      "         1.0432, 1.0029, 1.0012],\n",
      "        [0.0000, 1.0142, 1.0072, 1.0111, 1.0249, 1.0046, 1.0193, 1.0127, 1.0342,\n",
      "         1.0091, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0220, 1.0072, 1.0020, 1.0192, 1.0031, 1.0409, 1.0011, 1.0146,\n",
      "         1.0013, 1.0319, 1.0019, 1.0064, 1.0088, 1.0017, 1.0015, 1.0290, 1.0256,\n",
      "         1.0014, 1.0084, 1.0172, 1.0080, 1.0019, 1.0009, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0029, 1.0030, 1.0051, 1.0288, 1.0065, 1.0046, 1.0025, 1.0055,\n",
      "         1.0135, 1.0073, 1.0013, 1.0043, 1.0008, 1.0529, 1.0246, 1.0068, 1.0040,\n",
      "         1.0067, 1.0010, 1.0007, 1.0172, 1.0063, 1.0045, 1.0044, 1.0025, 1.0008,\n",
      "         1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0293, 1.0097, 1.0236, 1.0267, 1.0077, 1.0087, 1.0091, 1.0058,\n",
      "         1.0124, 1.0139, 1.0156, 1.0367, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0044, 1.0155, 1.0088, 1.0115, 1.0051, 1.0012, 1.0044, 1.0109,\n",
      "         1.0221, 1.0181, 1.0026, 1.0116, 1.0055, 1.0011, 1.0010, 1.0808, 1.0050,\n",
      "         1.0035, 1.0027, 1.0003, 1.0101, 1.0110, 1.0010, 1.0006]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 15, 15])  attentions_grads shape: torch.Size([3, 2, 12, 15, 15])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 15, 15])\n",
      "joint_attentions shape: torch.Size([2, 15, 15])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0075, 1.0153, 1.0484, 1.0101, 1.0036, 1.0271, 1.0475, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0192, 1.0307, 1.0166, 1.0081, 1.0140, 1.0154, 1.0227, 1.0057,\n",
      "         1.0067, 1.0103, 1.0199, 1.0102, 1.0085, 1.0123]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0151, 1.0085, 1.0164, 1.0018, 1.0194, 1.0006, 1.0009, 1.0038,\n",
      "         1.0013, 1.0052, 1.0011, 1.0268, 1.0110, 1.0068, 1.0011, 1.0006, 1.0406,\n",
      "         1.0391, 1.0037, 1.0010, 1.0235, 1.0129, 1.0057, 1.0009, 1.0005, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0006, 1.0021, 1.0050, 1.0022, 1.0010, 1.0009, 1.0029,\n",
      "         1.0059, 1.0172, 1.0105, 1.0066, 1.0008, 1.0028, 1.0012, 1.0057, 1.0131,\n",
      "         1.0095, 1.0035, 1.0043, 1.0002, 1.0011, 1.0011, 1.0014, 1.0018, 1.0025,\n",
      "         1.0015, 1.0026, 1.0013, 1.0009, 1.0009, 1.0008, 1.0068, 1.0032, 1.0007,\n",
      "         1.0020, 1.0023, 1.0039, 1.0297]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0030, 1.0127, 1.0062, 1.0124, 1.0100, 1.0202, 1.0010, 1.0063,\n",
      "         1.0212, 1.0225, 1.0117, 1.0017, 1.0225, 1.0106, 1.0259, 1.0080, 1.0191,\n",
      "         1.0017, 1.0355, 1.0011, 1.0018, 1.0005, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0021, 1.0008, 1.0012, 1.0112, 1.0013, 1.0010, 1.0007, 1.0014,\n",
      "         1.0040, 1.0025, 1.0672, 1.0025, 1.0001, 1.0013, 1.0004, 1.0005, 1.0022,\n",
      "         1.0069, 1.0026, 1.0005, 1.0009, 1.0004, 1.0007, 1.0026, 1.0025, 1.0038,\n",
      "         1.0005, 1.0004, 1.0009, 1.0000, 1.0021, 1.0030, 1.0044, 1.0006, 1.0017,\n",
      "         1.0107, 1.0039, 1.0251]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0067, 1.0035, 1.0040, 1.0010, 1.0005, 1.0315, 1.0039, 1.0010,\n",
      "         1.0805, 1.0011, 1.0003, 1.0034, 1.0005, 1.0017, 1.0021, 1.0011, 1.0113,\n",
      "         1.0008, 1.0100, 1.0083, 1.0020, 1.0021, 1.0026, 1.0071, 1.0000],\n",
      "        [0.0000, 1.0151, 1.0160, 1.0008, 1.0012, 1.0134, 1.0028, 1.0201, 1.0014,\n",
      "         1.0025, 1.0017, 1.0070, 1.0039, 1.0034, 1.0154, 1.0112, 1.0035, 1.0015,\n",
      "         1.0012, 1.0047, 1.0238, 1.0302, 1.0012, 1.0018, 1.0014, 1.0005]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0014, 1.0021, 1.0004, 1.0013, 1.0010, 1.0076, 1.0491, 1.0005,\n",
      "         1.0019, 1.0010, 1.0057, 1.0086, 1.0003, 1.0020, 1.0107, 1.0041, 1.0093,\n",
      "         1.0078, 1.0005, 1.0175, 1.0182, 1.0004, 1.0011, 1.0007, 1.0009, 1.0003,\n",
      "         1.0016, 1.0093],\n",
      "        [0.0000, 1.0012, 1.0345, 1.0078, 1.0139, 1.0010, 1.0130, 1.0056, 1.0005,\n",
      "         1.0049, 1.0013, 1.0759, 1.0090, 1.0105, 1.0022, 1.0081, 1.0050, 1.0145,\n",
      "         1.0041, 1.0145, 1.0045, 1.0073, 1.0157, 1.0064, 1.0007, 1.0006, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0118, 1.0515, 1.0100, 1.0076, 1.0050, 1.0034, 1.0195, 1.0019,\n",
      "         1.0100, 1.0041, 1.0119, 1.0065, 1.0012, 1.0018, 1.0061, 1.0035, 1.0322,\n",
      "         1.0055, 1.0023, 1.0008, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0085, 1.0016, 1.0368, 1.0094, 1.0008, 1.0103, 1.0061, 1.0010,\n",
      "         1.0031, 1.0170, 1.0019, 1.0076, 1.0072, 1.0246, 1.0011, 1.0056, 1.0115,\n",
      "         1.0048, 1.0040, 1.0009, 1.0007, 1.0074, 1.0026, 1.0224, 1.0049, 1.0018,\n",
      "         1.0017, 1.0005, 1.0004, 1.0008, 1.0037, 1.0005, 1.0168, 1.0009, 1.0027,\n",
      "         1.0006, 1.0007, 1.0015, 1.0002, 1.0007, 1.0134, 1.0003, 1.0002]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0131, 1.0107, 1.0097, 1.0053, 1.0132, 1.0150, 1.0035, 1.0022,\n",
      "         1.0277, 1.0169, 1.0051, 1.0132, 1.0370, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0044, 1.0003, 1.0029, 1.0071, 1.0023, 1.0047, 1.0012, 1.0331,\n",
      "         1.0079, 1.0102, 1.0004, 1.0016, 1.0085, 1.0021, 1.0121, 1.0012, 1.0010,\n",
      "         1.0003, 1.0017, 1.0024]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0014, 1.0007, 1.0002, 1.0079, 1.0000, 1.0013, 1.0005, 1.0005,\n",
      "         1.0004, 1.0049, 1.0025, 1.0088, 1.0005, 1.0031, 1.0004, 1.0010, 1.0045,\n",
      "         1.0002, 1.0014, 1.0009, 1.0002, 1.0025, 1.0019, 1.0002, 1.0005, 1.0001,\n",
      "         1.0141, 1.0021, 1.0003, 1.0006, 1.0347, 1.0002, 1.0208, 1.0007, 1.0068,\n",
      "         1.0009, 1.0031, 1.0007, 1.0028, 1.0024],\n",
      "        [0.0000, 1.0189, 1.0099, 1.0034, 1.0333, 1.0049, 1.0099, 1.0096, 1.0036,\n",
      "         1.0011, 1.0000, 1.0008, 1.0248, 1.0013, 1.0169, 1.0049, 1.0163, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0213, 1.0218, 1.0256, 1.0029, 1.0137, 1.0075, 1.0159, 1.0059,\n",
      "         1.0038, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0023, 1.0081, 1.0097, 1.0115, 1.0047, 1.0705, 1.0325, 1.0081,\n",
      "         1.0047, 1.0264, 1.0025, 1.0186, 1.0091, 1.0087, 1.0022, 1.0079, 1.0195,\n",
      "         1.0016, 1.0061]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0542, 1.0015, 1.0018, 1.0020, 1.0031, 1.0298, 1.0124, 1.0070,\n",
      "         1.0708, 1.0015, 1.0019, 1.0314, 1.0050, 1.0010, 1.0027, 1.0012, 1.0011,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0029, 1.0019, 1.0039, 1.0022, 1.0126, 1.0024, 1.0018, 1.0030,\n",
      "         1.0009, 1.0054, 1.0053, 1.0006, 1.0019, 1.0042, 1.0093, 1.0011, 1.0045,\n",
      "         1.0012, 1.0029, 1.0203, 1.0060, 1.0025, 1.0005, 1.0022, 1.0152, 1.0066,\n",
      "         1.0037, 1.0007, 1.0045, 1.0021, 1.0005, 1.0011, 1.0089, 1.0167]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0119, 1.0042, 1.0011, 1.0016, 1.0017, 1.0020, 1.0002, 1.0034,\n",
      "         1.0016, 1.0014, 1.0017, 1.0004, 1.0046, 1.0668, 1.0018, 1.0011, 1.0014,\n",
      "         1.0013, 1.0006, 1.0118, 1.0044, 1.0006, 1.0023, 1.0012, 1.0002, 1.0041,\n",
      "         1.0031, 1.0004, 1.0002, 1.0016, 1.0013, 1.0139, 1.0031, 1.0258],\n",
      "        [0.0000, 1.0032, 1.0010, 1.0012, 1.0024, 1.0112, 1.0125, 1.0347, 1.0014,\n",
      "         1.0023, 1.0012, 1.0237, 1.0004, 1.0077, 1.0248, 1.0025, 1.0007, 1.0350,\n",
      "         1.0139, 1.0026, 1.0074, 1.0057, 1.0020, 1.0056, 1.0078, 1.0162, 1.0007,\n",
      "         1.0006, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 19, 19])  attentions_grads shape: torch.Size([3, 2, 12, 19, 19])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 19, 19])\n",
      "joint_attentions shape: torch.Size([2, 19, 19])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0101, 1.0066, 1.0161, 1.0040, 1.0128, 1.0068, 1.0084, 1.0173,\n",
      "         1.0095, 1.0025, 1.0385, 1.0106, 1.0041, 1.0068, 1.0096, 1.0037, 1.0062,\n",
      "         1.0285],\n",
      "        [0.0000, 1.0252, 1.0205, 1.0109, 1.0019, 1.0141, 1.0100, 1.0039, 1.0108,\n",
      "         1.0168, 1.0542, 1.0234, 1.0229, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 38, 38])  attentions_grads shape: torch.Size([3, 2, 12, 38, 38])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 38, 38])\n",
      "joint_attentions shape: torch.Size([2, 38, 38])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0293, 1.0011, 1.0081, 1.0009, 1.0014, 1.0113, 1.0025, 1.0033,\n",
      "         1.0033, 1.0242, 1.0172, 1.0006, 1.0010, 1.0389, 1.0126, 1.0029, 1.0051,\n",
      "         1.0032, 1.0021, 1.0009, 1.0041, 1.0007, 1.0056, 1.0076, 1.0262, 1.0090,\n",
      "         1.0010, 1.0012, 1.0073, 1.0051, 1.0008, 1.0016, 1.0008, 1.0003, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0027, 1.0023, 1.0028, 1.0011, 1.0024, 1.0040, 1.0013, 1.0028,\n",
      "         1.0013, 1.0001, 1.0013, 1.0016, 1.0040, 1.0023, 1.0021, 1.0012, 1.0015,\n",
      "         1.0012, 1.0529, 1.0020, 1.0009, 1.0005, 1.0018, 1.0009, 1.0024, 1.0020,\n",
      "         1.0068, 1.0007, 1.0012, 1.0025, 1.0011, 1.0003, 1.0032, 1.0189, 1.0007,\n",
      "         1.0015, 1.0159]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0037, 1.0744, 1.0204, 1.0564, 1.0061, 1.0820, 1.0243, 1.0028,\n",
      "         1.0054, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0017, 1.0007, 1.0153, 1.0005, 1.0015, 1.0012, 1.0036, 1.0004,\n",
      "         1.0011, 1.0000, 1.0011, 1.0007, 1.0023, 1.0012, 1.0015, 1.0007, 1.0004,\n",
      "         1.0001, 1.0004, 1.0046, 1.0006, 1.0021, 1.0003, 1.0009, 1.0075, 1.0446,\n",
      "         1.0103, 1.0023, 1.0003, 1.0004, 1.0002, 1.0004, 1.0007, 1.0008, 1.0001,\n",
      "         1.0001, 1.0005, 1.0054]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0018, 1.0090, 1.0012, 1.0024, 1.0204, 1.0005, 1.0007, 1.0083,\n",
      "         1.0040, 1.0005, 1.0010, 1.0479, 1.0071, 1.0010, 1.0011, 1.0015, 1.0025,\n",
      "         1.0030, 1.0005, 1.0113, 1.0025, 1.0046, 1.0073, 1.0277, 1.0038, 1.0098],\n",
      "        [0.0000, 1.0075, 1.0079, 1.0039, 1.1050, 1.0033, 1.0072, 1.0168, 1.0019,\n",
      "         1.0097, 1.0020, 1.0011, 1.0208, 1.0313, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0018, 1.0070, 1.0006, 1.0033, 1.0200, 1.0019, 1.0087, 1.0106,\n",
      "         1.0270, 1.0092, 1.0120, 1.0006, 1.0065, 1.0138, 1.0056, 1.0016, 1.0058,\n",
      "         1.0028, 1.0133, 1.0023, 1.0013, 1.0037, 1.0026, 1.0197, 1.0056, 1.0188,\n",
      "         1.0178, 1.0050, 1.0007, 1.0004, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0003, 1.0054, 1.0019, 1.0234, 1.0024, 1.0030, 1.0007,\n",
      "         1.0004, 1.0004, 1.0007, 1.0022, 1.0000, 1.0003, 1.0015, 1.0035, 1.0031,\n",
      "         1.0021, 1.0019, 1.0028, 1.0010, 1.0035, 1.0011, 1.0075, 1.0015, 1.0004,\n",
      "         1.0054, 1.0293, 1.0039, 1.0007, 1.0004, 1.0004, 1.0011, 1.0016, 1.0011,\n",
      "         1.0005, 1.0004, 1.0116, 1.0020]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0512, 1.0026, 1.0015, 1.0344, 1.0100, 1.0024, 1.0032,\n",
      "         1.0023, 1.0014, 1.0434, 1.0076, 1.0027, 1.0252, 1.0185, 1.0116, 1.0026,\n",
      "         1.0056, 1.0011, 1.0006, 1.0029, 1.0219, 1.0094, 1.0036, 1.0016, 1.0007],\n",
      "        [0.0000, 1.0138, 1.0122, 1.0116, 1.0022, 1.0070, 1.0092, 1.0079, 1.0015,\n",
      "         1.0115, 1.0253, 1.0037, 1.0016, 1.0020, 1.0028, 1.0106, 1.0214, 1.0169,\n",
      "         1.0126, 1.0147, 1.0045, 1.0012, 1.0005, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0227, 1.0035, 1.0026, 1.0028, 1.0023, 1.0018, 1.0254, 1.0333,\n",
      "         1.0010, 1.0229, 1.0111, 1.0072, 1.0017, 1.0264, 1.0010, 1.0574, 1.0040,\n",
      "         1.0093, 1.0103, 1.0023, 1.0137, 1.0325, 1.0013, 1.0007, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0012, 1.0137, 1.0109, 1.0066, 1.0130, 1.0010, 1.0023, 1.0017,\n",
      "         1.0034, 1.0031, 1.0204, 1.0008, 1.0113, 1.0005, 1.0042, 1.0039, 1.0091,\n",
      "         1.0058, 1.0017, 1.0120, 1.0023, 1.0250, 1.0085, 1.0450, 1.0054, 1.0093,\n",
      "         1.0034, 1.0005, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0018, 1.0056, 1.0565, 1.0008, 1.0001, 1.0000, 1.0041, 1.0029,\n",
      "         1.0008, 1.0005, 1.0013, 1.0011, 1.0036, 1.0479, 1.0022, 1.0351, 1.0058,\n",
      "         1.0021, 1.0058, 1.0008, 1.0094, 1.0022, 1.0068, 1.0017, 1.0007],\n",
      "        [0.0000, 1.0109, 1.0023, 1.0028, 1.0018, 1.0127, 1.0077, 1.0122, 1.0105,\n",
      "         1.0345, 1.0101, 1.0121, 1.0752, 1.0131, 1.0276, 1.0208, 1.0069, 1.0024,\n",
      "         1.0043, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0111, 1.0043, 1.0784, 1.0024, 1.0015, 1.0042, 1.0036, 1.0005,\n",
      "         1.0029, 1.0015, 1.0022, 1.0320, 1.0007, 1.0042, 1.0024, 1.0002, 1.0036,\n",
      "         1.0046, 1.0022, 1.0054],\n",
      "        [0.0000, 1.0040, 1.0007, 1.0323, 1.0047, 1.0017, 1.0021, 1.0010, 1.0017,\n",
      "         1.0015, 1.0027, 1.0017, 1.0008, 1.0230, 1.0000, 1.0009, 1.0089, 1.0043,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0020, 1.0040, 1.0025, 1.0077, 1.0236, 1.0022, 1.0085, 1.0122,\n",
      "         1.0031, 1.0025, 1.0162, 1.0022, 1.0009, 1.0018, 1.0027, 1.0017, 1.0016,\n",
      "         1.0025, 1.0052, 1.0452, 1.0147, 1.0222],\n",
      "        [0.0000, 1.0019, 1.0220, 1.0157, 1.0601, 1.0018, 1.0431, 1.0029, 1.0032,\n",
      "         1.0062, 1.0117, 1.0180, 1.0018, 1.0591, 1.0144, 1.0013, 1.0015, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0112, 1.0049, 1.0217, 1.0029, 1.0048, 1.0441, 1.0024, 1.0354,\n",
      "         1.0020, 1.0048, 1.0619, 1.0041, 1.0019, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0038, 1.0036, 1.0065, 1.0209, 1.0036, 1.0021, 1.0010, 1.0029,\n",
      "         1.0133, 1.0007, 1.0016, 1.0078, 1.0016, 1.0030, 1.0147, 1.0008, 1.0034,\n",
      "         1.0033, 1.0094, 1.0020, 1.0009, 1.0007, 1.0039, 1.0353]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 42, 42])  attentions_grads shape: torch.Size([3, 2, 12, 42, 42])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 42, 42])\n",
      "joint_attentions shape: torch.Size([2, 42, 42])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0011, 1.0298, 1.0009, 1.0016, 1.0057, 1.0105, 1.0003,\n",
      "         1.0012, 1.0010, 1.0128, 1.0008, 1.0006, 1.0012, 1.0001, 1.0005, 1.0014,\n",
      "         1.0023, 1.0031, 1.0008, 1.0004, 1.0179, 1.0015, 1.0022, 1.0002, 1.0000,\n",
      "         1.0059, 1.0009, 1.0045, 1.0008, 1.0018, 1.0012, 1.0005, 1.0003, 1.0095,\n",
      "         1.0004, 1.0048, 1.0048, 1.0085, 1.0054, 1.0051],\n",
      "        [0.0000, 1.0007, 1.0091, 1.0040, 1.0173, 1.0015, 1.0038, 1.0031, 1.0008,\n",
      "         1.0135, 1.0087, 1.0062, 1.0009, 1.0010, 1.0096, 1.0028, 1.0016, 1.0011,\n",
      "         1.0289, 1.0017, 1.0020, 1.0027, 1.0120, 1.0384, 1.0363, 1.0012, 1.0030,\n",
      "         1.0104, 1.0005, 1.0004, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0093, 1.0020, 1.0139, 1.0098, 1.0020, 1.0019, 1.0067, 1.0359,\n",
      "         1.0160, 1.0021, 1.0048, 1.0239, 1.0210, 1.0156, 1.0053, 1.0106, 1.0011,\n",
      "         1.0051, 1.0138, 1.0066, 1.0021],\n",
      "        [0.0000, 1.0057, 1.0096, 1.0020, 1.0065, 1.0497, 1.0034, 1.0012, 1.0038,\n",
      "         1.0080, 1.0370, 1.0046, 1.0357, 1.0124, 1.0349, 1.0096, 1.0017, 1.0007,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0108, 1.0444, 1.0012, 1.0038, 1.0014, 1.0015, 1.0002, 1.0004,\n",
      "         1.0036, 1.0079, 1.0014, 1.0004, 1.0008, 1.0029, 1.0128, 1.0007, 1.0063,\n",
      "         1.0035, 1.0044, 1.0323, 1.0022, 1.0057, 1.0022, 1.0002, 1.0012, 1.0132],\n",
      "        [0.0000, 1.0049, 1.0567, 1.0193, 1.0484, 1.0614, 1.0105, 1.0171, 1.0029,\n",
      "         1.0014, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0044, 1.0016, 1.0015, 1.0006, 1.0408, 1.0036, 1.0006, 1.0011,\n",
      "         1.0005, 1.0000, 1.0073, 1.0012, 1.0521, 1.0009, 1.0093, 1.0002, 1.0011,\n",
      "         1.0052, 1.0007, 1.0098, 1.0002, 1.0002, 1.0011, 1.0004, 1.0038, 1.0010,\n",
      "         1.0003, 1.0004, 1.0001, 1.0009, 1.0100],\n",
      "        [0.0000, 1.0217, 1.0127, 1.0289, 1.0143, 1.0068, 1.0030, 1.0243, 1.0011,\n",
      "         1.0159, 1.0108, 1.0204, 1.0046, 1.0199, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0060, 1.0040, 1.0036, 1.0173, 1.0481, 1.0029, 1.0081, 1.0200,\n",
      "         1.0048, 1.0017, 1.0168, 1.0081, 1.0038, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0023, 1.0013, 1.0029, 1.0051, 1.0295, 1.0009, 1.0159, 1.0020,\n",
      "         1.0039, 1.0007, 1.0120, 1.0019, 1.0004, 1.0143, 1.0008, 1.0059, 1.0001,\n",
      "         1.0014, 1.0015, 1.0005, 1.0001, 1.0008, 1.0001, 1.0004, 1.0002, 1.0011,\n",
      "         1.0006, 1.0253, 1.0024, 1.0036, 1.0030, 1.0088, 1.0016, 1.0054, 1.0009,\n",
      "         1.0017]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 38, 38])  attentions_grads shape: torch.Size([3, 2, 12, 38, 38])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 38, 38])\n",
      "joint_attentions shape: torch.Size([2, 38, 38])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0009, 1.0010, 1.0009, 1.0065, 1.0006, 1.0046, 1.0069, 1.0023,\n",
      "         1.0136, 1.0025, 1.0088, 1.0180, 1.0009, 1.0136, 1.0132, 1.0007, 1.0469,\n",
      "         1.0056, 1.0017, 1.0168, 1.0025, 1.0019, 1.0013, 1.0532, 1.0008, 1.0011,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0088, 1.0101, 1.0016, 1.0057, 1.0014, 1.0012, 1.0006, 1.0011,\n",
      "         1.0055, 1.0050, 1.0013, 1.0054, 1.0020, 1.0026, 1.0012, 1.0283, 1.0086,\n",
      "         1.0036, 1.0031, 1.0005, 1.0021, 1.0060, 1.0009, 1.0006, 1.0041, 1.0000,\n",
      "         1.0003, 1.0031, 1.0006, 1.0004, 1.0018, 1.0012, 1.0054, 1.0004, 1.0003,\n",
      "         1.0066, 1.0316]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0044, 1.0146, 1.0579, 1.0011, 1.0024, 1.0058, 1.0065, 1.0014,\n",
      "         1.0058, 1.0097, 1.0012, 1.0116, 1.0149, 1.0310, 1.0196, 1.0008, 1.0049,\n",
      "         1.0205, 1.0022, 1.0005, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0013, 1.0025, 1.0619, 1.0025, 1.0035, 1.0052, 1.0091, 1.0009,\n",
      "         1.0042, 1.0210, 1.0009, 1.0092, 1.0009, 1.0014, 1.0366, 1.0053, 1.0042,\n",
      "         1.0082, 1.0024, 1.0042, 1.0007, 1.0107, 1.0037, 1.0049, 1.0086, 1.0051,\n",
      "         1.0096, 1.0047, 1.0007, 1.0004]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0205, 1.0031, 1.0047, 1.0047, 1.0117, 1.0129, 1.0104, 1.0042,\n",
      "         1.0059, 1.0023, 1.0055, 1.0185, 1.0001, 1.0053, 1.0041, 1.0039, 1.0143,\n",
      "         1.0040, 1.0140, 1.0004, 1.0024, 1.0032, 1.0133],\n",
      "        [0.0000, 1.0024, 1.0020, 1.0172, 1.0028, 1.0175, 1.0015, 1.0046, 1.0192,\n",
      "         1.0013, 1.0013, 1.0012, 1.0059, 1.0002, 1.0081, 1.0012, 1.0024, 1.0035,\n",
      "         1.0081, 1.0132, 1.0028, 1.0066, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0468, 1.0012, 1.0106, 1.0296, 1.0031, 1.0023, 1.0160,\n",
      "         1.0056, 1.0111, 1.0095, 1.0228, 1.0092, 1.0012, 1.0017, 1.0010, 1.0061,\n",
      "         1.0395, 1.0161, 1.0026, 1.0003],\n",
      "        [0.0000, 1.0032, 1.0536, 1.0189, 1.0305, 1.0078, 1.0183, 1.1251, 1.0153,\n",
      "         1.0026, 1.0039, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 17, 17])  attentions_grads shape: torch.Size([3, 2, 12, 17, 17])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 17, 17])\n",
      "joint_attentions shape: torch.Size([2, 17, 17])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0930, 1.0248, 1.0023, 1.0205, 1.0017, 1.0019, 1.0053, 1.0016,\n",
      "         1.0038, 1.0016, 1.0065, 1.0046, 1.0010, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0035, 1.0088, 1.0035, 1.0038, 1.0052, 1.0060, 1.0317, 1.0131,\n",
      "         1.0079, 1.0058, 1.0069, 1.0113, 1.0008, 1.0000, 1.0068, 1.0238]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0057, 1.0087, 1.0010, 1.0002, 1.0031, 1.0005, 1.0025, 1.0005,\n",
      "         1.0003, 1.0003, 1.0259, 1.0084, 1.0024, 1.0105, 1.0007, 1.0025, 1.0034,\n",
      "         1.0004, 1.0088, 1.0007, 1.0002, 1.0000, 1.0009, 1.0005, 1.0010, 1.0006,\n",
      "         1.0031, 1.0020, 1.0003, 1.0393, 1.0004, 1.0018, 1.0003, 1.0003, 1.0018,\n",
      "         1.0005, 1.0010, 1.0002, 1.0064, 1.0011],\n",
      "        [0.0000, 1.0410, 1.0024, 1.0086, 1.0041, 1.0010, 1.0009, 1.0020, 1.0022,\n",
      "         1.0014, 1.0010, 1.0019, 1.0017, 1.0012, 1.0023, 1.0016, 1.0004, 1.0181,\n",
      "         1.0016, 1.0041, 1.0003, 1.0227, 1.0071, 1.0202, 1.0068, 1.0105, 1.0014,\n",
      "         1.0025, 1.0172, 1.0033, 1.0015, 1.0404, 1.0017, 1.0008, 1.0004, 1.0001,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0239, 1.0361, 1.0151, 1.0036, 1.0332, 1.0271, 1.0254, 1.0074,\n",
      "         1.0019, 1.0094, 1.0039, 1.0006, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0042, 1.0007, 1.0066, 1.0151, 1.0031, 1.0157, 1.0132, 1.0029,\n",
      "         1.0008, 1.0152, 1.0012, 1.0012, 1.0007, 1.0049, 1.0016, 1.0034, 1.0009,\n",
      "         1.0022, 1.0344, 1.0025, 1.0017, 1.0044, 1.0078, 1.0022, 1.0023, 1.0291,\n",
      "         1.0008, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 19, 19])  attentions_grads shape: torch.Size([3, 2, 12, 19, 19])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 19, 19])\n",
      "joint_attentions shape: torch.Size([2, 19, 19])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0083, 1.0016, 1.0259, 1.0028, 1.0014, 1.0005, 1.0035,\n",
      "         1.0020, 1.0002, 1.0032, 1.0093, 1.0036, 1.0149, 1.0467, 1.0040, 1.0069,\n",
      "         1.0371],\n",
      "        [0.0000, 1.0137, 1.0088, 1.0145, 1.0089, 1.0042, 1.0146, 1.0707, 1.0157,\n",
      "         1.0038, 1.0020, 1.0444, 1.0172, 1.0077, 1.0027, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0106, 1.0159, 1.0793, 1.0072, 1.0014, 1.0029, 1.0036, 1.0015,\n",
      "         1.0044, 1.0035, 1.0003, 1.0033, 1.0158, 1.0337, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0047, 1.0083, 1.0049, 1.0016, 1.0205, 1.0019, 1.0016, 1.0218,\n",
      "         1.0023, 1.0116, 1.0281, 1.0025, 1.0212, 1.0039, 1.0050, 1.0035, 1.0015,\n",
      "         1.0306, 1.0090, 1.0018, 1.0006]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 53, 53])  attentions_grads shape: torch.Size([3, 2, 12, 53, 53])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 53, 53])\n",
      "joint_attentions shape: torch.Size([2, 53, 53])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0051, 1.0007, 1.0164, 1.0247, 1.0021, 1.0009, 1.0015, 1.0019,\n",
      "         1.0060, 1.0012, 1.0005, 1.0294, 1.0025, 1.0070, 1.0075, 1.0126, 1.0011,\n",
      "         1.0277, 1.0054, 1.0003, 1.0189, 1.0081, 1.0019, 1.0032, 1.0020, 1.0172,\n",
      "         1.0096, 1.0059, 1.0006, 1.0110, 1.0005, 1.0003, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0012, 1.0006, 1.0011, 1.0058, 1.0160, 1.0007, 1.0005, 1.0004,\n",
      "         1.0006, 1.0024, 1.0026, 1.0007, 1.0011, 1.0014, 1.0017, 1.0128, 1.0014,\n",
      "         1.0008, 1.0003, 1.0008, 1.0006, 1.0012, 1.0026, 1.0000, 1.0005, 1.0018,\n",
      "         1.0006, 1.0010, 1.0095, 1.0004, 1.0016, 1.0018, 1.0045, 1.0008, 1.0014,\n",
      "         1.0003, 1.0004, 1.0002, 1.0000, 1.0079, 1.0005, 1.0075, 1.0010, 1.0011,\n",
      "         1.0004, 1.0010, 1.0001, 1.0021, 1.0013, 1.0012, 1.0035, 1.0147]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0013, 1.0379, 1.0015, 1.0012, 1.0007, 1.0006, 1.0031, 1.0040,\n",
      "         1.0518, 1.0008, 1.0010, 1.0015, 1.0039, 1.0012, 1.0818, 1.0171, 1.0010,\n",
      "         1.0020, 1.0049, 1.0010, 1.0125, 1.0029, 1.0006, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0084, 1.0115, 1.0033, 1.0026, 1.0148, 1.0010, 1.0012, 1.0151,\n",
      "         1.0065, 1.0190, 1.0224, 1.0210, 1.0060, 1.0013, 1.0070, 1.0113, 1.0053,\n",
      "         1.0019, 1.0039, 1.0038, 1.0014, 1.0059, 1.0008, 1.0071, 1.0107]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0075, 1.0030, 1.0070, 1.0044, 1.0088, 1.0031, 1.0212, 1.0008,\n",
      "         1.0295, 1.0044, 1.0066, 1.0028, 1.0037, 1.0016, 1.0068, 1.0076, 1.0022,\n",
      "         1.0012, 1.0061, 1.0032, 1.0024, 1.0032, 1.0115, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0055, 1.0044, 1.0010, 1.0019, 1.0193, 1.0126, 1.0210, 1.0008,\n",
      "         1.0064, 1.0015, 1.0029, 1.0022, 1.0109, 1.0074, 1.0241, 1.0006, 1.0085,\n",
      "         1.0011, 1.0045, 1.0103, 1.0243, 1.0044, 1.0025, 1.0140, 1.0018, 1.0037,\n",
      "         1.0012, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0066, 1.0016, 1.0015, 1.0449, 1.0015, 1.0168, 1.0078,\n",
      "         1.0011, 1.0030, 1.0093, 1.0014, 1.0328, 1.0171, 1.0076, 1.0017, 1.0310,\n",
      "         1.0066, 1.0273, 1.0033, 1.0017, 1.0058, 1.0035, 1.0010, 1.0016, 1.0004,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0022, 1.0053, 1.0112, 1.0020, 1.0018, 1.0228, 1.0035, 1.0273,\n",
      "         1.0006, 1.0272, 1.0044, 1.0190, 1.0363, 1.0019, 1.0019, 1.0030, 1.0032,\n",
      "         1.0016, 1.0051, 1.0070, 1.0011, 1.0058, 1.0004, 1.0353, 1.0051, 1.0029,\n",
      "         1.0005]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0014, 1.0010, 1.0299, 1.0015, 1.0067, 1.0015, 1.0061, 1.0035,\n",
      "         1.0010, 1.0013, 1.0060, 1.0142, 1.0028, 1.0011, 1.0325, 1.0049, 1.0070,\n",
      "         1.0063, 1.0012, 1.0029, 1.0022, 1.0007, 1.0150, 1.0044, 1.0018, 1.0026],\n",
      "        [0.0000, 1.0022, 1.0101, 1.0086, 1.0017, 1.0015, 1.0013, 1.0033, 1.0010,\n",
      "         1.0016, 1.0013, 1.0070, 1.0111, 1.0015, 1.0021, 1.0198, 1.0163, 1.0008,\n",
      "         1.0038, 1.0294, 1.0075, 1.0031, 1.0017, 1.0010, 1.0169, 1.0004, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 38, 38])  attentions_grads shape: torch.Size([3, 2, 12, 38, 38])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 38, 38])\n",
      "joint_attentions shape: torch.Size([2, 38, 38])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0070, 1.0003, 1.0142, 1.0017, 1.0004, 1.0009, 1.0029, 1.0026,\n",
      "         1.0278, 1.0006, 1.0097, 1.0002, 1.0005, 1.0126, 1.0003, 1.0072, 1.0044,\n",
      "         1.0002, 1.0007, 1.0069, 1.0032, 1.0011, 1.0010, 1.0234, 1.0004, 1.0007,\n",
      "         1.0045, 1.0046, 1.0006, 1.0039, 1.0140, 1.0003, 1.0041, 1.0155, 1.0077,\n",
      "         1.0004, 1.0001],\n",
      "        [0.0000, 1.0114, 1.0081, 1.0073, 1.0007, 1.0020, 1.0017, 1.0015, 1.0184,\n",
      "         1.0071, 1.0016, 1.0097, 1.0048, 1.0051, 1.0019, 1.0037, 1.0008, 1.0005,\n",
      "         1.0015, 1.0040, 1.0011, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0067, 1.0039, 1.0028, 1.0021, 1.0028, 1.0331, 1.0164, 1.0338,\n",
      "         1.0058, 1.0348, 1.0088, 1.0061, 1.0109, 1.0071, 1.0285, 1.0062, 1.0034,\n",
      "         1.0016, 1.0405, 1.0018, 1.0011],\n",
      "        [0.0000, 1.0554, 1.0018, 1.0341, 1.0022, 1.0191, 1.0047, 1.0214, 1.0160,\n",
      "         1.0017, 1.0014, 1.0453, 1.0149, 1.0258, 1.0116, 1.0073, 1.0057, 1.0089,\n",
      "         1.0229, 1.0011, 1.0013, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0064, 1.0150, 1.0054, 1.0022, 1.0190, 1.0110, 1.0052, 1.0035,\n",
      "         1.0028, 1.0035, 1.0010, 1.0021, 1.0112, 1.0011, 1.0024, 1.0017, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0022, 1.0151, 1.0073, 1.0104, 1.0322, 1.0077, 1.0021, 1.0021,\n",
      "         1.0005, 1.0070, 1.0037, 1.0015, 1.0015, 1.0012, 1.0063, 1.0028, 1.0010,\n",
      "         1.0013, 1.0408, 1.0055, 1.0012, 1.0004]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0022, 1.0010, 1.0228, 1.0005, 1.0020, 1.0010, 1.0007, 1.0263,\n",
      "         1.0008, 1.0065, 1.0107, 1.0024, 1.0017, 1.0074, 1.0030, 1.0102, 1.0024,\n",
      "         1.0022, 1.0043, 1.0013, 1.0398, 1.0074, 1.0017, 1.0040, 1.0011, 1.0051,\n",
      "         1.0042, 1.0043, 1.0038, 1.0006, 1.0003],\n",
      "        [0.0000, 1.0010, 1.0036, 1.0009, 1.0011, 1.0010, 1.0013, 1.0038, 1.0030,\n",
      "         1.0002, 1.0031, 1.0007, 1.0009, 1.0007, 1.0021, 1.0012, 1.0009, 1.0017,\n",
      "         1.0002, 1.0057, 1.0018, 1.0032, 1.0033, 1.0017, 1.0828, 1.0045, 1.0000,\n",
      "         1.0010, 1.0012, 1.0136, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0023, 1.0032, 1.0068, 1.0227, 1.0026, 1.0027, 1.0018, 1.0012,\n",
      "         1.0019, 1.0015, 1.0023, 1.0016, 1.0011, 1.0039, 1.0025, 1.0218, 1.0020,\n",
      "         1.0048, 1.0026, 1.0011, 1.0061, 1.0024, 1.0016, 1.0011, 1.0001, 1.0022,\n",
      "         1.0040, 1.0009, 1.0013, 1.0035, 1.0033, 1.0380],\n",
      "        [0.0000, 1.0054, 1.0042, 1.0042, 1.0036, 1.0032, 1.0030, 1.0034, 1.0018,\n",
      "         1.0029, 1.0027, 1.0207, 1.0004, 1.0035, 1.0035, 1.0004, 1.0034, 1.0007,\n",
      "         1.0033, 1.0038, 1.0066, 1.0007, 1.0199, 1.0005, 1.0052, 1.0134, 1.0007,\n",
      "         1.0251, 1.0005, 1.0003, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0071, 1.0020, 1.0014, 1.0028, 1.0011, 1.0060, 1.0404, 1.0008,\n",
      "         1.0026, 1.0000, 1.0016, 1.0006, 1.0006, 1.0014, 1.0005, 1.0014, 1.0098,\n",
      "         1.0096, 1.0021, 1.0128, 1.0032, 1.0003, 1.0030, 1.0004, 1.0029, 1.0049,\n",
      "         1.0032, 1.0048, 1.0382],\n",
      "        [0.0000, 1.0423, 1.0047, 1.0058, 1.0042, 1.0124, 1.0061, 1.0211, 1.0006,\n",
      "         1.0057, 1.0402, 1.0271, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0211, 1.0177, 1.0020, 1.0202, 1.0005, 1.0009, 1.0031, 1.0038,\n",
      "         1.0029, 1.0013, 1.0044, 1.0023, 1.0243, 1.0006, 1.0346, 1.0011, 1.0011,\n",
      "         1.0034, 1.0198, 1.0009, 1.0026, 1.0007, 1.0007, 1.0003, 1.0000],\n",
      "        [0.0000, 1.0053, 1.0053, 1.0156, 1.0171, 1.0002, 1.0066, 1.0011, 1.0015,\n",
      "         1.0020, 1.0022, 1.0021, 1.0000, 1.0013, 1.0003, 1.0043, 1.0015, 1.0070,\n",
      "         1.0107, 1.0014, 1.0250, 1.0016, 1.0032, 1.0006, 1.0072, 1.0034]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0059, 1.0068, 1.0014, 1.0190, 1.0412, 1.0013, 1.0013, 1.0015,\n",
      "         1.0057, 1.0232, 1.0050, 1.0088, 1.0171, 1.0274, 1.0056, 1.0546, 1.0045,\n",
      "         1.0138, 1.0007, 1.0075, 1.0083, 1.0004],\n",
      "        [0.0000, 1.0019, 1.0627, 1.0094, 1.0024, 1.0221, 1.0037, 1.0183, 1.0027,\n",
      "         1.0022, 1.0110, 1.0128, 1.0137, 1.0094, 1.0064, 1.0495, 1.0013, 1.0025,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 55, 55])  attentions_grads shape: torch.Size([3, 2, 12, 55, 55])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 55, 55])\n",
      "joint_attentions shape: torch.Size([2, 55, 55])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0020, 1.0005, 1.0007, 1.0036, 1.0004, 1.0005, 1.0020, 1.0089,\n",
      "         1.0025, 1.0081, 1.0019, 1.0007, 1.0041, 1.0141, 1.0005, 1.0039, 1.0022,\n",
      "         1.0005, 1.0038, 1.0007, 1.0160, 1.0015, 1.0008, 1.0008, 1.0107, 1.0008,\n",
      "         1.0013, 1.0154, 1.0003, 1.0023, 1.0010, 1.0030, 1.0103, 1.0023, 1.0064,\n",
      "         1.0004, 1.0011, 1.0020, 1.0005, 1.0088, 1.0180, 1.0086, 1.0006, 1.0009,\n",
      "         1.0017, 1.0029, 1.0004, 1.0011, 1.0003, 1.0060, 1.0043, 1.0133, 1.0004,\n",
      "         1.0001],\n",
      "        [0.0000, 1.0000, 1.0178, 1.0047, 1.0221, 1.0109, 1.0014, 1.0009, 1.0080,\n",
      "         1.0027, 1.0009, 1.0290, 1.0068, 1.0016, 1.0013, 1.0008, 1.0069, 1.0021,\n",
      "         1.0026, 1.0071, 1.0027, 1.0007, 1.0009, 1.0008, 1.0013, 1.0007, 1.0007,\n",
      "         1.0008, 1.0014, 1.0045, 1.0006, 1.0021, 1.0005, 1.0007, 1.0004, 1.0102,\n",
      "         1.0005, 1.0083, 1.0368, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0015, 1.0116, 1.0015, 1.0016, 1.0154, 1.0071, 1.0006, 1.0043,\n",
      "         1.0195, 1.0102, 1.0105, 1.0171, 1.0224, 1.0047, 1.0254, 1.0023, 1.0020,\n",
      "         1.0036, 1.0014, 1.0126, 1.0106, 1.0055, 1.0004, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0054, 1.0015, 1.0021, 1.0053, 1.0032, 1.0036, 1.0302, 1.0043,\n",
      "         1.0020, 1.0043, 1.0156, 1.0012, 1.0042, 1.0036, 1.0012, 1.0029, 1.0044,\n",
      "         1.0084, 1.0009, 1.0020, 1.0031, 1.0068, 1.0003, 1.0016, 1.0055, 1.0018,\n",
      "         1.0008]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0671, 1.0027, 1.0015, 1.0044, 1.0005, 1.0052, 1.0013, 1.0015,\n",
      "         1.0136, 1.0009, 1.0015, 1.0002, 1.0025, 1.0007, 1.0188, 1.0008, 1.0014,\n",
      "         1.0095, 1.0010, 1.0086, 1.0002, 1.0171, 1.0044, 1.0002, 1.0018, 1.0030,\n",
      "         1.0065],\n",
      "        [0.0000, 1.0045, 1.0028, 1.0016, 1.0256, 1.0035, 1.0098, 1.0234, 1.0260,\n",
      "         1.0019, 1.0089, 1.0082, 1.0013, 1.0014, 1.0157, 1.0031, 1.0041, 1.0027,\n",
      "         1.0077, 1.0422, 1.0042, 1.0054, 1.0012, 1.0006, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0112, 1.0948, 1.0008, 1.0141, 1.0016, 1.0024, 1.0027, 1.0015,\n",
      "         1.0042, 1.0076, 1.0010, 1.0095, 1.0013, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0056, 1.0011, 1.0048, 1.0075, 1.0328, 1.0078, 1.0008, 1.0031,\n",
      "         1.0235, 1.0023, 1.0002, 1.0055, 1.0001, 1.0011, 1.0033, 1.0000, 1.0001,\n",
      "         1.0021, 1.0018]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 9, 9])  attentions_grads shape: torch.Size([3, 2, 12, 9, 9])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 9, 9])\n",
      "joint_attentions shape: torch.Size([2, 9, 9])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0780, 1.0106, 1.0052, 1.0181, 1.0150, 1.0000, 1.0034, 1.0009],\n",
      "        [0.0000, 1.0161, 1.0178, 1.0114, 1.0302, 1.0104, 1.0232, 1.0516, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0031, 1.0176, 1.0008, 1.0130, 1.0021, 1.0078, 1.0088, 1.0339,\n",
      "         1.0020, 1.0153, 1.0209, 1.0029, 1.0021, 1.0134, 1.0103, 1.0019, 1.0113,\n",
      "         1.0026, 1.0015, 1.0099, 1.0007, 1.0027, 1.0008, 1.0008, 1.0033, 1.0088,\n",
      "         1.0006, 1.0180, 1.0009, 1.0020, 1.0038, 1.0021, 1.0007, 1.0003],\n",
      "        [0.0000, 1.0013, 1.0065, 1.0019, 1.0154, 1.0043, 1.0050, 1.0088, 1.0021,\n",
      "         1.0012, 1.0019, 1.0118, 1.0045, 1.0099, 1.0045, 1.0044, 1.0019, 1.0014,\n",
      "         1.0162, 1.0030, 1.0026, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0074, 1.0013, 1.0102, 1.0537, 1.0149, 1.0132, 1.0024, 1.0335,\n",
      "         1.0272, 1.0078, 1.0013, 1.0030, 1.0192, 1.0018, 1.0017, 1.0134, 1.0010,\n",
      "         1.0021, 1.0009, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0262, 1.0037, 1.0330, 1.0002, 1.0046, 1.0037, 1.0083, 1.0010,\n",
      "         1.0010, 1.0252, 1.0036, 1.0096, 1.0187, 1.0045, 1.0056, 1.0015, 1.0015,\n",
      "         1.0054, 1.0063, 1.0020, 1.0057, 1.0375, 1.0020, 1.0025, 1.0029, 1.0320,\n",
      "         1.0009, 1.0086, 1.0007, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0049, 1.0011, 1.0014, 1.0066, 1.0073, 1.0016, 1.0005, 1.0005,\n",
      "         1.0013, 1.0067, 1.0002, 1.0016, 1.0012, 1.0024, 1.0021, 1.0013, 1.0112,\n",
      "         1.0004, 1.0019, 1.0174, 1.0048, 1.0093, 1.0027, 1.0012, 1.0069],\n",
      "        [0.0000, 1.0044, 1.0052, 1.0059, 1.0061, 1.0058, 1.0060, 1.0048, 1.0005,\n",
      "         1.0029, 1.0024, 1.0017, 1.0104, 1.0024, 1.0014, 1.0009, 1.0085, 1.0096,\n",
      "         1.0045, 1.0327, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0264, 1.0071, 1.0042, 1.0011, 1.0041, 1.0102, 1.0012, 1.0007,\n",
      "         1.0026, 1.0079, 1.0041, 1.0043, 1.0103, 1.0069, 1.0045, 1.0017, 1.0434,\n",
      "         1.0146, 1.0118, 1.0077, 1.0086, 1.0065, 1.0045, 1.0108, 1.0070, 1.0139,\n",
      "         1.0068, 1.0061, 1.0181, 1.0006, 1.0003],\n",
      "        [0.0000, 1.0054, 1.0103, 1.0043, 1.0061, 1.0014, 1.0008, 1.0013, 1.0012,\n",
      "         1.0018, 1.0000, 1.0042, 1.0013, 1.0011, 1.0113, 1.0092, 1.0038, 1.0005,\n",
      "         1.0003, 1.0008, 1.0011, 1.0008, 1.0064, 1.0076, 1.0061, 1.0146, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 13, 13])  attentions_grads shape: torch.Size([3, 2, 12, 13, 13])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 13, 13])\n",
      "joint_attentions shape: torch.Size([2, 13, 13])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0342, 1.0031, 1.0358, 1.0187, 1.0132, 1.0030, 1.0077, 1.0973,\n",
      "         1.0035, 1.0016, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0089, 1.0077, 1.0415, 1.0169, 1.0036, 1.0009, 1.0154, 1.0254,\n",
      "         1.0365, 1.0011, 1.0026, 1.0008]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 38, 38])  attentions_grads shape: torch.Size([3, 2, 12, 38, 38])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 38, 38])\n",
      "joint_attentions shape: torch.Size([2, 38, 38])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0003, 1.0093, 1.0116, 1.0013, 1.0013, 1.0020, 1.0002, 1.0012,\n",
      "         1.0008, 1.0007, 1.0300, 1.0009, 1.0035, 1.0013, 1.0023, 1.0009, 1.0018,\n",
      "         1.0071, 1.0006, 1.0046, 1.0015, 1.0086, 1.0217, 1.0022, 1.0040, 1.0039,\n",
      "         1.0064, 1.0008, 1.0015, 1.0127, 1.0022, 1.0011, 1.0013, 1.0053, 1.0020,\n",
      "         1.0004, 1.0001],\n",
      "        [0.0000, 1.0015, 1.0206, 1.0025, 1.0539, 1.0047, 1.0010, 1.0079, 1.0011,\n",
      "         1.0198, 1.0037, 1.0206, 1.0131, 1.0162, 1.0060, 1.0091, 1.0046, 1.0031,\n",
      "         1.0041, 1.0012, 1.0008, 1.0228, 1.0028, 1.0232, 1.0123, 1.0231, 1.0017,\n",
      "         1.0005, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0061, 1.0022, 1.0010, 1.0130, 1.0006, 1.0022, 1.0000, 1.0095,\n",
      "         1.0099, 1.0017, 1.0006, 1.0675, 1.0027, 1.0008, 1.0003, 1.0087, 1.0244,\n",
      "         1.0101, 1.0031, 1.0076, 1.0009, 1.0004, 1.0035, 1.0248, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0039, 1.0023, 1.0044, 1.0035, 1.0013, 1.0013, 1.0063, 1.0029,\n",
      "         1.0013, 1.0182, 1.0242, 1.0014, 1.0037, 1.0026, 1.0020, 1.0013, 1.0036,\n",
      "         1.0109, 1.0050, 1.0091, 1.0015, 1.0011, 1.0015, 1.0050, 1.0005, 1.0003,\n",
      "         1.0053, 1.0010, 1.0015, 1.0032, 1.0037, 1.0006, 1.0007, 1.0042, 1.0016,\n",
      "         1.0049, 1.0017, 1.0062]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0061, 1.0109, 1.0006, 1.0007, 1.0006, 1.0041, 1.0008, 1.0026,\n",
      "         1.0006, 1.0082, 1.0002, 1.0007, 1.0462, 1.0054, 1.0055, 1.0006, 1.0020,\n",
      "         1.0023, 1.0124, 1.0020, 1.0019, 1.0009, 1.0050, 1.0006, 1.0025, 1.0071,\n",
      "         1.0007, 1.0007, 1.0065, 1.0062, 1.0243, 1.0058, 1.0005, 1.0006, 1.0005,\n",
      "         1.0130, 1.0014, 1.0004, 1.0001],\n",
      "        [0.0000, 1.0034, 1.0305, 1.0014, 1.0081, 1.0066, 1.0123, 1.0059, 1.0008,\n",
      "         1.0011, 1.0368, 1.0162, 1.0201, 1.0356, 1.0384, 1.0057, 1.0011, 1.0438,\n",
      "         1.0003, 1.0016, 1.0014, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0399, 1.0039, 1.0016, 1.0717, 1.0018, 1.0015, 1.0000, 1.0043,\n",
      "         1.0005, 1.0003, 1.0023, 1.0118, 1.0007, 1.0228, 1.0178, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0042, 1.0107, 1.0021, 1.0015, 1.0007, 1.0558, 1.0029, 1.0063,\n",
      "         1.0203, 1.0051, 1.0067, 1.0014, 1.0003, 1.0019, 1.0154, 1.0023, 1.0001,\n",
      "         1.0024, 1.0069]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 46, 46])  attentions_grads shape: torch.Size([3, 2, 12, 46, 46])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 46, 46])\n",
      "joint_attentions shape: torch.Size([2, 46, 46])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0024, 1.0080, 1.0068, 1.0025, 1.0061, 1.0017, 1.0011, 1.0065,\n",
      "         1.0091, 1.0009, 1.0010, 1.0284, 1.0117, 1.0036, 1.0043, 1.0084, 1.0216,\n",
      "         1.0014, 1.0010, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0027, 1.0001, 1.0007, 1.0020, 1.0013, 1.0025, 1.0013, 1.0005,\n",
      "         1.0020, 1.0045, 1.0010, 1.0003, 1.0144, 1.0016, 1.0031, 1.0005, 1.0001,\n",
      "         1.0005, 1.0057, 1.0011, 1.0016, 1.0015, 1.0037, 1.0050, 1.0014, 1.0027,\n",
      "         1.0015, 1.0010, 1.0003, 1.0008, 1.0004, 1.0005, 1.0027, 1.0001, 1.0081,\n",
      "         1.0038, 1.0559, 1.0054, 1.0004, 1.0013, 1.0010, 1.0016, 1.0012, 1.0019,\n",
      "         1.0045]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 43, 43])  attentions_grads shape: torch.Size([3, 2, 12, 43, 43])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 43, 43])\n",
      "joint_attentions shape: torch.Size([2, 43, 43])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0060, 1.0182, 1.0017, 1.0058, 1.0124, 1.0007, 1.0026,\n",
      "         1.0050, 1.0010, 1.0024, 1.0034, 1.0184, 1.0010, 1.0014, 1.0326, 1.0026,\n",
      "         1.0056, 1.0061, 1.0012, 1.0032, 1.0019, 1.0158, 1.0350, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0009, 1.0013, 1.0020, 1.0006, 1.0029, 1.0064, 1.0007,\n",
      "         1.0067, 1.0007, 1.0006, 1.0018, 1.0032, 1.0004, 1.0037, 1.0030, 1.0055,\n",
      "         1.0006, 1.0001, 1.0005, 1.0007, 1.0102, 1.0012, 1.0220, 1.0099, 1.0009,\n",
      "         1.0010, 1.0006, 1.0024, 1.0006, 1.0011, 1.0026, 1.0049, 1.0019, 1.0000,\n",
      "         1.0014, 1.0010, 1.0003, 1.0003, 1.0004, 1.0066, 1.0033]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 15, 15])  attentions_grads shape: torch.Size([3, 2, 12, 15, 15])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 15, 15])\n",
      "joint_attentions shape: torch.Size([2, 15, 15])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0115, 1.0078, 1.0012, 1.0434, 1.0021, 1.0027, 1.0111, 1.0038,\n",
      "         1.0151, 1.0043, 1.0185, 1.0005, 1.0087, 1.0121],\n",
      "        [0.0000, 1.0517, 1.0097, 1.0079, 1.0187, 1.0017, 1.0305, 1.0012, 1.0305,\n",
      "         1.0164, 1.0378, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 14, 14])  attentions_grads shape: torch.Size([3, 2, 12, 14, 14])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 14, 14])\n",
      "joint_attentions shape: torch.Size([2, 14, 14])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0064, 1.0076, 1.0072, 1.0846, 1.0069, 1.0229, 1.0327, 1.0357,\n",
      "         1.0190, 1.0046, 1.0058, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0108, 1.0085, 1.0080, 1.0965, 1.0025, 1.0073, 1.0141, 1.0037,\n",
      "         1.0057, 1.0113, 1.0131, 1.0073, 1.0099]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0087, 1.0034, 1.0036, 1.0054, 1.0021, 1.0000, 1.0026, 1.0003,\n",
      "         1.0166, 1.0029, 1.0067, 1.0012, 1.0001, 1.0218, 1.0023, 1.0081, 1.0028,\n",
      "         1.0051, 1.0035, 1.0026, 1.0035],\n",
      "        [0.0000, 1.0066, 1.0045, 1.0070, 1.0055, 1.0171, 1.0137, 1.0077, 1.0036,\n",
      "         1.0825, 1.0028, 1.0004, 1.0047, 1.0000, 1.0018, 1.0032, 1.0069, 1.0018,\n",
      "         1.0008, 1.0027, 1.0045, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0064, 1.0155, 1.0197, 1.0021, 1.0005, 1.0367, 1.0277, 1.0031,\n",
      "         1.0024, 1.0010, 1.0053, 1.0152, 1.0072, 1.0009, 1.0019, 1.0067, 1.0094,\n",
      "         1.0023, 1.0006],\n",
      "        [0.0000, 1.0018, 1.0852, 1.0081, 1.0029, 1.0049, 1.0025, 1.0173, 1.0061,\n",
      "         1.0035, 1.0070, 1.0355, 1.0215, 1.0016, 1.0055, 1.0041, 1.0113, 1.0216,\n",
      "         1.0023, 1.0008]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0046, 1.0020, 1.0136, 1.0269, 1.0022, 1.0005, 1.0013, 1.0024,\n",
      "         1.0050, 1.0048, 1.0024, 1.0024, 1.0015, 1.0427, 1.0002, 1.0026, 1.0009,\n",
      "         1.0011, 1.0498, 1.0010, 1.0008, 1.0015, 1.0133, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0010, 1.0023, 1.0021, 1.0002, 1.0017, 1.0050, 1.0021, 1.0007,\n",
      "         1.0006, 1.0362, 1.0016, 1.0003, 1.0005, 1.0222, 1.0030, 1.0032, 1.0033,\n",
      "         1.0040, 1.0014, 1.0134, 1.0014, 1.0045, 1.0014, 1.0024, 1.0309, 1.0027,\n",
      "         1.0017, 1.0003, 1.0045, 1.0003, 1.0017, 1.0013]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0074, 1.0018, 1.0197, 1.0014, 1.0155, 1.0097, 1.0038, 1.0071,\n",
      "         1.0060, 1.0008, 1.0034, 1.0056, 1.0063, 1.0080, 1.0023, 1.0018, 1.0008,\n",
      "         1.0539, 1.0038, 1.0165, 1.0019, 1.0094, 1.0029, 1.0036, 1.0037, 1.0009,\n",
      "         1.0002, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0149, 1.0005, 1.0018, 1.0016, 1.0013, 1.0019, 1.0085, 1.0429,\n",
      "         1.0010, 1.0083, 1.0697, 1.0008, 1.0002, 1.0016, 1.0049, 1.0073, 1.0022,\n",
      "         1.0024, 1.0035, 1.0033, 1.0012, 1.0168, 1.0005, 1.0003, 1.0036, 1.0056,\n",
      "         1.0016, 1.0021, 1.0022]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 38, 38])  attentions_grads shape: torch.Size([3, 2, 12, 38, 38])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 38, 38])\n",
      "joint_attentions shape: torch.Size([2, 38, 38])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0065, 1.0565, 1.0060, 1.0053, 1.0017, 1.0346, 1.0019, 1.0027,\n",
      "         1.0031, 1.0096, 1.0127, 1.0023, 1.0005, 1.0009, 1.0010, 1.0013, 1.0012,\n",
      "         1.0009, 1.0005, 1.0006, 1.0042, 1.0185, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0029, 1.0132, 1.0074, 1.0007, 1.0111, 1.0127, 1.0033, 1.0115,\n",
      "         1.0095, 1.0010, 1.0013, 1.0128, 1.0115, 1.0065, 1.0012, 1.0019, 1.0008,\n",
      "         1.0338, 1.0020, 1.0012, 1.0020, 1.0022, 1.0099, 1.0007, 1.0011, 1.0012,\n",
      "         1.0026, 1.0021, 1.0037, 1.0330, 1.0013, 1.0020, 1.0016, 1.0013, 1.0205,\n",
      "         1.0005, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0172, 1.0143, 1.0850, 1.0012, 1.0078, 1.0101, 1.0036, 1.0063,\n",
      "         1.0085, 1.0028, 1.0223, 1.0046, 1.0018, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0010, 1.0040, 1.0187, 1.0604, 1.0054, 1.0035, 1.0169, 1.0101,\n",
      "         1.0122, 1.0009, 1.0013, 1.0118, 1.0055, 1.0031, 1.0007, 1.0019, 1.0069,\n",
      "         1.0054, 1.0136, 1.0077, 1.0126, 1.0071, 1.0179, 1.0015, 1.0010]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0007, 1.0143, 1.0014, 1.0261, 1.0013, 1.0013, 1.0295, 1.0006,\n",
      "         1.0207, 1.0066, 1.0033, 1.0059, 1.0052, 1.0045, 1.0005, 1.0011, 1.0019,\n",
      "         1.0065, 1.0073, 1.0109, 1.0013, 1.0053, 1.0026, 1.0024, 1.0022, 1.0007,\n",
      "         1.0026, 1.0112, 1.0142, 1.0012, 1.0105, 1.0007, 1.0002],\n",
      "        [0.0000, 1.0068, 1.0396, 1.0052, 1.0037, 1.0016, 1.0045, 1.0104, 1.0042,\n",
      "         1.0007, 1.0283, 1.0031, 1.0137, 1.0058, 1.0276, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0033, 1.1034, 1.0102, 1.0029, 1.0249, 1.0189, 1.0055, 1.0122,\n",
      "         1.0345, 1.0024, 1.0032, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0027, 1.0268, 1.0010, 1.0009, 1.0009, 1.0137, 1.0025, 1.0010,\n",
      "         1.0013, 1.0016, 1.0010, 1.0024, 1.0050, 1.0034, 1.0014, 1.0014, 1.0030,\n",
      "         1.0007, 1.0677, 1.0004, 1.0006, 1.0006, 1.0011, 1.0005, 1.0072, 1.0001,\n",
      "         1.0007, 1.0001, 1.0000, 1.0005, 1.0034, 1.0047, 1.0001, 1.0003, 1.0002,\n",
      "         1.0008, 1.0007, 1.0084, 1.0167]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0351, 1.0015, 1.0031, 1.0152, 1.0226, 1.0026, 1.0052, 1.0079,\n",
      "         1.0215, 1.0093, 1.0018, 1.0574, 1.0004, 1.0010, 1.0247, 1.0092, 1.0502,\n",
      "         1.0017, 1.0004, 1.0000],\n",
      "        [0.0000, 1.0027, 1.0012, 1.0018, 1.0104, 1.0034, 1.0014, 1.0774, 1.0077,\n",
      "         1.0021, 1.0015, 1.0019, 1.0069, 1.0097, 1.0021, 1.0028, 1.0045, 1.0034,\n",
      "         1.0003, 1.0063, 1.0163]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0027, 1.0040, 1.0015, 1.0010, 1.0046, 1.0007, 1.0001,\n",
      "         1.0017, 1.0009, 1.0019, 1.0012, 1.0020, 1.0012, 1.0006, 1.0008, 1.0049,\n",
      "         1.0030, 1.0008, 1.0026, 1.0158, 1.0136, 1.0005, 1.0020, 1.0011, 1.0015,\n",
      "         1.0098, 1.0013, 1.0004, 1.0005, 1.0006, 1.0006, 1.0119, 1.0019, 1.0036],\n",
      "        [0.0000, 1.0071, 1.0097, 1.0079, 1.0094, 1.0029, 1.0028, 1.0129, 1.0037,\n",
      "         1.0634, 1.0281, 1.0287, 1.0151, 1.0274, 1.0020, 1.0016, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0008, 1.0006, 1.0032, 1.0118, 1.0012, 1.0006, 1.0170,\n",
      "         1.0017, 1.0027, 1.0021, 1.0009, 1.0026, 1.0309, 1.0012, 1.0019, 1.0005,\n",
      "         1.0005, 1.0037, 1.0344, 1.0018, 1.0002, 1.0020, 1.0011, 1.0002, 1.0017,\n",
      "         1.0011, 1.0013, 1.0003, 1.0009, 1.0015, 1.0013, 1.0020, 1.0181],\n",
      "        [0.0000, 1.1357, 1.0080, 1.0043, 1.0160, 1.0153, 1.0064, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 54, 54])  attentions_grads shape: torch.Size([3, 2, 12, 54, 54])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 54, 54])\n",
      "joint_attentions shape: torch.Size([2, 54, 54])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0052, 1.0035, 1.0021, 1.0034, 1.0611, 1.0035, 1.0004, 1.0103,\n",
      "         1.0016, 1.0064, 1.0020, 1.0082, 1.0016, 1.0031, 1.0107, 1.0026, 1.0089,\n",
      "         1.0093, 1.0052, 1.0010, 1.0012, 1.0032, 1.0020, 1.0009, 1.0044, 1.0020,\n",
      "         1.0022, 1.0006, 1.0043, 1.0078, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0222, 1.0025, 1.0004, 1.0011, 1.0057, 1.0002, 1.0005, 1.0053,\n",
      "         1.0011, 1.0008, 1.0001, 1.0006, 1.0009, 1.0020, 1.0005, 1.0022, 1.0022,\n",
      "         1.0095, 1.0004, 1.0019, 1.0004, 1.0008, 1.0005, 1.0076, 1.0007, 1.0105,\n",
      "         1.0005, 1.0008, 1.0002, 1.0005, 1.0017, 1.0010, 1.0002, 1.0031, 1.0037,\n",
      "         1.0004, 1.0040, 1.0052, 1.0007, 1.0014, 1.0122, 1.0004, 1.0066, 1.0009,\n",
      "         1.0001, 1.0024, 1.0005, 1.0008, 1.0011, 1.0001, 1.0291, 1.0144, 1.0025]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0024, 1.0378, 1.0037, 1.0034, 1.0154, 1.0135, 1.0110, 1.0622,\n",
      "         1.0025, 1.0237, 1.0087, 1.0104, 1.0310, 1.0094, 1.0027, 1.0013, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0212, 1.0173, 1.0012, 1.0006, 1.0025, 1.0025, 1.0026, 1.0004,\n",
      "         1.0136, 1.0030, 1.0008, 1.0009, 1.0003, 1.0018, 1.0006, 1.0058, 1.0006,\n",
      "         1.0029, 1.0115, 1.0042, 1.0017, 1.0012, 1.0010, 1.0024, 1.0070]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0007, 1.0094, 1.0018, 1.0044, 1.0018, 1.0014, 1.0013, 1.0012,\n",
      "         1.0010, 1.0010, 1.0053, 1.0011, 1.0009, 1.0009, 1.0003, 1.0001, 1.0005,\n",
      "         1.0217, 1.0005, 1.0010, 1.0016, 1.0506, 1.0022, 1.0025, 1.0014, 1.0119,\n",
      "         1.0004, 1.0020, 1.0033, 1.0076, 1.0006, 1.0002, 1.0009, 1.0009, 1.0047,\n",
      "         1.0014, 1.0022, 1.0017],\n",
      "        [0.0000, 1.0178, 1.0141, 1.0300, 1.0054, 1.0037, 1.0017, 1.0035, 1.0081,\n",
      "         1.0085, 1.0875, 1.0134, 1.0231, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 16, 16])  attentions_grads shape: torch.Size([3, 2, 12, 16, 16])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 16, 16])\n",
      "joint_attentions shape: torch.Size([2, 16, 16])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0262, 1.0031, 1.0054, 1.0656, 1.0221, 1.0165, 1.0053, 1.0360,\n",
      "         1.0008, 1.0247, 1.0095, 1.0061, 1.0041, 1.0020, 1.0000],\n",
      "        [0.0000, 1.0230, 1.0021, 1.0025, 1.0133, 1.0021, 1.0284, 1.0024, 1.0234,\n",
      "         1.0241, 1.0017, 1.0046, 1.0025, 1.0303, 1.0022, 1.0010]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 12, 12])  attentions_grads shape: torch.Size([3, 2, 12, 12, 12])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 12, 12])\n",
      "joint_attentions shape: torch.Size([2, 12, 12])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0102, 1.0074, 1.0044, 1.0458, 1.0177, 1.0293, 1.0090, 1.0436,\n",
      "         1.0568, 1.0099, 1.0025],\n",
      "        [0.0000, 1.0430, 1.0023, 1.0118, 1.0176, 1.0090, 1.0007, 1.0040, 1.0098,\n",
      "         1.0127, 1.0254, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0027, 1.0113, 1.0090, 1.0034, 1.0006, 1.0010, 1.0085, 1.0028,\n",
      "         1.0061, 1.0031, 1.0009, 1.0024, 1.0156, 1.0025, 1.0220, 1.0013, 1.0065,\n",
      "         1.0064, 1.0008, 1.0082, 1.0286, 1.0005, 1.0015, 1.0078, 1.0009, 1.0010],\n",
      "        [0.0000, 1.0061, 1.0012, 1.0018, 1.0111, 1.0020, 1.0013, 1.0034, 1.0003,\n",
      "         1.0028, 1.0027, 1.0008, 1.0000, 1.0005, 1.0021, 1.0008, 1.0000, 1.0044,\n",
      "         1.0006, 1.0023, 1.0318, 1.0006, 1.0029, 1.0002, 1.0006, 1.0017, 1.0002]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0023, 1.0375, 1.0050, 1.0030, 1.0038, 1.0055, 1.0178, 1.0017,\n",
      "         1.0011, 1.0504, 1.0096, 1.0020, 1.0023, 1.0026, 1.0201, 1.0021, 1.0129,\n",
      "         1.0033, 1.0329, 1.0038, 1.0005],\n",
      "        [0.0000, 1.0152, 1.0017, 1.0261, 1.0019, 1.0022, 1.0046, 1.0023, 1.0016,\n",
      "         1.0030, 1.0340, 1.0016, 1.0259, 1.0013, 1.0014, 1.0014, 1.0048, 1.0030,\n",
      "         1.0172, 1.0006, 1.0002, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0057, 1.0032, 1.0060, 1.0104, 1.0042, 1.0055, 1.0039, 1.0051,\n",
      "         1.0008, 1.0006, 1.0022, 1.0005, 1.0183, 1.0013, 1.0018, 1.0115, 1.0116,\n",
      "         1.0087, 1.0072, 1.0005, 1.0064, 1.0020, 1.0129, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0183, 1.0014, 1.0020, 1.0194, 1.0024, 1.0006, 1.0004, 1.0017,\n",
      "         1.0009, 1.0014, 1.0017, 1.0031, 1.0001, 1.0042, 1.0081, 1.0010, 1.0014,\n",
      "         1.0006, 1.0017, 1.0005, 1.0010, 1.0010, 1.0066, 1.0041, 1.0079, 1.0075,\n",
      "         1.0008, 1.0002, 1.0005, 1.0015, 1.0099, 1.0001, 1.0006, 1.0030, 1.0033,\n",
      "         1.0148]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0072, 1.0024, 1.0084, 1.0087, 1.0049, 1.0053, 1.0051, 1.0026,\n",
      "         1.0012, 1.0113, 1.0014, 1.0017, 1.0053, 1.0030, 1.0513, 1.0010, 1.0022,\n",
      "         1.0017, 1.0015, 1.0024, 1.0013],\n",
      "        [0.0000, 1.0163, 1.0373, 1.0100, 1.0113, 1.0593, 1.0176, 1.0962, 1.0072,\n",
      "         1.0030, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0250, 1.0030, 1.0010, 1.0050, 1.0078, 1.0416, 1.0202, 1.0068,\n",
      "         1.0426, 1.0020, 1.0001, 1.0011, 1.0502, 1.0066, 1.0022, 1.0076, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0059, 1.0026, 1.0040, 1.0015, 1.0351, 1.0091, 1.0173, 1.0011,\n",
      "         1.0110, 1.0039, 1.0007, 1.0084, 1.0036, 1.0094, 1.0174, 1.0024, 1.0061,\n",
      "         1.0010, 1.0081, 1.0120, 1.0155, 1.0020, 1.0015, 1.0218]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0039, 1.0074, 1.0046, 1.0005, 1.0019, 1.0021, 1.0009, 1.0036,\n",
      "         1.0016, 1.0013, 1.0000, 1.0030, 1.0027, 1.0015, 1.0146, 1.0041, 1.0021,\n",
      "         1.0011, 1.0190, 1.0035, 1.0007, 1.0073, 1.0010, 1.0003, 1.0002, 1.0008,\n",
      "         1.0008, 1.0216, 1.0007, 1.0006, 1.0013, 1.0028],\n",
      "        [0.0000, 1.0075, 1.0605, 1.0095, 1.0090, 1.0079, 1.0033, 1.0097, 1.0041,\n",
      "         1.0036, 1.0067, 1.0000, 1.0020, 1.0062, 1.0240, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 52, 52])  attentions_grads shape: torch.Size([3, 2, 12, 52, 52])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 52, 52])\n",
      "joint_attentions shape: torch.Size([2, 52, 52])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0017, 1.0016, 1.0008, 1.0004, 1.0008, 1.0000, 1.0018, 1.0008,\n",
      "         1.0009, 1.0004, 1.0028, 1.0034, 1.0053, 1.0009, 1.0012, 1.0010, 1.0146,\n",
      "         1.0049, 1.0010, 1.0007, 1.0088, 1.0020, 1.0008, 1.0009, 1.0016, 1.0001,\n",
      "         1.0046, 1.0007, 1.0006, 1.0001, 1.0005, 1.0042, 1.0310, 1.0007, 1.0016,\n",
      "         1.0283, 1.0002, 1.0017, 1.0002, 1.0007, 1.0006, 1.0000, 1.0007, 1.0003,\n",
      "         1.0021, 1.0059, 1.0010, 1.0035, 1.0001, 1.0024, 1.0051],\n",
      "        [0.0000, 1.0011, 1.0222, 1.0035, 1.0210, 1.0040, 1.0029, 1.0017, 1.0042,\n",
      "         1.0184, 1.0084, 1.0114, 1.0015, 1.0089, 1.0040, 1.0740, 1.0012, 1.0330,\n",
      "         1.0057, 1.0057, 1.0211, 1.0103, 1.0045, 1.0031, 1.0039, 1.0008, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0045, 1.0107, 1.0011, 1.0488, 1.0015, 1.0016, 1.0084, 1.0011,\n",
      "         1.0272, 1.0098, 1.0023, 1.0063, 1.0010, 1.0148, 1.0059, 1.0068, 1.0022,\n",
      "         1.0144, 1.0015, 1.0116, 1.0015, 1.0040, 1.0011, 1.0093, 1.0024, 1.0003,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0003, 1.0026, 1.0233, 1.0092, 1.0011, 1.0010, 1.0009, 1.0007,\n",
      "         1.0012, 1.0017, 1.0188, 1.0135, 1.0069, 1.0006, 1.0054, 1.0283, 1.0018,\n",
      "         1.0011, 1.0032, 1.0096, 1.0017, 1.0016, 1.0092, 1.0067, 1.0022, 1.0013,\n",
      "         1.0005]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0014, 1.0200, 1.0117, 1.0001, 1.0071, 1.0213, 1.0011, 1.0073,\n",
      "         1.0022, 1.0016, 1.0163, 1.0080, 1.0059, 1.0063, 1.0087, 1.0079, 1.0033,\n",
      "         1.0033, 1.0041, 1.0005, 1.0034, 1.0025, 1.0379, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0139, 1.0121, 1.0027, 1.0014, 1.0038, 1.0026, 1.0029,\n",
      "         1.0094, 1.0017, 1.0106, 1.0057, 1.0013, 1.0010, 1.0020, 1.0045, 1.0055,\n",
      "         1.0016, 1.0069, 1.0326, 1.0032, 1.0017, 1.0030, 1.0006, 1.0027, 1.0004,\n",
      "         1.0134, 1.0333]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0325, 1.0279, 1.0025, 1.0107, 1.0032, 1.0021, 1.0065, 1.0005,\n",
      "         1.0017, 1.0095, 1.0007, 1.0008, 1.0008, 1.0019, 1.0011, 1.0165, 1.0013,\n",
      "         1.0003, 1.0004, 1.0152, 1.0025, 1.0021, 1.0014, 1.0005, 1.0024, 1.0006,\n",
      "         1.0002, 1.0028, 1.0007, 1.0026, 1.0001, 1.0002, 1.0079, 1.0030, 1.0015,\n",
      "         1.0000, 1.0003, 1.0014, 1.0011, 1.0011],\n",
      "        [0.0000, 1.0065, 1.0109, 1.0013, 1.0321, 1.0010, 1.0120, 1.0020, 1.0193,\n",
      "         1.0054, 1.0012, 1.0010, 1.0328, 1.0005, 1.0048, 1.0097, 1.0006, 1.0009,\n",
      "         1.0015, 1.0020, 1.0090, 1.0008, 1.0010, 1.0015, 1.0017, 1.0006, 1.0042,\n",
      "         1.0007, 1.0248, 1.0010, 1.0057, 1.0017, 1.0007, 1.0015, 1.0081, 1.0043,\n",
      "         1.0006, 1.0002, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0214, 1.0007, 1.0008, 1.0165, 1.0003, 1.0021, 1.0225,\n",
      "         1.0058, 1.0020, 1.0075, 1.0027, 1.0422, 1.0007, 1.0213, 1.0008, 1.0009,\n",
      "         1.0039, 1.0024, 1.0046, 1.0029, 1.0044, 1.0080, 1.0027, 1.0110, 1.0066,\n",
      "         1.0009, 1.0003],\n",
      "        [0.0000, 1.0064, 1.0195, 1.0026, 1.0029, 1.0100, 1.0023, 1.0084, 1.0242,\n",
      "         1.0012, 1.0269, 1.0054, 1.0018, 1.0212, 1.0007, 1.0007, 1.0128, 1.0049,\n",
      "         1.0024, 1.0024, 1.0036, 1.0121, 1.0006, 1.0098, 1.0075, 1.0108, 1.0062,\n",
      "         1.0008, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0024, 1.0007, 1.0008, 1.0220, 1.0096, 1.0055, 1.0007, 1.0085,\n",
      "         1.0043, 1.0189, 1.0012, 1.0028, 1.0007, 1.0006, 1.0049, 1.0152, 1.0185,\n",
      "         1.0017, 1.0161, 1.0012, 1.0057, 1.0009, 1.0193, 1.0117, 1.0010, 1.0043,\n",
      "         1.0008, 1.0002],\n",
      "        [0.0000, 1.0096, 1.0527, 1.0235, 1.0331, 1.0254, 1.0261, 1.0220, 1.0049,\n",
      "         1.0022, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0029, 1.0037, 1.0135, 1.0030, 1.0020, 1.0035, 1.0086, 1.0020,\n",
      "         1.0068, 1.0314, 1.0054, 1.0008, 1.0013, 1.0037, 1.0022, 1.0014, 1.0113,\n",
      "         1.0405, 1.0017, 1.0005],\n",
      "        [0.0000, 1.0173, 1.0084, 1.0036, 1.0025, 1.0077, 1.0090, 1.0034, 1.0814,\n",
      "         1.0356, 1.0073, 1.0383, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 38, 38])  attentions_grads shape: torch.Size([3, 2, 12, 38, 38])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 38, 38])\n",
      "joint_attentions shape: torch.Size([2, 38, 38])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0013, 1.0013, 1.0011, 1.0079, 1.0145, 1.0101, 1.0013, 1.0121,\n",
      "         1.0128, 1.0035, 1.0039, 1.0106, 1.0169, 1.0167, 1.0117, 1.0078, 1.0036,\n",
      "         1.0011, 1.0010, 1.0054, 1.0022, 1.0036, 1.0015, 1.0449, 1.0013, 1.0075,\n",
      "         1.0083, 1.0092, 1.0045, 1.0010, 1.0023, 1.0233, 1.0018, 1.0007, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0008, 1.0005, 1.0025, 1.0020, 1.0041, 1.0014, 1.0022, 1.0066,\n",
      "         1.0022, 1.0029, 1.0120, 1.0011, 1.0006, 1.0008, 1.0039, 1.0002, 1.0051,\n",
      "         1.0083, 1.0008, 1.0006, 1.0010, 1.0003, 1.0002, 1.0036, 1.0008, 1.0048,\n",
      "         1.0023, 1.0021, 1.0172, 1.0006, 1.0008, 1.0038, 1.0113, 1.0005, 1.0050,\n",
      "         1.0024, 1.0036]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0338, 1.0093, 1.0011, 1.0327, 1.0007, 1.0007, 1.0005,\n",
      "         1.0007, 1.0232, 1.0065, 1.0184, 1.0005, 1.0111, 1.0089, 1.0004, 1.0006,\n",
      "         1.0042, 1.0049, 1.0019, 1.0021, 1.0032, 1.0040, 1.0015, 1.0021, 1.0107,\n",
      "         1.0058, 1.0020, 1.0031, 1.0032, 1.0068, 1.0028, 1.0015, 1.0022, 1.0004,\n",
      "         1.0086, 1.0011, 1.0011, 1.0005, 1.0002],\n",
      "        [0.0000, 1.0179, 1.0075, 1.0053, 1.0025, 1.0024, 1.0035, 1.0031, 1.0043,\n",
      "         1.0020, 1.0055, 1.0010, 1.0006, 1.0208, 1.0044, 1.0005, 1.0005, 1.0147,\n",
      "         1.0025, 1.0033, 1.0083, 1.0018, 1.0008, 1.0011, 1.0079, 1.0015, 1.0015,\n",
      "         1.0025, 1.0009, 1.0013, 1.0017, 1.0111, 1.0012, 1.0142, 1.0158, 1.0008,\n",
      "         1.0015, 1.0015, 1.0006, 1.0001, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 19, 19])  attentions_grads shape: torch.Size([3, 2, 12, 19, 19])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 19, 19])\n",
      "joint_attentions shape: torch.Size([2, 19, 19])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0064, 1.0156, 1.0046, 1.0047, 1.0028, 1.0030, 1.0033, 1.0082,\n",
      "         1.0184, 1.0031, 1.0031, 1.0063, 1.0000, 1.0048, 1.0096, 1.0433, 1.0066,\n",
      "         1.0237],\n",
      "        [0.0000, 1.1364, 1.0437, 1.0347, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0011, 1.0045, 1.0008, 1.0403, 1.0031, 1.0006, 1.0105, 1.0127,\n",
      "         1.0069, 1.0005, 1.0129, 1.0007, 1.0057, 1.0012, 1.0006, 1.0160, 1.0143,\n",
      "         1.0011, 1.0004, 1.0016, 1.0011, 1.0008, 1.0069, 1.0056, 1.0370, 1.0099,\n",
      "         1.0014, 1.0028, 1.0045, 1.0012, 1.0009, 1.0008, 1.0003],\n",
      "        [0.0000, 1.0410, 1.0156, 1.0020, 1.0154, 1.0005, 1.0016, 1.0402, 1.0026,\n",
      "         1.0197, 1.0011, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0051, 1.0213, 1.0262, 1.0059, 1.0261, 1.0056, 1.0034, 1.0264,\n",
      "         1.0107, 1.0227, 1.0175, 1.0014, 1.0027, 1.0261, 1.0030, 1.0076, 1.0173,\n",
      "         1.0018, 1.0027],\n",
      "        [0.0000, 1.0286, 1.0210, 1.0745, 1.0094, 1.0052, 1.0105, 1.0126, 1.0193,\n",
      "         1.0327, 1.0017, 1.0017, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0105, 1.0096, 1.0021, 1.0101, 1.0110, 1.0043, 1.0125, 1.0113,\n",
      "         1.0091, 1.0480, 1.0020, 1.0085, 1.0303, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0008, 1.0022, 1.0535, 1.0007, 1.0003, 1.0014, 1.0002, 1.0551,\n",
      "         1.0002, 1.0094, 1.0011, 1.0028, 1.0014, 1.0005, 1.0012, 1.0041, 1.0008,\n",
      "         1.0281, 1.0005, 1.0020, 1.0014, 1.0005, 1.0008, 1.0152, 1.0054, 1.0014,\n",
      "         1.0039, 1.0014, 1.0010]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0070, 1.0070, 1.0011, 1.0016, 1.0108, 1.0006, 1.0231, 1.0011,\n",
      "         1.0023, 1.0034, 1.0112, 1.0010, 1.0027, 1.0008, 1.0063, 1.0005, 1.0204,\n",
      "         1.0025, 1.0021, 1.0143, 1.0013, 1.0008, 1.0005, 1.0307, 1.0033, 1.0006,\n",
      "         1.0008, 1.0190, 1.0035, 1.0013, 1.0009, 1.0076, 1.0025, 1.0061, 1.0003,\n",
      "         1.0006, 1.0030, 1.0004, 1.0010, 1.0008, 1.0011, 1.0002, 1.0001],\n",
      "        [0.0000, 1.0046, 1.0036, 1.0018, 1.0128, 1.0129, 1.0179, 1.0044, 1.0027,\n",
      "         1.0023, 1.0036, 1.0015, 1.0181, 1.0017, 1.0059, 1.0027, 1.0112, 1.0079,\n",
      "         1.0015, 1.0048, 1.0013, 1.0019, 1.0011, 1.0047, 1.0077, 1.0124, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0256, 1.0020, 1.0120, 1.0041, 1.0025, 1.0020, 1.0432, 1.0047,\n",
      "         1.0133, 1.0063, 1.0095, 1.0355, 1.0031, 1.0077, 1.0035, 1.0346, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0018, 1.0039, 1.0104, 1.0026, 1.0045, 1.0020, 1.0613, 1.0002,\n",
      "         1.0017, 1.0006, 1.0148, 1.0021, 1.0003, 1.0008, 1.0030, 1.0010, 1.0005,\n",
      "         1.0146, 1.0007, 1.0009, 1.0004, 1.0054, 1.0116, 1.0018, 1.0009, 1.0009,\n",
      "         1.0007, 1.0004, 1.0003, 1.0002, 1.0007, 1.0011, 1.0395]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0004, 1.0040, 1.0013, 1.0005, 1.0004, 1.0010, 1.0014, 1.0003,\n",
      "         1.0014, 1.0003, 1.0009, 1.0015, 1.0072, 1.0006, 1.0006, 1.0009, 1.0101,\n",
      "         1.0101, 1.0009, 1.0071, 1.0487, 1.0011, 1.0158, 1.0026, 1.0046, 1.0013,\n",
      "         1.0026, 1.0016, 1.0051, 1.0058, 1.0021, 1.0003, 1.0010, 1.0055, 1.0000,\n",
      "         1.0002, 1.0013, 1.0007, 1.0056, 1.0003, 1.0131, 1.0017, 1.0030],\n",
      "        [0.0000, 1.0019, 1.0343, 1.0010, 1.0025, 1.0056, 1.0021, 1.0253, 1.0113,\n",
      "         1.0033, 1.0009, 1.0019, 1.0022, 1.0021, 1.0104, 1.0009, 1.0030, 1.0103,\n",
      "         1.0016, 1.0004, 1.0037, 1.0077, 1.0088, 1.0011, 1.0014, 1.0334, 1.0056,\n",
      "         1.0013, 1.0118, 1.0123, 1.0015, 1.0006, 1.0002, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0016, 1.0083, 1.0389, 1.0040, 1.0057, 1.0176, 1.0005, 1.0062,\n",
      "         1.0122, 1.0014, 1.0128, 1.0006, 1.0037, 1.0341, 1.0027, 1.0042, 1.0171,\n",
      "         1.0143, 1.0018, 1.0007, 1.0025, 1.0164, 1.0035, 1.0061, 1.0010, 1.0051,\n",
      "         1.0252, 1.0006, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0098, 1.0019, 1.0056, 1.0069, 1.0013, 1.0023, 1.0104, 1.0005,\n",
      "         1.0006, 1.0006, 1.0059, 1.0019, 1.0004, 1.0018, 1.0023, 1.0019, 1.0061,\n",
      "         1.0003, 1.0012, 1.0052, 1.0004, 1.0075, 1.0046, 1.0001, 1.0011, 1.0014,\n",
      "         1.0472, 1.0011, 1.0006, 1.0006, 1.0032, 1.0010, 1.0090, 1.0005, 1.0002,\n",
      "         1.0004, 1.0020, 1.0001, 1.0028, 1.0037]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0291, 1.0033, 1.0032, 1.0034, 1.0516, 1.0047, 1.0017, 1.0091,\n",
      "         1.0054, 1.0048, 1.0012, 1.0029, 1.0006, 1.0012, 1.0026, 1.0111, 1.0052,\n",
      "         1.0041, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0028, 1.0009, 1.0034, 1.0248, 1.0010, 1.0091, 1.0103, 1.0019,\n",
      "         1.0019, 1.0055, 1.0010, 1.0010, 1.0036, 1.0006, 1.0006, 1.0026, 1.0009,\n",
      "         1.0057, 1.0017, 1.0376, 1.0008, 1.0060, 1.0011, 1.0097, 1.0010, 1.0114,\n",
      "         1.0010, 1.0007, 1.0071, 1.0007, 1.0268, 1.0208, 1.0005, 1.0003]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0013, 1.0063, 1.0010, 1.0012, 1.0017, 1.0012, 1.0056, 1.0384,\n",
      "         1.0011, 1.0019, 1.0015, 1.0170, 1.0213, 1.0013, 1.0009, 1.0010, 1.0008,\n",
      "         1.0024, 1.0015, 1.0066, 1.0081, 1.0006, 1.0058, 1.0121, 1.0041, 1.0070,\n",
      "         1.0022, 1.0035, 1.0021, 1.0080, 1.0010, 1.0003],\n",
      "        [0.0000, 1.0049, 1.0194, 1.0184, 1.0048, 1.0073, 1.0269, 1.0832, 1.0077,\n",
      "         1.0397, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0009, 1.0085, 1.0400, 1.0035, 1.0015, 1.0143, 1.0026, 1.0266,\n",
      "         1.0030, 1.0104, 1.0029, 1.0024, 1.0118, 1.0006, 1.0119, 1.0054, 1.0022,\n",
      "         1.0069, 1.0048, 1.0035, 1.0007, 1.0007, 1.0026, 1.0069, 1.0010, 1.0013,\n",
      "         1.0211, 1.0120, 1.0005, 1.0103, 1.0039, 1.0088, 1.0005, 1.0002],\n",
      "        [0.0000, 1.0055, 1.0037, 1.0018, 1.0024, 1.0010, 1.0039, 1.0045, 1.0014,\n",
      "         1.0064, 1.0013, 1.0052, 1.0512, 1.0490, 1.0009, 1.0016, 1.0038, 1.0009,\n",
      "         1.0643, 1.0055, 1.0032, 1.0019, 1.0303, 1.0009, 1.0004, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0078, 1.0006, 1.0004, 1.0004, 1.0056, 1.0007, 1.0011, 1.0004,\n",
      "         1.0105, 1.0017, 1.0071, 1.0003, 1.0594, 1.0101, 1.0032, 1.0039, 1.0006,\n",
      "         1.0112, 1.0106, 1.0005, 1.0004, 1.0505, 1.0010, 1.0017, 1.0046, 1.0022,\n",
      "         1.0118, 1.0003, 1.0039, 1.0005, 1.0295, 1.0003, 1.0002],\n",
      "        [0.0000, 1.0087, 1.0076, 1.0248, 1.0029, 1.0177, 1.0015, 1.0014, 1.0030,\n",
      "         1.0082, 1.0179, 1.0013, 1.0016, 1.0053, 1.0083, 1.0011, 1.0098, 1.0006,\n",
      "         1.0028, 1.0031, 1.0051, 1.0080, 1.0026, 1.0019, 1.0004, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0102, 1.0196, 1.0047, 1.0050, 1.0090, 1.0111, 1.0005, 1.0013,\n",
      "         1.0066, 1.0041, 1.0051, 1.0024, 1.0657, 1.0128, 1.0074, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0007, 1.0143, 1.0057, 1.0078, 1.0084, 1.0017, 1.0256, 1.0044,\n",
      "         1.0036, 1.0125, 1.0018, 1.0411, 1.0120, 1.0007, 1.0181, 1.0044, 1.0017,\n",
      "         1.0270, 1.0106, 1.0452, 1.0010, 1.0010]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0045, 1.0490, 1.0069, 1.0228, 1.0029, 1.0008, 1.0018, 1.0052,\n",
      "         1.0141, 1.0116, 1.0244, 1.0012, 1.0237, 1.0244, 1.0014, 1.0265, 1.0139,\n",
      "         1.0030, 1.0009],\n",
      "        [0.0000, 1.0049, 1.0193, 1.0151, 1.0115, 1.0085, 1.0232, 1.0014, 1.0043,\n",
      "         1.0167, 1.0081, 1.0135, 1.0121, 1.0068, 1.0206, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0152, 1.0016, 1.0247, 1.0053, 1.0028, 1.0029, 1.0053, 1.0012,\n",
      "         1.0039, 1.0108, 1.0207, 1.0228, 1.0033, 1.0052, 1.0146, 1.0011, 1.0629,\n",
      "         1.0261, 1.0025, 1.0284, 1.0015, 1.0018, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0021, 1.0044, 1.0031, 1.0009, 1.0047, 1.0021, 1.0005, 1.0004,\n",
      "         1.0036, 1.0353, 1.0013, 1.0055, 1.0031, 1.0007, 1.0008, 1.0024, 1.0011,\n",
      "         1.0176, 1.0051, 1.0002, 1.0375, 1.0059, 1.0073, 1.0056, 1.0060, 1.0017,\n",
      "         1.0024, 1.0066, 1.0014, 1.0028, 1.0020, 1.0008, 1.0012, 1.0027, 1.0012,\n",
      "         1.0020, 1.0005, 1.0389, 1.0005, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 53, 53])  attentions_grads shape: torch.Size([3, 2, 12, 53, 53])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 53, 53])\n",
      "joint_attentions shape: torch.Size([2, 53, 53])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0018, 1.0013, 1.0019, 1.0500, 1.0007, 1.0013, 1.0017, 1.0010,\n",
      "         1.0006, 1.0013, 1.0058, 1.0023, 1.0005, 1.0004, 1.0008, 1.0033, 1.0013,\n",
      "         1.0011, 1.0002, 1.0012, 1.0014, 1.0004, 1.0007, 1.0039, 1.0028, 1.0057,\n",
      "         1.0022, 1.0011, 1.0398, 1.0004, 1.0000, 1.0036, 1.0005, 1.0024, 1.0007,\n",
      "         1.0068, 1.0006, 1.0012, 1.0114, 1.0011, 1.0075, 1.0005, 1.0004, 1.0009,\n",
      "         1.0014, 1.0015, 1.0021, 1.0052, 1.0001, 1.0006, 1.0011, 1.0014],\n",
      "        [0.0000, 1.0144, 1.0050, 1.0205, 1.0182, 1.0032, 1.0108, 1.0022, 1.0164,\n",
      "         1.0023, 1.0029, 1.0049, 1.0064, 1.0045, 1.0287, 1.0252, 1.0041, 1.0053,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0287, 1.0082, 1.0143, 1.0076, 1.0230, 1.0026, 1.0506, 1.0164,\n",
      "         1.0357, 1.0133, 1.0024, 1.0037, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0024, 1.0104, 1.0018, 1.0018, 1.0053, 1.0014, 1.0014, 1.0005,\n",
      "         1.0034, 1.0011, 1.0010, 1.0201, 1.0106, 1.0119, 1.0004, 1.0149, 1.0112,\n",
      "         1.0026, 1.0008, 1.0076, 1.0088, 1.0004, 1.0054, 1.0064, 1.0029, 1.0010,\n",
      "         1.0016, 1.0143, 1.0003, 1.0014, 1.0085, 1.0009, 1.0011, 1.0109, 1.0003,\n",
      "         1.0004, 1.0011, 1.0004, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0101, 1.0012, 1.0113, 1.0128, 1.0009, 1.0021, 1.0087, 1.0049,\n",
      "         1.0011, 1.0007, 1.0004, 1.0003, 1.0103, 1.0034, 1.0011, 1.0005, 1.0018,\n",
      "         1.0024, 1.0019, 1.0022, 1.0100, 1.0008, 1.0035, 1.0018, 1.0271, 1.0031,\n",
      "         1.0025, 1.0004, 1.0032, 1.0127, 1.0013, 1.0008, 1.0003],\n",
      "        [0.0000, 1.0020, 1.0021, 1.0061, 1.0055, 1.0034, 1.0325, 1.0027, 1.0021,\n",
      "         1.0066, 1.0011, 1.0123, 1.0305, 1.0074, 1.0034, 1.0028, 1.0031, 1.0039,\n",
      "         1.0009, 1.0027, 1.0062, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0029, 1.0015, 1.0143, 1.0027, 1.0064, 1.0131, 1.0226, 1.0023,\n",
      "         1.0051, 1.0047, 1.0148, 1.0078, 1.0012, 1.0043, 1.0047, 1.0065, 1.0032,\n",
      "         1.0397, 1.0000],\n",
      "        [0.0000, 1.0322, 1.0084, 1.0315, 1.0103, 1.0131, 1.0049, 1.0065, 1.0066,\n",
      "         1.0022, 1.0018, 1.0536, 1.0147, 1.0125, 1.0066, 1.0095, 1.0007, 1.0292,\n",
      "         1.0016, 1.0006]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0263, 1.0061, 1.0029, 1.0013, 1.0234, 1.0096, 1.0019, 1.0038,\n",
      "         1.0236, 1.0337, 1.0189, 1.0016, 1.0133, 1.0034, 1.0051, 1.0013, 1.0268,\n",
      "         1.0229, 1.0051, 1.0013, 1.0008],\n",
      "        [0.0000, 1.0084, 1.0089, 1.0036, 1.0033, 1.0057, 1.0292, 1.0335, 1.0036,\n",
      "         1.0032, 1.0031, 1.0049, 1.0123, 1.0011, 1.0125, 1.0243, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 16, 16])  attentions_grads shape: torch.Size([3, 2, 12, 16, 16])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 16, 16])\n",
      "joint_attentions shape: torch.Size([2, 16, 16])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0041, 1.0234, 1.0031, 1.0278, 1.0012, 1.0023, 1.0004, 1.0167,\n",
      "         1.0516, 1.0146, 1.0378, 1.0074, 1.0225, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0110, 1.0079, 1.1021, 1.0011, 1.0029, 1.0203, 1.0038, 1.0053,\n",
      "         1.0085, 1.0024, 1.0359, 1.0168, 1.0320, 1.0031, 1.0005]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0007, 1.0090, 1.0012, 1.0013, 1.0027, 1.0163, 1.0008, 1.0034,\n",
      "         1.0041, 1.0009, 1.0036, 1.0022, 1.0080, 1.0267, 1.0048, 1.0022, 1.0016,\n",
      "         1.0020, 1.0181, 1.0369, 1.0008, 1.0085, 1.0052, 1.0071, 1.0009, 1.0032,\n",
      "         1.0065, 1.0052, 1.0012, 1.0072, 1.0009, 1.0006, 1.0343, 1.0013, 1.0009,\n",
      "         1.0082, 1.0006, 1.0002],\n",
      "        [0.0000, 1.0043, 1.0026, 1.0026, 1.0011, 1.0111, 1.0008, 1.0035, 1.0102,\n",
      "         1.0023, 1.0014, 1.0043, 1.0009, 1.0112, 1.0007, 1.0140, 1.0053, 1.0010,\n",
      "         1.0107, 1.0015, 1.0024, 1.0064, 1.0011, 1.0086, 1.0161, 1.0111, 1.0014,\n",
      "         1.0477, 1.0069, 1.0012, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0007, 1.0010, 1.0040, 1.0246, 1.0044, 1.0078, 1.0022,\n",
      "         1.0021, 1.0146, 1.0026, 1.0167, 1.0118, 1.0040, 1.0235, 1.0029, 1.0030,\n",
      "         1.0082, 1.0025, 1.0035, 1.0022, 1.0021, 1.0004, 1.0075, 1.0064, 1.0008,\n",
      "         1.0016, 1.0021, 1.0004, 1.0004, 1.0042, 1.0149, 1.0025, 1.0006, 1.0075,\n",
      "         1.0005, 1.0004, 1.0241, 1.0009, 1.0001, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0126, 1.0014, 1.0015, 1.0010, 1.0016, 1.0012, 1.0004, 1.0064,\n",
      "         1.0031, 1.0033, 1.0039, 1.0085, 1.0007, 1.0035, 1.0221, 1.0030, 1.0033,\n",
      "         1.0032, 1.0005, 1.0005, 1.0005, 1.0002, 1.0017, 1.0007, 1.0003, 1.0035,\n",
      "         1.0011, 1.0017, 1.0004, 1.0017, 1.0014, 1.0003, 1.0009, 1.0050, 1.0018,\n",
      "         1.0028, 1.0076, 1.0039, 1.0029, 1.0183, 1.0058, 1.0045, 1.0114]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0011, 1.0077, 1.0202, 1.0076, 1.0020, 1.0026, 1.0018, 1.0058,\n",
      "         1.0019, 1.0090, 1.0042, 1.0027, 1.0008, 1.0190, 1.0463, 1.0006, 1.0258,\n",
      "         1.0039, 1.0010, 1.0012, 1.0059, 1.0179, 1.0009, 1.0006],\n",
      "        [0.0000, 1.0049, 1.0055, 1.0256, 1.0050, 1.0048, 1.0130, 1.0015, 1.0088,\n",
      "         1.0212, 1.0065, 1.0284, 1.0132, 1.0008, 1.0197, 1.0102, 1.0011, 1.0004,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0142, 1.0019, 1.0079, 1.0201, 1.0441, 1.0317, 1.0032, 1.0013,\n",
      "         1.0079, 1.0021, 1.0042, 1.0408, 1.0040, 1.0238, 1.0056, 1.0016, 1.0000],\n",
      "        [0.0000, 1.0123, 1.0010, 1.0313, 1.0459, 1.0114, 1.0013, 1.0011, 1.0117,\n",
      "         1.0016, 1.0170, 1.0208, 1.0109, 1.0011, 1.0017, 1.0036, 1.0020, 1.0063]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0657, 1.0087, 1.0032, 1.0096, 1.0276, 1.0327, 1.0058, 1.0395,\n",
      "         1.0183, 1.0071, 1.0063, 1.0029, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0041, 1.0029, 1.0006, 1.0062, 1.0012, 1.0016, 1.0059, 1.0051,\n",
      "         1.0010, 1.0038, 1.0029, 1.0016, 1.0016, 1.0011, 1.0045, 1.0075, 1.0122,\n",
      "         1.0088, 1.0059, 1.0169, 1.0007, 1.0038, 1.0043, 1.0009, 1.0106, 1.0006,\n",
      "         1.0027, 1.0030, 1.0042, 1.0015, 1.0207]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0315, 1.0011, 1.0015, 1.0009, 1.0006, 1.0119, 1.0053, 1.0004,\n",
      "         1.0018, 1.0012, 1.0017, 1.0066, 1.0384, 1.0028, 1.0011, 1.0011, 1.0012,\n",
      "         1.0276, 1.0080, 1.0106, 1.0012, 1.0012, 1.0124, 1.0136],\n",
      "        [0.0000, 1.0701, 1.0021, 1.0013, 1.0007, 1.0009, 1.0087, 1.0023, 1.0012,\n",
      "         1.0002, 1.0039, 1.0092, 1.0002, 1.0094, 1.0068, 1.0037, 1.0013, 1.0081,\n",
      "         1.0057, 1.0011, 1.0016, 1.0008, 1.0359, 1.0062, 1.0197]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0386, 1.0028, 1.0151, 1.0034, 1.0239, 1.0106, 1.0073, 1.0204,\n",
      "         1.0017, 1.0203, 1.0406, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0023, 1.0029, 1.0068, 1.0143, 1.0208, 1.0155, 1.0009, 1.0009,\n",
      "         1.0115, 1.0002, 1.0011, 1.0030, 1.0026, 1.0312, 1.0067, 1.0217, 1.0071,\n",
      "         1.0017, 1.0037, 1.0088]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0096, 1.0072, 1.0050, 1.0078, 1.0107, 1.0051, 1.0040, 1.0034,\n",
      "         1.0014, 1.0096, 1.0029, 1.0033, 1.0471, 1.0187, 1.0056, 1.0187, 1.0078,\n",
      "         1.0290, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0007, 1.0056, 1.0204, 1.0116, 1.0097, 1.0260, 1.0034, 1.0004,\n",
      "         1.0022, 1.0006, 1.0006, 1.0005, 1.0168, 1.0270, 1.0004, 1.0076, 1.0186,\n",
      "         1.0010, 1.0175, 1.0089, 1.0216, 1.0009, 1.0066, 1.0005, 1.0016, 1.0147,\n",
      "         1.0006, 1.0005, 1.0180, 1.0006, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0017, 1.0014, 1.0003, 1.0020, 1.0012, 1.0018, 1.0070, 1.0764,\n",
      "         1.0001, 1.0022, 1.0003, 1.0088, 1.0039, 1.0020, 1.0023, 1.0104, 1.0029,\n",
      "         1.0015, 1.0010, 1.0079, 1.0008, 1.0000, 1.0016, 1.0007, 1.0015, 1.0006,\n",
      "         1.0055, 1.0006, 1.0063, 1.0107],\n",
      "        [0.0000, 1.0020, 1.0071, 1.0292, 1.0012, 1.0112, 1.0007, 1.0061, 1.0015,\n",
      "         1.0068, 1.0015, 1.0069, 1.0051, 1.0020, 1.0009, 1.0824, 1.0260, 1.0129,\n",
      "         1.0024, 1.0042, 1.0051, 1.0067, 1.0100, 1.0024, 1.0008, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 48, 48])  attentions_grads shape: torch.Size([3, 2, 12, 48, 48])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 48, 48])\n",
      "joint_attentions shape: torch.Size([2, 48, 48])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0033, 1.0011, 1.0024, 1.0218, 1.0042, 1.0382, 1.0120, 1.0009,\n",
      "         1.0013, 1.0017, 1.0005, 1.0239, 1.0021, 1.0005, 1.0021, 1.0019, 1.0066,\n",
      "         1.0004, 1.0010, 1.0011, 1.0077, 1.0026, 1.0011, 1.0108, 1.0003, 1.0005,\n",
      "         1.0027, 1.0006, 1.0013, 1.0002, 1.0010, 1.0037, 1.0005, 1.0009, 1.0002,\n",
      "         1.0008, 1.0014, 1.0007, 1.0001, 1.0004, 1.0007, 1.0021, 1.0011, 1.0076,\n",
      "         1.0028, 1.0007, 1.0024],\n",
      "        [0.0000, 1.0709, 1.0068, 1.0412, 1.0023, 1.0117, 1.0062, 1.0050, 1.0154,\n",
      "         1.0096, 1.0010, 1.0032, 1.0144, 1.0041, 1.0022, 1.0014, 1.0012, 1.0190,\n",
      "         1.0031, 1.0020, 1.0019, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0050, 1.0105, 1.0039, 1.0052, 1.0462, 1.0060, 1.0018, 1.0009,\n",
      "         1.0126, 1.0035, 1.0046, 1.0018, 1.0037, 1.0038, 1.0019, 1.0029, 1.0014,\n",
      "         1.0000, 1.0145, 1.0011, 1.0014, 1.0041, 1.0016, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0125, 1.0015, 1.0015, 1.0016, 1.0076, 1.0336, 1.0043, 1.0010,\n",
      "         1.0012, 1.0041, 1.0010, 1.0002, 1.0004, 1.0006, 1.0016, 1.0062, 1.0022,\n",
      "         1.0008, 1.0010, 1.0096, 1.0011, 1.0006, 1.0013, 1.0006, 1.0020, 1.0018,\n",
      "         1.0140, 1.0007, 1.0080, 1.0306]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0006, 1.0209, 1.0101, 1.0008, 1.0070, 1.0144, 1.0018, 1.0023,\n",
      "         1.0097, 1.0054, 1.0030, 1.0032, 1.0045, 1.0007, 1.0036, 1.0005, 1.0078,\n",
      "         1.0005, 1.0003, 1.0007, 1.0049, 1.0034, 1.0006, 1.0059, 1.0025, 1.0083,\n",
      "         1.0006, 1.0006, 1.0017, 1.0087, 1.0087, 1.0054, 1.0021, 1.0026, 1.0181,\n",
      "         1.0004, 1.0005, 1.0042, 1.0009, 1.0072, 1.0095, 1.0005, 1.0002],\n",
      "        [0.0000, 1.0074, 1.0037, 1.0066, 1.0032, 1.0081, 1.0043, 1.0013, 1.0031,\n",
      "         1.0058, 1.0063, 1.0052, 1.0075, 1.0015, 1.0194, 1.0005, 1.0022, 1.0027,\n",
      "         1.0043, 1.0013, 1.0017, 1.0032, 1.0037, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0030, 1.0190, 1.0122, 1.0047, 1.0144, 1.0012, 1.0107, 1.0126,\n",
      "         1.0018, 1.0028, 1.0018, 1.0046, 1.0027, 1.0030, 1.0019, 1.0149, 1.0526,\n",
      "         1.0039, 1.0012, 1.0116, 1.0007, 1.0115, 1.0006, 1.0031, 1.0232],\n",
      "        [0.0000, 1.0010, 1.0021, 1.0804, 1.0005, 1.0067, 1.0019, 1.0002, 1.0010,\n",
      "         1.0009, 1.0000, 1.0042, 1.0015, 1.0009, 1.0025, 1.0049, 1.0005, 1.0017,\n",
      "         1.0015, 1.0007, 1.0026, 1.0018, 1.0008, 1.0007, 1.0040, 1.0028]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 12, 12])  attentions_grads shape: torch.Size([3, 2, 12, 12, 12])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 12, 12])\n",
      "joint_attentions shape: torch.Size([2, 12, 12])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0378, 1.0201, 1.0372, 1.0097, 1.0254, 1.0066, 1.0696, 1.0146,\n",
      "         1.0062, 1.0019, 1.0000],\n",
      "        [0.0000, 1.0047, 1.0263, 1.0200, 1.0178, 1.0120, 1.0039, 1.0051, 1.0164,\n",
      "         1.0286, 1.0024, 1.0018]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 14, 14])  attentions_grads shape: torch.Size([3, 2, 12, 14, 14])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 14, 14])\n",
      "joint_attentions shape: torch.Size([2, 14, 14])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0102, 1.0060, 1.0237, 1.0253, 1.0001, 1.0036, 1.0545, 1.0011,\n",
      "         1.0036, 1.0289, 1.0385, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0293, 1.0045, 1.0025, 1.0159, 1.0059, 1.0058, 1.0142, 1.0274,\n",
      "         1.0285, 1.0025, 1.0027, 1.0030, 1.0007]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 45, 45])  attentions_grads shape: torch.Size([3, 2, 12, 45, 45])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 45, 45])\n",
      "joint_attentions shape: torch.Size([2, 45, 45])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0020, 1.0050, 1.0084, 1.0025, 1.0167, 1.0012, 1.0007,\n",
      "         1.0013, 1.0012, 1.0035, 1.0022, 1.0010, 1.0001, 1.0244, 1.0004, 1.0022,\n",
      "         1.0008, 1.0003, 1.0015, 1.0011, 1.0020, 1.0004, 1.0005, 1.0081, 1.0002,\n",
      "         1.0007, 1.0005, 1.0004, 1.0018, 1.0244, 1.0005, 1.0006, 1.0007, 1.0003,\n",
      "         1.0033, 1.0005, 1.0012, 1.0023, 1.0007, 1.0008, 1.0008, 1.0016, 1.0067],\n",
      "        [0.0000, 1.0128, 1.0810, 1.0159, 1.0111, 1.0038, 1.0050, 1.0012, 1.0108,\n",
      "         1.0113, 1.0110, 1.0030, 1.0086, 1.0305, 1.0354, 1.0014, 1.0006, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0006, 1.0030, 1.0043, 1.0027, 1.0021, 1.0047, 1.0125, 1.0009,\n",
      "         1.0025, 1.0090, 1.0163, 1.0005, 1.0071, 1.0032, 1.0017, 1.0060, 1.0250,\n",
      "         1.0022, 1.0023, 1.0061, 1.0006, 1.0010, 1.0080, 1.0124, 1.0011, 1.0009,\n",
      "         1.0115, 1.0118, 1.0185, 1.0009, 1.0091, 1.0007, 1.0003],\n",
      "        [0.0000, 1.0040, 1.0072, 1.0040, 1.0213, 1.0011, 1.0043, 1.0123, 1.0060,\n",
      "         1.0192, 1.0015, 1.0011, 1.0183, 1.0096, 1.0026, 1.0007, 1.0215, 1.0041,\n",
      "         1.0022, 1.0007, 1.0010, 1.0374, 1.0015, 1.0096, 1.0008, 1.0221, 1.0010,\n",
      "         1.0005, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0277, 1.0020, 1.0271, 1.0115, 1.0029, 1.0033, 1.0040, 1.0010,\n",
      "         1.0053, 1.0020, 1.0032, 1.0132, 1.0011, 1.0100, 1.0055, 1.0032, 1.0035,\n",
      "         1.0012, 1.0076, 1.0067, 1.0341],\n",
      "        [0.0000, 1.0024, 1.0028, 1.0033, 1.0107, 1.0016, 1.0012, 1.0064, 1.0362,\n",
      "         1.0392, 1.0013, 1.0016, 1.0063, 1.0075, 1.0015, 1.0036, 1.0188, 1.0502,\n",
      "         1.0245, 1.0159, 1.0016, 1.0007]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0030, 1.0019, 1.0013, 1.0017, 1.0286, 1.0014, 1.0030, 1.0037,\n",
      "         1.0009, 1.0348, 1.0115, 1.0033, 1.0162, 1.0108, 1.0070, 1.0009, 1.0131,\n",
      "         1.0009, 1.0026, 1.0011, 1.0207, 1.0056, 1.0057, 1.0193, 1.0012, 1.0009,\n",
      "         1.0049, 1.0402, 1.0005, 1.0002],\n",
      "        [0.0000, 1.0351, 1.0044, 1.0239, 1.0064, 1.0073, 1.0046, 1.0026, 1.0076,\n",
      "         1.0159, 1.0072, 1.0009, 1.0215, 1.0205, 1.0272, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0022, 1.0159, 1.0006, 1.0011, 1.0054, 1.0039, 1.0074, 1.0160,\n",
      "         1.0007, 1.0063, 1.0041, 1.0102, 1.0062, 1.0004, 1.0030, 1.0010, 1.0011,\n",
      "         1.0006, 1.0035, 1.0018, 1.0041, 1.0007, 1.0016, 1.0018, 1.0046, 1.0038,\n",
      "         1.0209, 1.0016, 1.0045, 1.0063, 1.0073, 1.0008, 1.0026, 1.0002],\n",
      "        [0.0000, 1.0012, 1.0584, 1.0008, 1.0112, 1.0009, 1.0124, 1.0055, 1.0096,\n",
      "         1.0031, 1.0097, 1.0131, 1.0105, 1.0253, 1.0015, 1.0243, 1.0063, 1.0268,\n",
      "         1.0021, 1.0015, 1.0049, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 19, 19])  attentions_grads shape: torch.Size([3, 2, 12, 19, 19])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 19, 19])\n",
      "joint_attentions shape: torch.Size([2, 19, 19])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0096, 1.0192, 1.0038, 1.0152, 1.0091, 1.0188, 1.0107, 1.0259,\n",
      "         1.0039, 1.0136, 1.0404, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0029, 1.0103, 1.0035, 1.0137, 1.0083, 1.0007, 1.0001, 1.0090,\n",
      "         1.0023, 1.0032, 1.0008, 1.0068, 1.0081, 1.0054, 1.0072, 1.0043, 1.0114,\n",
      "         1.0492]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0013, 1.0068, 1.0438, 1.0067, 1.0312, 1.0021, 1.0170, 1.0033,\n",
      "         1.0343, 1.0059, 1.0029, 1.0048, 1.0134, 1.0032, 1.0194, 1.0009, 1.0023,\n",
      "         1.0011, 1.0163, 1.0033, 1.0007, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0014, 1.0013, 1.0007, 1.0229, 1.0052, 1.0027, 1.0235,\n",
      "         1.0007, 1.0021, 1.0260, 1.0051, 1.0053, 1.0011, 1.0082, 1.0015, 1.0006,\n",
      "         1.0006, 1.0051, 1.0031, 1.0012, 1.0094, 1.0061, 1.0154, 1.0071, 1.0223,\n",
      "         1.0008, 1.0020, 1.0039, 1.0036, 1.0016, 1.0003, 1.0009, 1.0234, 1.0110,\n",
      "         1.0005, 1.0046, 1.0020, 1.0005, 1.0018, 1.0030, 1.0004, 1.0001]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0013, 1.0265, 1.0006, 1.0047, 1.0055, 1.0024, 1.0029, 1.0045,\n",
      "         1.0210, 1.0018, 1.0046, 1.0006, 1.0063, 1.0136, 1.0120, 1.0022, 1.0052,\n",
      "         1.0007, 1.0036, 1.0019, 1.0058, 1.0028, 1.0007, 1.0050, 1.0131, 1.0026,\n",
      "         1.0013, 1.0388, 1.0151, 1.0022, 1.0022, 1.0515, 1.0025, 1.0005, 1.0002],\n",
      "        [0.0000, 1.1151, 1.0250, 1.0079, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0143, 1.0229, 1.0081, 1.0240, 1.0031, 1.0186, 1.0039, 1.0066,\n",
      "         1.0035, 1.0336, 1.0014, 1.0145, 1.0171, 1.0341, 1.0043, 1.0016, 1.0074,\n",
      "         1.0061, 1.0198, 1.0010, 1.0107, 1.0005, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0030, 1.0006, 1.0016, 1.0111, 1.0052, 1.0019, 1.0013, 1.0014,\n",
      "         1.0012, 1.0043, 1.0037, 1.0030, 1.0018, 1.0048, 1.0001, 1.0086, 1.0079,\n",
      "         1.0035, 1.0127, 1.0019, 1.0006, 1.0020, 1.0063, 1.0013, 1.0009, 1.0100,\n",
      "         1.0012, 1.0016, 1.0079, 1.0045, 1.0025, 1.0134]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0146, 1.0025, 1.0017, 1.0006, 1.0001, 1.0022, 1.0011,\n",
      "         1.0017, 1.0026, 1.0017, 1.0017, 1.0013, 1.0133, 1.0001, 1.0009, 1.0006,\n",
      "         1.0007, 1.0006, 1.0159, 1.0005, 1.0004, 1.0006, 1.0003, 1.0020, 1.0008,\n",
      "         1.0016, 1.0008, 1.0006, 1.0009, 1.0008, 1.0015, 1.0058, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0007, 1.0006, 1.0009, 1.0009, 1.0027, 1.0021, 1.0013, 1.0004,\n",
      "         1.0004, 1.0038, 1.0013, 1.0009, 1.0093, 1.0030, 1.0007, 1.0166, 1.0009,\n",
      "         1.0148, 1.0019, 1.0027, 1.0014, 1.0007, 1.0003, 1.0849, 1.0010, 1.0144,\n",
      "         1.0005, 1.0042, 1.0007, 1.0084, 1.0011, 1.0016, 1.0003, 1.0008, 1.0011,\n",
      "         1.0276, 1.0086, 1.0072, 1.0004, 1.0003, 1.0212, 1.0003, 1.0001]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 49, 49])  attentions_grads shape: torch.Size([3, 2, 12, 49, 49])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 49, 49])\n",
      "joint_attentions shape: torch.Size([2, 49, 49])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0011, 1.0179, 1.0018, 1.0105, 1.0007, 1.0032, 1.0008, 1.0069,\n",
      "         1.0029, 1.0009, 1.0056, 1.0048, 1.0031, 1.0007, 1.0032, 1.0047, 1.0008,\n",
      "         1.0006, 1.0041, 1.0019, 1.0006, 1.0252, 1.0059, 1.0013, 1.0020, 1.0034,\n",
      "         1.0005, 1.0015, 1.0053, 1.0009, 1.0011, 1.0003, 1.0014, 1.0009, 1.0004,\n",
      "         1.0071, 1.0010, 1.0015, 1.0059, 1.0077, 1.0259, 1.0399, 1.0023, 1.0002,\n",
      "         1.0018, 1.0045, 1.0004, 1.0002],\n",
      "        [0.0000, 1.0183, 1.0044, 1.0052, 1.0050, 1.0086, 1.0110, 1.0026, 1.0085,\n",
      "         1.0264, 1.0018, 1.0392, 1.0268, 1.0214, 1.0046, 1.0039, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0006, 1.0022, 1.0010, 1.0016, 1.0010, 1.0021, 1.0024, 1.0007,\n",
      "         1.0031, 1.0005, 1.0033, 1.0017, 1.0064, 1.0109, 1.0028, 1.0028, 1.0176,\n",
      "         1.0034, 1.0018, 1.0018, 1.0002, 1.0012, 1.0002, 1.0010, 1.0014, 1.0004,\n",
      "         1.0002, 1.0036, 1.0007, 1.0004, 1.0395, 1.0012, 1.0530],\n",
      "        [0.0000, 1.0147, 1.0021, 1.0596, 1.0194, 1.0009, 1.0120, 1.0057, 1.0135,\n",
      "         1.0084, 1.0052, 1.0017, 1.0216, 1.0165, 1.0020, 1.0132, 1.0040, 1.0020,\n",
      "         1.0012, 1.0024, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0194, 1.0007, 1.0145, 1.0036, 1.0043, 1.0692, 1.0178, 1.0031,\n",
      "         1.0023, 1.0043, 1.0067, 1.0028, 1.0046, 1.0091, 1.0014, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0028, 1.0092, 1.0010, 1.0046, 1.0017, 1.0021, 1.0070, 1.0094,\n",
      "         1.0342, 1.0013, 1.0094, 1.0406, 1.0077, 1.0449, 1.0012, 1.0100, 1.0015,\n",
      "         1.0012, 1.0205, 1.0043, 1.0083, 1.0043, 1.0182, 1.0014, 1.0008]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0571, 1.0123, 1.0140, 1.0071, 1.0222, 1.0178, 1.0059, 1.0018,\n",
      "         1.0280, 1.0190, 1.0052, 1.0019, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0029, 1.0032, 1.0007, 1.0006, 1.0034, 1.0001, 1.0004,\n",
      "         1.0014, 1.0014, 1.0005, 1.0081, 1.0063, 1.0001, 1.0001, 1.0006, 1.0005,\n",
      "         1.0000, 1.0004, 1.0004, 1.0003, 1.0015, 1.0002, 1.0002, 1.0006, 1.0022,\n",
      "         1.0155, 1.0067, 1.0007, 1.0009, 1.0093, 1.0002, 1.0001, 1.0003, 1.0007,\n",
      "         1.0007, 1.0009, 1.0018, 1.0018, 1.0004, 1.0006, 1.0016, 1.0005]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 13, 13])  attentions_grads shape: torch.Size([3, 2, 12, 13, 13])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 13, 13])\n",
      "joint_attentions shape: torch.Size([2, 13, 13])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0146, 1.0267, 1.0284, 1.0364, 1.0030, 1.0096, 1.0028, 1.0024,\n",
      "         1.0432, 1.0253, 1.0034, 1.0028],\n",
      "        [0.0000, 1.1044, 1.0062, 1.0071, 1.0352, 1.0575, 1.0118, 1.0453, 1.0190,\n",
      "         1.0043, 1.0067, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0008, 1.0066, 1.0064, 1.0055, 1.0069, 1.0194, 1.0167, 1.0039,\n",
      "         1.0095, 1.0012, 1.0016, 1.0035, 1.0018, 1.0159, 1.0015, 1.0007, 1.0136,\n",
      "         1.0014, 1.0036, 1.0041, 1.0048, 1.0031, 1.0228, 1.0006, 1.0075, 1.0288,\n",
      "         1.0012, 1.0012, 1.0005, 1.0328, 1.0061, 1.0007, 1.0002],\n",
      "        [0.0000, 1.0032, 1.0057, 1.0078, 1.0021, 1.0022, 1.0350, 1.0052, 1.0056,\n",
      "         1.0016, 1.0038, 1.0039, 1.0023, 1.0011, 1.0085, 1.0203, 1.0117, 1.0073,\n",
      "         1.0207, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0139, 1.0486, 1.0184, 1.0256, 1.0128, 1.0360, 1.0037, 1.0605,\n",
      "         1.0087, 1.0028, 1.0013, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0059, 1.0139, 1.0161, 1.0011, 1.0022, 1.0050, 1.0204, 1.0019,\n",
      "         1.0045, 1.0227, 1.0010, 1.0109, 1.0095, 1.0135, 1.0013, 1.0011, 1.0015,\n",
      "         1.0223, 1.0013, 1.0058, 1.0020, 1.0239, 1.0009, 1.0128, 1.0016, 1.0080,\n",
      "         1.0218, 1.0007, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0082, 1.0855, 1.0247, 1.0062, 1.0042, 1.0073, 1.0017, 1.0080,\n",
      "         1.0019, 1.0322, 1.0015, 1.0168, 1.0151, 1.0020, 1.0060, 1.0020, 1.0027,\n",
      "         1.0171, 1.0012, 1.0027],\n",
      "        [0.0000, 1.0025, 1.0178, 1.0253, 1.0016, 1.0113, 1.0314, 1.0029, 1.0019,\n",
      "         1.0123, 1.0180, 1.0509, 1.0024, 1.0088, 1.0144, 1.0036, 1.0013, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0060, 1.0403, 1.0070, 1.0175, 1.0083, 1.0055, 1.0205, 1.0063,\n",
      "         1.0068, 1.0011, 1.0028, 1.0299, 1.0097, 1.0053, 1.0022, 1.0102, 1.0298,\n",
      "         1.0154, 1.0044, 1.0018],\n",
      "        [0.0000, 1.0032, 1.0534, 1.0263, 1.0368, 1.0135, 1.0584, 1.0189, 1.0027,\n",
      "         1.0076, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0095, 1.0049, 1.0197, 1.0013, 1.0030, 1.0110, 1.0022, 1.0030,\n",
      "         1.0030, 1.0121, 1.0137, 1.0131, 1.0067, 1.0028, 1.0016, 1.0045, 1.0031],\n",
      "        [0.0000, 1.0093, 1.0827, 1.0211, 1.0071, 1.0165, 1.0034, 1.0031, 1.0358,\n",
      "         1.0030, 1.0097, 1.0025, 1.0401, 1.0046, 1.0058, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0033, 1.0010, 1.0181, 1.0117, 1.0323, 1.0016, 1.0063, 1.0153,\n",
      "         1.0153, 1.0047, 1.0024, 1.0172, 1.0211, 1.0058, 1.0011, 1.0041, 1.0201,\n",
      "         1.0120, 1.0007, 1.0137, 1.0011, 1.0005],\n",
      "        [0.0000, 1.0053, 1.1341, 1.0029, 1.0045, 1.0012, 1.0058, 1.0009, 1.0013,\n",
      "         1.0094, 1.0016, 1.0013, 1.0007, 1.0123, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0557, 1.0237, 1.0037, 1.0330, 1.0287, 1.0132, 1.0013, 1.0026,\n",
      "         1.0024, 1.0161, 1.0083, 1.0049, 1.0094, 1.0008, 1.0019, 1.0036, 1.0030,\n",
      "         1.0203, 1.0010, 1.0010, 1.0005, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0041, 1.0061, 1.0010, 1.0062, 1.0037, 1.0017, 1.0207, 1.0077,\n",
      "         1.0010, 1.0019, 1.0925, 1.0022, 1.0021, 1.0123, 1.0145, 1.0005, 1.0010,\n",
      "         1.0147, 1.0042, 1.0191, 1.0089, 1.0040, 1.0018, 1.0093, 1.0152, 1.0154,\n",
      "         1.0018, 1.0050, 1.0006, 1.0005]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0095, 1.0097, 1.0081, 1.0355, 1.0148, 1.0064, 1.0030, 1.0041,\n",
      "         1.0065, 1.0022, 1.0026, 1.0054, 1.0153, 1.0075, 1.0047, 1.0015, 1.0065,\n",
      "         1.0077, 1.0015, 1.0113, 1.0023, 1.0041],\n",
      "        [0.0000, 1.0960, 1.0041, 1.0015, 1.0015, 1.0039, 1.0063, 1.0037, 1.0140,\n",
      "         1.0110, 1.0028, 1.0019, 1.0013, 1.0023, 1.0024, 1.0084, 1.0172, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 9, 9])  attentions_grads shape: torch.Size([3, 2, 12, 9, 9])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 9, 9])\n",
      "joint_attentions shape: torch.Size([2, 9, 9])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0067, 1.0297, 1.0328, 1.1046, 1.0461, 1.0084, 1.0052, 1.0000],\n",
      "        [0.0000, 1.0069, 1.0056, 1.0028, 1.0067, 1.0211, 1.1144, 1.0062, 1.0371]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 10, 10])  attentions_grads shape: torch.Size([3, 2, 12, 10, 10])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 10, 10])\n",
      "joint_attentions shape: torch.Size([2, 10, 10])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0208, 1.0156, 1.0151, 1.0074, 1.0379, 1.0256, 1.0084, 1.0159,\n",
      "         1.0424],\n",
      "        [0.0000, 1.0045, 1.0273, 1.0365, 1.0140, 1.0057, 1.0132, 1.0062, 1.0083,\n",
      "         1.0029]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0203, 1.0009, 1.0083, 1.0096, 1.0076, 1.0019, 1.0036, 1.0012,\n",
      "         1.0050, 1.0012, 1.0011, 1.0029, 1.0019, 1.0128, 1.0003, 1.0005, 1.0143,\n",
      "         1.0111, 1.0022, 1.0172, 1.0004, 1.0018, 1.0009, 1.0008, 1.0014, 1.0078,\n",
      "         1.0035, 1.0027, 1.0049, 1.0009, 1.0018, 1.0090, 1.0006, 1.0003],\n",
      "        [0.0000, 1.0177, 1.0027, 1.0125, 1.0038, 1.0019, 1.0013, 1.0142, 1.0019,\n",
      "         1.0036, 1.0173, 1.0058, 1.0032, 1.0038, 1.0073, 1.0231, 1.0030, 1.0008,\n",
      "         1.0010, 1.0240, 1.0057, 1.0192, 1.0150, 1.0030, 1.0168, 1.0021, 1.0004,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0256, 1.0109, 1.0002, 1.0221, 1.0012, 1.0202, 1.0050,\n",
      "         1.0099, 1.0054, 1.0025, 1.0041, 1.0135, 1.0011, 1.0033, 1.0024, 1.0006],\n",
      "        [0.0000, 1.0094, 1.0212, 1.0103, 1.0427, 1.0035, 1.0122, 1.0480, 1.0171,\n",
      "         1.0322, 1.0265, 1.0255, 1.0026, 1.0032, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0044, 1.0107, 1.0186, 1.0037, 1.0303, 1.0042, 1.0097, 1.0081,\n",
      "         1.0046, 1.0059, 1.0006, 1.0043, 1.0129, 1.0316, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0003, 1.0382, 1.0079, 1.0030, 1.0045, 1.0008, 1.0058, 1.0027,\n",
      "         1.0055, 1.0034, 1.0017, 1.0004, 1.0010, 1.0033, 1.0029, 1.0033, 1.0111,\n",
      "         1.0054, 1.0003, 1.0006, 1.0010, 1.0558, 1.0018, 1.0023, 1.0067]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0147, 1.0070, 1.0039, 1.0051, 1.0610, 1.0093, 1.0033, 1.0006,\n",
      "         1.0037, 1.0058, 1.0038, 1.0038, 1.0050, 1.0154, 1.0092, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0028, 1.0045, 1.0282, 1.0071, 1.0156, 1.0026, 1.0069, 1.0382,\n",
      "         1.0023, 1.0402, 1.0123, 1.0088, 1.0406, 1.0019, 1.0046, 1.0131, 1.0020,\n",
      "         1.0025, 1.0004]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0183, 1.0044, 1.0047, 1.0010, 1.0471, 1.0004, 1.0213, 1.0182,\n",
      "         1.0012, 1.0009, 1.0073, 1.0004, 1.0080, 1.0016, 1.0109, 1.0027, 1.0130,\n",
      "         1.0218, 1.0038, 1.0057, 1.0051, 1.0021, 1.0004, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0010, 1.0410, 1.0055, 1.0036, 1.0017, 1.0022, 1.0028, 1.0290,\n",
      "         1.0013, 1.0006, 1.0026, 1.0036, 1.0065, 1.0006, 1.0033, 1.0017, 1.0385,\n",
      "         1.0008, 1.0026, 1.0014, 1.0028, 1.0086, 1.0477, 1.0111, 1.0174, 1.0011,\n",
      "         1.0004]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 50, 50])  attentions_grads shape: torch.Size([3, 2, 12, 50, 50])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 50, 50])\n",
      "joint_attentions shape: torch.Size([2, 50, 50])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0037, 1.0030, 1.0022, 1.0023, 1.0049, 1.0005, 1.0008, 1.0018,\n",
      "         1.0123, 1.0021, 1.0005, 1.0025, 1.0354, 1.0071, 1.0036, 1.0015, 1.0006,\n",
      "         1.0037, 1.0012, 1.0202, 1.0014, 1.0080, 1.0007, 1.0036, 1.0025, 1.0032,\n",
      "         1.0063, 1.0084, 1.0108, 1.0022, 1.0033, 1.0022, 1.0016, 1.0017, 1.0016,\n",
      "         1.0007, 1.0003, 1.0047, 1.0101, 1.0078, 1.0052, 1.0003, 1.0016, 1.0100,\n",
      "         1.0005, 1.0058, 1.0100, 1.0004, 1.0001],\n",
      "        [0.0000, 1.0008, 1.0012, 1.0126, 1.0012, 1.0016, 1.0117, 1.0013, 1.0058,\n",
      "         1.0012, 1.0020, 1.0004, 1.0059, 1.0010, 1.0025, 1.0197, 1.0012, 1.0017,\n",
      "         1.0036, 1.0013, 1.0013, 1.0005, 1.0071, 1.0013, 1.0013, 1.0340, 1.0005,\n",
      "         1.0004, 1.0031, 1.0007, 1.0008, 1.0202, 1.0012, 1.0041, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0024, 1.0077, 1.0055, 1.0377, 1.0151, 1.0011, 1.0050, 1.0083,\n",
      "         1.0028, 1.0013, 1.0015, 1.0219, 1.0140, 1.0025, 1.0246, 1.0014, 1.0042,\n",
      "         1.0029, 1.0008, 1.0011, 1.0096, 1.0068, 1.0009, 1.0181, 1.0055, 1.0068,\n",
      "         1.0154, 1.0415, 1.0150, 1.0005, 1.0016, 1.0023, 1.0008, 1.0004],\n",
      "        [0.0000, 1.0149, 1.0077, 1.0042, 1.0014, 1.0259, 1.0095, 1.0008, 1.0007,\n",
      "         1.0176, 1.0028, 1.0031, 1.0008, 1.0090, 1.0195, 1.0018, 1.0013, 1.0081,\n",
      "         1.0147, 1.0016, 1.0091, 1.0011, 1.0010, 1.0045, 1.0039, 1.0333, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0038, 1.0017, 1.0057, 1.0028, 1.0023, 1.0014, 1.0019, 1.0024,\n",
      "         1.0019, 1.0021, 1.0172, 1.0178, 1.0049, 1.0037, 1.0097, 1.0026, 1.0242,\n",
      "         1.0020, 1.0023, 1.0050, 1.0017, 1.0102, 1.0018, 1.0224, 1.0084, 1.0043,\n",
      "         1.0121, 1.0189],\n",
      "        [0.0000, 1.0065, 1.0202, 1.0501, 1.0110, 1.0241, 1.0282, 1.0183, 1.0167,\n",
      "         1.0122, 1.0033, 1.0216, 1.0036, 1.0042, 1.0141, 1.0015, 1.0012, 1.0130,\n",
      "         1.0017, 1.0017, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0035, 1.0009, 1.0270, 1.0008, 1.0098, 1.0019, 1.0009, 1.0005,\n",
      "         1.0008, 1.0022, 1.0012, 1.0026, 1.0001, 1.0084, 1.0020, 1.0012, 1.0007,\n",
      "         1.0005, 1.0036, 1.0005, 1.0011, 1.0000, 1.0208, 1.0004, 1.0114, 1.0031,\n",
      "         1.0005, 1.0071, 1.0019, 1.0012, 1.0019, 1.0404],\n",
      "        [0.0000, 1.0043, 1.0251, 1.0015, 1.0021, 1.0121, 1.0007, 1.0069, 1.0195,\n",
      "         1.0022, 1.0013, 1.0053, 1.0075, 1.0013, 1.0053, 1.0019, 1.0350, 1.0036,\n",
      "         1.0417, 1.0119, 1.0080, 1.0061, 1.0083, 1.0012, 1.0005, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0090, 1.0002, 1.0020, 1.0039, 1.0015, 1.0014, 1.0130, 1.0415,\n",
      "         1.0019, 1.0055, 1.0021, 1.0016, 1.0005, 1.0087, 1.0245, 1.0043, 1.0033,\n",
      "         1.0264, 1.0006, 1.0013, 1.0015, 1.0002, 1.0052, 1.0016, 1.0005, 1.0019,\n",
      "         1.0008, 1.0003, 1.0088, 1.0065, 1.0053, 1.0017, 1.0026, 1.0036, 1.0138],\n",
      "        [0.0000, 1.0666, 1.0133, 1.0036, 1.0017, 1.0021, 1.0010, 1.0118, 1.0018,\n",
      "         1.0558, 1.0066, 1.0173, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 47, 47])  attentions_grads shape: torch.Size([3, 2, 12, 47, 47])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 47, 47])\n",
      "joint_attentions shape: torch.Size([2, 47, 47])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0187, 1.0037, 1.0026, 1.0440, 1.0015, 1.0208, 1.0141, 1.0106,\n",
      "         1.0062, 1.0050, 1.0224, 1.0023, 1.0131, 1.0100, 1.0226, 1.0048, 1.0038,\n",
      "         1.0015, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0016, 1.0049, 1.0030, 1.0009, 1.0008, 1.0042, 1.0082, 1.0015,\n",
      "         1.0038, 1.0005, 1.0006, 1.0006, 1.0093, 1.0022, 1.0036, 1.0005, 1.0360,\n",
      "         1.0013, 1.0029, 1.0033, 1.0010, 1.0008, 1.0209, 1.0194, 1.0040, 1.0023,\n",
      "         1.0100, 1.0007, 1.0045, 1.0014, 1.0124, 1.0054, 1.0007, 1.0006, 1.0068,\n",
      "         1.0063, 1.0008, 1.0004, 1.0017, 1.0059, 1.0018, 1.0024, 1.0167, 1.0041,\n",
      "         1.0004, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0534, 1.0051, 1.0039, 1.0245, 1.0312, 1.0062, 1.0137, 1.0020,\n",
      "         1.0150, 1.0001, 1.0187, 1.0021, 1.0014, 1.0045, 1.0067, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0085, 1.0096, 1.0014, 1.0009, 1.0028, 1.0008, 1.0016, 1.0037,\n",
      "         1.0040, 1.0016, 1.0183, 1.0003, 1.0016, 1.0086, 1.0004, 1.0010, 1.0009,\n",
      "         1.0025, 1.0004, 1.0019, 1.0015]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0112, 1.0199, 1.0659, 1.0310, 1.0031, 1.0293, 1.0202, 1.0137,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0028, 1.0019, 1.0023, 1.0016, 1.0133, 1.0034, 1.0007, 1.0002,\n",
      "         1.0017, 1.0029, 1.0475, 1.0009, 1.0016, 1.0002, 1.0032, 1.0054, 1.0029,\n",
      "         1.0007, 1.0060, 1.0098, 1.0005, 1.0000, 1.0029, 1.0023, 1.0006, 1.0103,\n",
      "         1.0005, 1.0012, 1.0055, 1.0005, 1.0012, 1.0060]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0053, 1.0040, 1.0028, 1.0016, 1.0021, 1.0078, 1.0133, 1.0047,\n",
      "         1.0047, 1.0092, 1.0102, 1.0076, 1.0026, 1.0016, 1.0069, 1.0096, 1.0010,\n",
      "         1.0062, 1.0058, 1.0009, 1.0003, 1.0019, 1.0005, 1.0011, 1.0200, 1.0055,\n",
      "         1.0086, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0005, 1.0179, 1.0073, 1.0014, 1.0011, 1.0035, 1.0052, 1.0035,\n",
      "         1.0073, 1.0006, 1.0088, 1.0102, 1.0054, 1.0198, 1.0189, 1.0304, 1.0135,\n",
      "         1.0006, 1.0008, 1.0053, 1.0126, 1.0039, 1.0153, 1.0046, 1.0055, 1.0013,\n",
      "         1.0033, 1.0008, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0016, 1.0409, 1.0081, 1.0053, 1.0073, 1.0110, 1.0010, 1.0154,\n",
      "         1.0035, 1.0159, 1.0022, 1.0096, 1.0037, 1.0143, 1.0050, 1.0017, 1.0727,\n",
      "         1.0012, 1.0003],\n",
      "        [0.0000, 1.0009, 1.0090, 1.0760, 1.0020, 1.0184, 1.0023, 1.0247, 1.0435,\n",
      "         1.0015, 1.0016, 1.0141, 1.0020, 1.0012, 1.0128, 1.0475, 1.0008, 1.0007,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 14, 14])  attentions_grads shape: torch.Size([3, 2, 12, 14, 14])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 14, 14])\n",
      "joint_attentions shape: torch.Size([2, 14, 14])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0056, 1.0116, 1.0040, 1.0113, 1.0055, 1.0721, 1.0090, 1.0138,\n",
      "         1.0039, 1.0138, 1.0435, 1.0030, 1.0010],\n",
      "        [0.0000, 1.0045, 1.0037, 1.0835, 1.0005, 1.0094, 1.0005, 1.0017, 1.0051,\n",
      "         1.0482, 1.0088, 1.0044, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0066, 1.0040, 1.0034, 1.0059, 1.0038, 1.0023, 1.0003, 1.0006,\n",
      "         1.0042, 1.0038, 1.0045, 1.0011, 1.0011, 1.0022, 1.0100, 1.0018, 1.0013,\n",
      "         1.0001, 1.0020, 1.0633, 1.0007, 1.0004, 1.0026, 1.0005, 1.0116, 1.0009,\n",
      "         1.0048, 1.0014, 1.0161, 1.0013, 1.0063, 1.0012, 1.0132],\n",
      "        [0.0000, 1.0055, 1.0008, 1.0103, 1.0006, 1.0018, 1.0041, 1.0014, 1.0016,\n",
      "         1.0053, 1.0041, 1.0061, 1.0037, 1.0067, 1.0022, 1.0016, 1.0236, 1.0175,\n",
      "         1.0091, 1.0286, 1.0071, 1.0010, 1.0021, 1.0050, 1.0022, 1.0146, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0005, 1.0157, 1.0008, 1.0019, 1.0003, 1.0115, 1.0003, 1.0099,\n",
      "         1.0029, 1.0061, 1.0005, 1.0032, 1.0046, 1.0002, 1.0012, 1.0133, 1.0014,\n",
      "         1.0005, 1.0053, 1.0017, 1.0027, 1.0010, 1.0005, 1.0004, 1.0016, 1.0034,\n",
      "         1.0061, 1.0010, 1.0005, 1.0008, 1.0022, 1.0068, 1.0017, 1.0015, 1.0019,\n",
      "         1.0013, 1.0009, 1.0289, 1.0003, 1.0002],\n",
      "        [0.0000, 1.0146, 1.0020, 1.0141, 1.0109, 1.0011, 1.0034, 1.0029, 1.0025,\n",
      "         1.0288, 1.0033, 1.0077, 1.0053, 1.0363, 1.0315, 1.0543, 1.0220, 1.0024,\n",
      "         1.0015, 1.0042, 1.0011, 1.0155, 1.0052, 1.0014, 1.0004, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0006, 1.0585, 1.0024, 1.0120, 1.0061, 1.0117, 1.0018, 1.0071,\n",
      "         1.0115, 1.0019, 1.0016, 1.0858, 1.0158, 1.0024, 1.0019, 1.0038, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0026, 1.0028, 1.0094, 1.0022, 1.0286, 1.0020, 1.0015,\n",
      "         1.0020, 1.0008, 1.0011, 1.0603, 1.0018, 1.0008, 1.0087, 1.0035, 1.0028,\n",
      "         1.0012, 1.0015, 1.0019, 1.0005, 1.0085, 1.0143, 1.0351]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0024, 1.0306, 1.0013, 1.0050, 1.0076, 1.0024, 1.0110,\n",
      "         1.0032, 1.0092, 1.0082, 1.0015, 1.0108, 1.0010, 1.0124, 1.0207, 1.0028,\n",
      "         1.0356, 1.0336, 1.0082, 1.0043, 1.0051, 1.0130, 1.0144, 1.0017, 1.0006],\n",
      "        [0.0000, 1.0061, 1.0059, 1.0073, 1.0020, 1.0102, 1.0141, 1.0064, 1.0011,\n",
      "         1.0026, 1.0069, 1.0013, 1.0069, 1.0227, 1.0014, 1.0013, 1.0047, 1.0020,\n",
      "         1.0013, 1.0063, 1.0031, 1.0028, 1.0080, 1.0106, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0405, 1.0045, 1.0137, 1.0046, 1.0124, 1.0143, 1.0058, 1.0029,\n",
      "         1.0118, 1.0028, 1.0018, 1.0059, 1.0031, 1.0009, 1.0007, 1.0025, 1.0029,\n",
      "         1.0151, 1.0278, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0166, 1.0052, 1.0003, 1.0075, 1.0031, 1.0011, 1.0291, 1.0181,\n",
      "         1.0008, 1.0026, 1.0015, 1.0014, 1.0022, 1.0017, 1.0016, 1.0039, 1.0041,\n",
      "         1.0008, 1.0258, 1.0029, 1.0017, 1.0006, 1.0013, 1.0003, 1.0027, 1.0257,\n",
      "         1.0132]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0185, 1.0092, 1.0031, 1.0009, 1.0164, 1.0015, 1.0012, 1.0051,\n",
      "         1.0098, 1.0006, 1.0005, 1.0019, 1.0059, 1.0011, 1.0048, 1.0008, 1.0032,\n",
      "         1.0305, 1.0111, 1.0104, 1.0072, 1.0014, 1.0007, 1.0052, 1.0211, 1.0393,\n",
      "         1.0011, 1.0004],\n",
      "        [0.0000, 1.0163, 1.0059, 1.0452, 1.0034, 1.0150, 1.0185, 1.0149, 1.0145,\n",
      "         1.0045, 1.0066, 1.0197, 1.0031, 1.0084, 1.0057, 1.0019, 1.0018, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0180, 1.0024, 1.0052, 1.0309, 1.0054, 1.0229, 1.0040, 1.0029,\n",
      "         1.0140, 1.0060, 1.0008, 1.0006, 1.0383, 1.0088, 1.0124, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0090, 1.0056, 1.0411, 1.0014, 1.0111, 1.0009, 1.0033, 1.0084,\n",
      "         1.0033, 1.0005, 1.0042, 1.0009, 1.0102, 1.0007, 1.0017, 1.0043, 1.0143,\n",
      "         1.0046, 1.0005, 1.0033, 1.0052, 1.0057, 1.0094, 1.0055, 1.0164, 1.0015,\n",
      "         1.0041, 1.0190, 1.0005, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0008, 1.0011, 1.0016, 1.0017, 1.0023, 1.0014, 1.0007,\n",
      "         1.0033, 1.0013, 1.0037, 1.0010, 1.0032, 1.0022, 1.0012, 1.0027, 1.0069,\n",
      "         1.0026, 1.0017, 1.0031, 1.0010, 1.0007, 1.0033, 1.0000, 1.0006, 1.0006,\n",
      "         1.0194, 1.0005, 1.0006, 1.0010, 1.0227, 1.0283, 1.0010, 1.0024, 1.0041],\n",
      "        [0.0000, 1.0007, 1.0099, 1.0145, 1.0116, 1.0028, 1.0054, 1.0009, 1.0014,\n",
      "         1.0010, 1.0038, 1.0007, 1.0047, 1.0068, 1.0239, 1.0256, 1.0018, 1.0246,\n",
      "         1.0243, 1.0005, 1.0036, 1.0007, 1.0113, 1.0037, 1.0086, 1.0071, 1.0078,\n",
      "         1.0009, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 47, 47])  attentions_grads shape: torch.Size([3, 2, 12, 47, 47])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 47, 47])\n",
      "joint_attentions shape: torch.Size([2, 47, 47])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0009, 1.0038, 1.0103, 1.0011, 1.0077, 1.0016, 1.0004, 1.0074,\n",
      "         1.0058, 1.0018, 1.0008, 1.0252, 1.0014, 1.0111, 1.0006, 1.0007, 1.0009,\n",
      "         1.0173, 1.0019, 1.0010, 1.0020, 1.0031, 1.0015, 1.0085, 1.0068, 1.0020,\n",
      "         1.0111, 1.0202, 1.0008, 1.0011, 1.0006, 1.0010, 1.0039, 1.0053, 1.0018,\n",
      "         1.0140, 1.0021, 1.0003, 1.0007, 1.0039, 1.0245, 1.0021, 1.0030, 1.0007,\n",
      "         1.0004, 1.0001],\n",
      "        [0.0000, 1.1041, 1.0015, 1.0147, 1.0030, 1.0025, 1.0024, 1.0013, 1.0011,\n",
      "         1.0046, 1.0207, 1.0059, 1.0087, 1.0020, 1.0054, 1.0013, 1.0048, 1.0053,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0027, 1.0167, 1.0045, 1.0111, 1.0016, 1.0198, 1.0032, 1.0203,\n",
      "         1.0016, 1.0065, 1.0056, 1.0018, 1.0062, 1.0018, 1.0005, 1.0055, 1.0035,\n",
      "         1.0044, 1.0044, 1.0007, 1.0031, 1.0023, 1.0018, 1.0088, 1.0024, 1.0021,\n",
      "         1.0025],\n",
      "        [0.0000, 1.0076, 1.0030, 1.0150, 1.0023, 1.0216, 1.0077, 1.0016, 1.0035,\n",
      "         1.0070, 1.0047, 1.0007, 1.0107, 1.0038, 1.0023, 1.0120, 1.0015, 1.0114,\n",
      "         1.0021, 1.0029, 1.0007, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0031, 1.0320, 1.0011, 1.0097, 1.0016, 1.0011, 1.0042, 1.0034,\n",
      "         1.0057, 1.0029, 1.0047, 1.0015, 1.0021, 1.0039, 1.0094, 1.0019, 1.0015,\n",
      "         1.0019, 1.0016, 1.0032, 1.0041, 1.0055, 1.0040, 1.0021, 1.0012, 1.0252,\n",
      "         1.0002, 1.0001, 1.0007, 1.0015, 1.0211],\n",
      "        [0.0000, 1.0627, 1.0053, 1.0018, 1.0159, 1.0030, 1.0201, 1.0066, 1.0138,\n",
      "         1.0155, 1.0455, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0026, 1.0007, 1.0016, 1.0233, 1.0050, 1.0228, 1.0017, 1.0005,\n",
      "         1.0003, 1.0035, 1.0011, 1.0007, 1.0013, 1.0006, 1.0005, 1.0006, 1.0012,\n",
      "         1.0135, 1.0036, 1.0016, 1.0011, 1.0005, 1.0011, 1.0000, 1.0017, 1.0062,\n",
      "         1.0010, 1.0010, 1.0003, 1.0015, 1.0092, 1.0003, 1.0043, 1.0007, 1.0004,\n",
      "         1.0067, 1.0082, 1.0226],\n",
      "        [0.0000, 1.0478, 1.0023, 1.0013, 1.0030, 1.0003, 1.0025, 1.0006, 1.0010,\n",
      "         1.0032, 1.0056, 1.0028, 1.0065, 1.0021, 1.0089, 1.0235, 1.0734, 1.0030,\n",
      "         1.0073, 1.0067, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0035, 1.0013, 1.0422, 1.0029, 1.0009, 1.0062, 1.0299, 1.0037,\n",
      "         1.0043, 1.0009, 1.0140, 1.0098, 1.0020, 1.0503, 1.0004, 1.0046, 1.0044,\n",
      "         1.0298, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0030, 1.0223, 1.0129, 1.0106, 1.0032, 1.0075, 1.0498, 1.0053,\n",
      "         1.0012, 1.0452, 1.0084, 1.0008, 1.0202, 1.0113, 1.0108, 1.0009, 1.0010,\n",
      "         1.0049, 1.0008, 1.0055, 1.0014, 1.0010, 1.0012, 1.0009, 1.0027, 1.0015,\n",
      "         1.0039, 1.0180, 1.0014, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0011, 1.0010, 1.0001, 1.0054, 1.0126, 1.0035, 1.0031, 1.0525,\n",
      "         1.0018, 1.0004, 1.0003, 1.0000, 1.0002, 1.0052, 1.0011, 1.0005, 1.0007,\n",
      "         1.0017, 1.0006],\n",
      "        [0.0000, 1.0022, 1.0348, 1.0073, 1.0385, 1.0432, 1.0033, 1.0446, 1.0155,\n",
      "         1.0170, 1.0112, 1.0577, 1.0027, 1.0013, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 50, 50])  attentions_grads shape: torch.Size([3, 2, 12, 50, 50])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 50, 50])\n",
      "joint_attentions shape: torch.Size([2, 50, 50])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0010, 1.0016, 1.0021, 1.0004, 1.0028, 1.0005, 1.0007,\n",
      "         1.0008, 1.0028, 1.0019, 1.0038, 1.0007, 1.0020, 1.0010, 1.0017, 1.0023,\n",
      "         1.0006, 1.0035, 1.0007, 1.0006, 1.0009, 1.0008, 1.0003, 1.0023, 1.0010,\n",
      "         1.0012, 1.0117, 1.0004, 1.0000, 1.0006, 1.0004, 1.0012, 1.0027, 1.0089,\n",
      "         1.0051, 1.0012, 1.0009, 1.0017, 1.0013, 1.0008, 1.0002, 1.0014, 1.0014,\n",
      "         1.0581, 1.0003, 1.0007, 1.0023, 1.0029],\n",
      "        [0.0000, 1.0039, 1.0030, 1.0004, 1.0067, 1.0032, 1.0263, 1.0015, 1.0007,\n",
      "         1.0012, 1.0009, 1.0024, 1.0024, 1.0008, 1.0024, 1.0002, 1.0017, 1.0012,\n",
      "         1.0260, 1.0021, 1.0020, 1.0012, 1.0013, 1.0027, 1.0193, 1.0007, 1.0098,\n",
      "         1.0008, 1.0016, 1.0007, 1.0021, 1.0005, 1.0002, 1.0028, 1.0044, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0066, 1.0023, 1.0028, 1.0021, 1.0325, 1.0887, 1.0086, 1.0643,\n",
      "         1.0247, 1.0438, 1.0032, 1.0265, 1.0063, 1.0039, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0071, 1.0301, 1.0099, 1.0084, 1.0025, 1.0021, 1.0018, 1.0269,\n",
      "         1.0054, 1.0322, 1.0162, 1.0021, 1.0243, 1.0072, 1.0344, 1.0014, 1.0143,\n",
      "         1.0018, 1.0013]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0248, 1.0074, 1.0043, 1.0011, 1.0062, 1.0037, 1.0121, 1.0023,\n",
      "         1.0005, 1.0405, 1.0055, 1.0014, 1.0016, 1.0012, 1.0117, 1.0034, 1.0301,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0052, 1.0548, 1.0079, 1.0042, 1.0041, 1.0403, 1.0124, 1.0006,\n",
      "         1.0143, 1.0016, 1.0098, 1.0051, 1.0127, 1.0016, 1.0046, 1.0034, 1.0256,\n",
      "         1.0084, 1.0018, 1.0012]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0047, 1.0012, 1.0016, 1.0259, 1.0023, 1.0008, 1.0203, 1.0027,\n",
      "         1.0056, 1.0158, 1.0041, 1.0150, 1.0061, 1.0182, 1.0005, 1.0025, 1.0015,\n",
      "         1.0096, 1.0003, 1.0030, 1.0171, 1.0008, 1.0023, 1.0230, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0034, 1.0026, 1.0025, 1.0030, 1.0140, 1.0083, 1.0049, 1.0076,\n",
      "         1.0024, 1.0031, 1.0096, 1.0009, 1.0008, 1.0016, 1.0004, 1.0047, 1.0020,\n",
      "         1.0018, 1.0072, 1.0028, 1.0006, 1.0083, 1.0117, 1.0037, 1.0070, 1.0027,\n",
      "         1.0012, 1.0131, 1.0014, 1.0023, 1.0055, 1.0008, 1.0016, 1.0162]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0032, 1.0365, 1.0017, 1.0054, 1.0080, 1.0113, 1.0046, 1.0050,\n",
      "         1.0014, 1.0015, 1.0158, 1.0044, 1.0006, 1.0125, 1.0031, 1.0044, 1.0119,\n",
      "         1.0142, 1.0051, 1.0218],\n",
      "        [0.0000, 1.0144, 1.0165, 1.0285, 1.0262, 1.0049, 1.0817, 1.0566, 1.0081,\n",
      "         1.0050, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 46, 46])  attentions_grads shape: torch.Size([3, 2, 12, 46, 46])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 46, 46])\n",
      "joint_attentions shape: torch.Size([2, 46, 46])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0091, 1.0030, 1.0004, 1.0016, 1.0032, 1.0219, 1.0008, 1.0004,\n",
      "         1.0020, 1.0006, 1.0005, 1.0152, 1.0051, 1.0008, 1.0029, 1.0032, 1.0004,\n",
      "         1.0005, 1.0060, 1.0001, 1.0007, 1.0006, 1.0007, 1.0101, 1.0124, 1.0110,\n",
      "         1.0011, 1.0010, 1.0013, 1.0014, 1.0032, 1.0005, 1.0011, 1.0176, 1.0005,\n",
      "         1.0013, 1.0003, 1.0005, 1.0255, 1.0005, 1.0011, 1.0004, 1.0072, 1.0002,\n",
      "         1.0001],\n",
      "        [0.0000, 1.0425, 1.0025, 1.0047, 1.0112, 1.0027, 1.0092, 1.0085, 1.0065,\n",
      "         1.0017, 1.0007, 1.0277, 1.0018, 1.0019, 1.0015, 1.0005, 1.0022, 1.0031,\n",
      "         1.0387, 1.0008, 1.0005, 1.0002, 1.0036, 1.0125, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0030, 1.0131, 1.0506, 1.0022, 1.0025, 1.0095, 1.0253, 1.0141,\n",
      "         1.0837, 1.0291, 1.0025, 1.0047, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0043, 1.0014, 1.0028, 1.0186, 1.0330, 1.0054, 1.0019, 1.0046,\n",
      "         1.0279, 1.0007, 1.0047, 1.0169, 1.0104, 1.0144, 1.0008, 1.0040, 1.0080,\n",
      "         1.0070, 1.0279, 1.0389, 1.0046, 1.0011, 1.0091, 1.0149, 1.0015, 1.0012]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0077, 1.0046, 1.0225, 1.0025, 1.0057, 1.0018, 1.0073, 1.0212,\n",
      "         1.0015, 1.0268, 1.0082, 1.0016, 1.0215, 1.0013, 1.0003, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0041, 1.0006, 1.0005, 1.0005, 1.0035, 1.0170, 1.0122, 1.0010,\n",
      "         1.0047, 1.0079, 1.0006, 1.0046, 1.0038, 1.0049, 1.0018, 1.0019, 1.0211,\n",
      "         1.0010, 1.0005, 1.0017, 1.0208, 1.0085, 1.0018]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0026, 1.0012, 1.0010, 1.0888, 1.0011, 1.0254, 1.0014, 1.0162,\n",
      "         1.0015, 1.0220, 1.0049, 1.0042, 1.0056, 1.0184, 1.0009, 1.0139, 1.0043,\n",
      "         1.0026, 1.0008, 1.0014, 1.0166, 1.0054, 1.0011, 1.0004, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0016, 1.0036, 1.0053, 1.0044, 1.0195, 1.0014, 1.0035, 1.0029,\n",
      "         1.0128, 1.0008, 1.0009, 1.0009, 1.0186, 1.0032, 1.0110, 1.0083, 1.0016,\n",
      "         1.0015, 1.0020, 1.0009, 1.0189, 1.0118, 1.0014, 1.0012, 1.0057, 1.0344,\n",
      "         1.0007, 1.0051, 1.0075, 1.0181, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0259, 1.0012, 1.0219, 1.0058, 1.0002, 1.0053, 1.0059, 1.0009,\n",
      "         1.0014, 1.0062, 1.0020, 1.0034, 1.0008, 1.0008, 1.0007, 1.0014, 1.0027,\n",
      "         1.0056, 1.0022, 1.0003, 1.0003, 1.0007, 1.0117, 1.0032, 1.0036, 1.0077,\n",
      "         1.0022, 1.0035],\n",
      "        [0.0000, 1.0183, 1.0432, 1.0215, 1.0043, 1.0160, 1.0257, 1.0204, 1.0107,\n",
      "         1.0355, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0095, 1.0013, 1.0014, 1.0012, 1.0061, 1.0023, 1.0028, 1.0326,\n",
      "         1.0077, 1.0008, 1.0018, 1.0416, 1.0180, 1.0005, 1.0007, 1.0108, 1.0206,\n",
      "         1.0120, 1.0125, 1.0070, 1.0050, 1.0063, 1.0009, 1.0006],\n",
      "        [0.0000, 1.0031, 1.0052, 1.0001, 1.0002, 1.0014, 1.0143, 1.0023, 1.0009,\n",
      "         1.0019, 1.0017, 1.0049, 1.0025, 1.0023, 1.0007, 1.0003, 1.0053, 1.0001,\n",
      "         1.0029, 1.0010, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0075, 1.0020, 1.0139, 1.0009, 1.0007, 1.0043, 1.0039, 1.0009,\n",
      "         1.0185, 1.0021, 1.0033, 1.0055, 1.0032, 1.0010, 1.0007, 1.0005, 1.0306,\n",
      "         1.0020, 1.0065, 1.0036, 1.0015, 1.0011, 1.0051, 1.0021, 1.0252, 1.0035,\n",
      "         1.0006, 1.0260, 1.0010, 1.0017, 1.0012, 1.0151, 1.0006, 1.0007, 1.0006,\n",
      "         1.0093, 1.0137, 1.0004, 1.0001],\n",
      "        [0.0000, 1.0260, 1.0008, 1.0051, 1.0147, 1.0489, 1.0028, 1.0105, 1.0020,\n",
      "         1.0048, 1.0012, 1.0082, 1.0043, 1.0104, 1.0050, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0030, 1.0013, 1.0007, 1.0618, 1.0028, 1.0008, 1.0001, 1.0006,\n",
      "         1.0037, 1.0039, 1.0003, 1.0099, 1.0128, 1.0016, 1.0003, 1.0004, 1.0003,\n",
      "         1.0017, 1.0002, 1.0007, 1.0015, 1.0026, 1.0062, 1.0005, 1.0006, 1.0461,\n",
      "         1.0014, 1.0093, 1.0004, 1.0019, 1.0421],\n",
      "        [0.0000, 1.0042, 1.0018, 1.0051, 1.0276, 1.0192, 1.0073, 1.0176, 1.0024,\n",
      "         1.0013, 1.0056, 1.0028, 1.0020, 1.0029, 1.0020, 1.0006, 1.0078, 1.0038,\n",
      "         1.0347, 1.0005, 1.0013, 1.0032, 1.0012, 1.0016, 1.0034, 1.0034, 1.0036,\n",
      "         1.0035, 1.0218, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0026, 1.0189, 1.0019, 1.0008, 1.0062, 1.0010, 1.0092, 1.0045,\n",
      "         1.0329, 1.0009, 1.0008, 1.0038, 1.0061, 1.0196, 1.0031, 1.0035, 1.0037,\n",
      "         1.0007, 1.0048, 1.0055, 1.0023, 1.0052, 1.0113, 1.0237, 1.0039, 1.0011,\n",
      "         1.0310, 1.0319, 1.0011, 1.0003],\n",
      "        [0.0000, 1.0041, 1.0084, 1.0032, 1.0017, 1.0233, 1.0030, 1.0033, 1.0013,\n",
      "         1.0001, 1.0149, 1.0006, 1.0014, 1.0010, 1.0076, 1.0013, 1.0037, 1.0022,\n",
      "         1.0020, 1.0144, 1.0053, 1.0063, 1.0053, 1.0010, 1.0341, 1.0137, 1.0102,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0123, 1.0130, 1.0035, 1.0055, 1.0014, 1.0030, 1.0055, 1.0011,\n",
      "         1.0155, 1.0008, 1.0845, 1.0066, 1.0034, 1.0024, 1.0013, 1.0116, 1.0041,\n",
      "         1.0248, 1.0000],\n",
      "        [0.0000, 1.0056, 1.0113, 1.0072, 1.0014, 1.0090, 1.0208, 1.0221, 1.0020,\n",
      "         1.0028, 1.0156, 1.0036, 1.0050, 1.0056, 1.0035, 1.0018, 1.0013, 1.0045,\n",
      "         1.0074, 1.0181]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0033, 1.0035, 1.0207, 1.0016, 1.0130, 1.0028, 1.0097, 1.0059,\n",
      "         1.0088, 1.0027, 1.0184, 1.0037, 1.0035, 1.0025, 1.0130, 1.0087, 1.0046,\n",
      "         1.0135, 1.0262, 1.0018, 1.0183, 1.0146, 1.0007, 1.0004, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0046, 1.0175, 1.0021, 1.0095, 1.0081, 1.0014, 1.0006, 1.0008,\n",
      "         1.0424, 1.0032, 1.0090, 1.0016, 1.0323, 1.0071, 1.0034, 1.0041, 1.0322,\n",
      "         1.0037, 1.0008, 1.0364, 1.0080, 1.0007, 1.0037, 1.0035, 1.0008, 1.0143,\n",
      "         1.0155, 1.0006, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0122, 1.1252, 1.0152, 1.0606, 1.0139, 1.0033, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0142, 1.0275, 1.0172, 1.0105, 1.0073, 1.0004, 1.0300, 1.0169,\n",
      "         1.0010, 1.0042, 1.0042, 1.0110, 1.0081, 1.0025, 1.0411, 1.0033, 1.0161,\n",
      "         1.0051, 1.0007, 1.0093, 1.0106, 1.0138, 1.0070, 1.0013, 1.0005]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 42, 42])  attentions_grads shape: torch.Size([3, 2, 12, 42, 42])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 42, 42])\n",
      "joint_attentions shape: torch.Size([2, 42, 42])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0025, 1.0256, 1.0115, 1.0070, 1.0061, 1.0022, 1.0046,\n",
      "         1.0027, 1.0010, 1.0015, 1.0080, 1.0014, 1.0040, 1.0127, 1.0020, 1.0009,\n",
      "         1.0092, 1.0149, 1.0029, 1.0098, 1.0135, 1.0066, 1.0012, 1.0013, 1.0074,\n",
      "         1.0016, 1.0038, 1.0009, 1.0172, 1.0006, 1.0006, 1.0009, 1.0083, 1.0038,\n",
      "         1.0008, 1.0016, 1.0106, 1.0293, 1.0006, 1.0001],\n",
      "        [0.0000, 1.0201, 1.0110, 1.0068, 1.0132, 1.0623, 1.0657, 1.0079, 1.0060,\n",
      "         1.0095, 1.0198, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0064, 1.0018, 1.0022, 1.0174, 1.0432, 1.0068, 1.0017, 1.0012,\n",
      "         1.0011, 1.0006, 1.0094, 1.0153, 1.0101, 1.0071, 1.0080, 1.0068, 1.0349,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0039, 1.0022, 1.0026, 1.0058, 1.0227, 1.0013, 1.0017, 1.0205,\n",
      "         1.0088, 1.0014, 1.0015, 1.0118, 1.0009, 1.0133, 1.0037, 1.0131, 1.0245,\n",
      "         1.0010, 1.0071, 1.0420, 1.0150, 1.0046, 1.0039, 1.0013, 1.0008]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 42, 42])  attentions_grads shape: torch.Size([3, 2, 12, 42, 42])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 42, 42])\n",
      "joint_attentions shape: torch.Size([2, 42, 42])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0161, 1.0041, 1.0017, 1.0080, 1.0047, 1.0050, 1.0020,\n",
      "         1.0033, 1.0072, 1.0140, 1.0030, 1.0023, 1.0008, 1.0105, 1.0006, 1.0158,\n",
      "         1.0017, 1.0259, 1.0006, 1.0005, 1.0054, 1.0005, 1.0029, 1.0016, 1.0028,\n",
      "         1.0008, 1.0007, 1.0043, 1.0010, 1.0037, 1.0027, 1.0083, 1.0013, 1.0005,\n",
      "         1.0006, 1.0005, 1.0024, 1.0153, 1.0005, 1.0001],\n",
      "        [0.0000, 1.0110, 1.0071, 1.0254, 1.0030, 1.0060, 1.0185, 1.0270, 1.0039,\n",
      "         1.0170, 1.0026, 1.0073, 1.0020, 1.0020, 1.0034, 1.0022, 1.0019, 1.0049,\n",
      "         1.0180, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0023, 1.0825, 1.0266, 1.0067, 1.0118, 1.0153, 1.0656, 1.0033,\n",
      "         1.0170, 1.0025, 1.0011, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0169, 1.0080, 1.0036, 1.0032, 1.0014, 1.0128, 1.0034,\n",
      "         1.0054, 1.0132, 1.0007, 1.0240, 1.0116, 1.0055, 1.0016, 1.0123, 1.0022,\n",
      "         1.0076, 1.0017, 1.0427, 1.0089, 1.0259, 1.0016, 1.0007]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 42, 42])  attentions_grads shape: torch.Size([3, 2, 12, 42, 42])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 42, 42])\n",
      "joint_attentions shape: torch.Size([2, 42, 42])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0076, 1.0007, 1.0456, 1.0023, 1.0109, 1.0025, 1.0031, 1.0068,\n",
      "         1.0008, 1.0005, 1.0006, 1.0007, 1.0110, 1.0005, 1.0042, 1.0044, 1.0008,\n",
      "         1.0011, 1.0085, 1.0005, 1.0013, 1.0018, 1.0187, 1.0016, 1.0092, 1.0089,\n",
      "         1.0122, 1.0025, 1.0056, 1.0005, 1.0099, 1.0008, 1.0076, 1.0007, 1.0016,\n",
      "         1.0002, 1.0012, 1.0021, 1.0006, 1.0005, 1.0001],\n",
      "        [0.0000, 1.0027, 1.0007, 1.0050, 1.0106, 1.0104, 1.0166, 1.0005, 1.0137,\n",
      "         1.0008, 1.0042, 1.0045, 1.0077, 1.0104, 1.0222, 1.0004, 1.0224, 1.0057,\n",
      "         1.0006, 1.0167, 1.0025, 1.0017, 1.0060, 1.0008, 1.0020, 1.0216, 1.0011,\n",
      "         1.0035, 1.0007, 1.0089, 1.0063, 1.0044, 1.0017, 1.0021, 1.0026, 1.0024,\n",
      "         1.0005, 1.0136, 1.0047, 1.0065, 1.0002, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0445, 1.0374, 1.0205, 1.0258, 1.0106, 1.0153, 1.0076, 1.0128,\n",
      "         1.0182, 1.0135, 1.0017, 1.0075, 1.0022, 1.0057, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0088, 1.0010, 1.0020, 1.0023, 1.0086, 1.0105, 1.0006, 1.0028,\n",
      "         1.0272, 1.0002, 1.0036, 1.0025, 1.0211, 1.0004, 1.0008, 1.0048, 1.0563,\n",
      "         1.0014, 1.0000, 1.0038, 1.0028, 1.0012, 1.0005, 1.0002, 1.0135, 1.0002,\n",
      "         1.0008, 1.0056, 1.0194]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0012, 1.0118, 1.0168, 1.0068, 1.0147, 1.0023, 1.0060, 1.0198,\n",
      "         1.0115, 1.0009, 1.0014, 1.0450, 1.0379, 1.0046, 1.0008, 1.0166, 1.0177,\n",
      "         1.0335, 1.0009, 1.0034, 1.0058, 1.0010, 1.0010],\n",
      "        [0.0000, 1.0402, 1.0108, 1.0443, 1.0259, 1.0058, 1.0184, 1.0436, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0402, 1.0036, 1.0036, 1.0137, 1.0162, 1.0033, 1.0207, 1.0030,\n",
      "         1.0063, 1.0024, 1.0020, 1.0000, 1.0006, 1.0123, 1.0050, 1.0036, 1.0072,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0013, 1.0320, 1.0250, 1.0005, 1.0012, 1.0066, 1.0110, 1.0014,\n",
      "         1.0026, 1.0004, 1.0018, 1.0023, 1.0036, 1.0056, 1.0069, 1.0131, 1.0011,\n",
      "         1.0105, 1.0285, 1.0027, 1.0186, 1.0082, 1.0049, 1.0151, 1.0034, 1.0123,\n",
      "         1.0018, 1.0009]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 11, 11])  attentions_grads shape: torch.Size([3, 2, 12, 11, 11])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 11, 11])\n",
      "joint_attentions shape: torch.Size([2, 11, 11])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0142, 1.0230, 1.0078, 1.0267, 1.0080, 1.0074, 1.0360, 1.0173,\n",
      "         1.0306, 1.0000],\n",
      "        [0.0000, 1.0038, 1.0738, 1.0136, 1.0107, 1.0293, 1.0067, 1.0026, 1.0117,\n",
      "         1.0041, 1.0023]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0288, 1.0024, 1.0018, 1.0251, 1.0072, 1.0025, 1.0100, 1.0083,\n",
      "         1.0108, 1.0022, 1.0015, 1.0090, 1.0100, 1.0058, 1.0036, 1.0028, 1.0351,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0057, 1.0085, 1.0005, 1.0036, 1.0007, 1.0029, 1.0087, 1.0094,\n",
      "         1.0006, 1.0083, 1.0151, 1.0013, 1.0115, 1.0028, 1.0325, 1.0003, 1.0028,\n",
      "         1.0005, 1.0039, 1.0110, 1.0013, 1.0147, 1.0050, 1.0006, 1.0195, 1.0008,\n",
      "         1.0117, 1.0056, 1.0034, 1.0017, 1.0011, 1.0007, 1.0005, 1.0276, 1.0006,\n",
      "         1.0073, 1.0015, 1.0011, 1.0006, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 53, 53])  attentions_grads shape: torch.Size([3, 2, 12, 53, 53])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 53, 53])\n",
      "joint_attentions shape: torch.Size([2, 53, 53])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0014, 1.0029, 1.0012, 1.0018, 1.0081, 1.0005, 1.0059, 1.0055,\n",
      "         1.0007, 1.0022, 1.0067, 1.0039, 1.0007, 1.0005, 1.0019, 1.0043, 1.0007,\n",
      "         1.0035, 1.0152, 1.0011, 1.0188, 1.0178, 1.0047, 1.0104, 1.0004, 1.0007,\n",
      "         1.0007, 1.0009, 1.0042, 1.0112, 1.0014, 1.0055, 1.0123, 1.0014, 1.0010,\n",
      "         1.0081, 1.0014, 1.0003, 1.0115, 1.0006, 1.0026, 1.0006, 1.0005, 1.0042,\n",
      "         1.0014, 1.0002, 1.0009, 1.0093, 1.0002, 1.0068, 1.0005, 1.0001],\n",
      "        [0.0000, 1.0016, 1.0092, 1.0181, 1.0125, 1.0136, 1.0055, 1.0095, 1.0010,\n",
      "         1.0013, 1.0008, 1.0082, 1.0327, 1.0070, 1.0016, 1.0010, 1.0104, 1.0060,\n",
      "         1.0010, 1.0007, 1.0192, 1.0055, 1.0657, 1.0270, 1.0018, 1.0012, 1.0004,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 48, 48])  attentions_grads shape: torch.Size([3, 2, 12, 48, 48])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 48, 48])\n",
      "joint_attentions shape: torch.Size([2, 48, 48])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0008, 1.0007, 1.0057, 1.0199, 1.0116, 1.0008, 1.0037, 1.0643,\n",
      "         1.0020, 1.0064, 1.0016, 1.0021, 1.0035, 1.0003, 1.0122, 1.0040, 1.0028,\n",
      "         1.0020, 1.0080, 1.0011, 1.0003, 1.0008, 1.0021, 1.0019, 1.0078, 1.0253,\n",
      "         1.0008, 1.0093, 1.0028, 1.0003, 1.0020, 1.0156, 1.0006, 1.0024, 1.0020,\n",
      "         1.0016, 1.0007, 1.0003, 1.0039, 1.0002, 1.0010, 1.0243, 1.0003, 1.0004,\n",
      "         1.0029, 1.0002, 1.0001],\n",
      "        [0.0000, 1.0028, 1.0021, 1.0021, 1.0105, 1.0106, 1.0080, 1.0006, 1.0026,\n",
      "         1.0009, 1.0018, 1.0023, 1.0172, 1.0043, 1.0075, 1.0005, 1.0027, 1.0009,\n",
      "         1.0016, 1.0430, 1.0024, 1.0003, 1.0011, 1.0078, 1.0008, 1.0021, 1.0056,\n",
      "         1.0038, 1.0002, 1.0015, 1.0017, 1.0077, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0043, 1.0053, 1.0019, 1.0313, 1.0063, 1.0055, 1.0009, 1.0078,\n",
      "         1.0013, 1.0140, 1.0096, 1.0035, 1.0010, 1.0004, 1.0013, 1.0013, 1.0037,\n",
      "         1.0004, 1.0010, 1.0017, 1.0004, 1.0002, 1.0005, 1.0023, 1.0194, 1.0228,\n",
      "         1.0016, 1.0018, 1.0023, 1.0074],\n",
      "        [0.0000, 1.0591, 1.0037, 1.0291, 1.0055, 1.0168, 1.0047, 1.0088, 1.0041,\n",
      "         1.0084, 1.0015, 1.0275, 1.0100, 1.0010, 1.0081, 1.0095, 1.0060, 1.0134,\n",
      "         1.0099, 1.0196, 1.0016, 1.0007, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0023, 1.0033, 1.0024, 1.0041, 1.0017, 1.0012, 1.0023, 1.0011,\n",
      "         1.0011, 1.0030, 1.0019, 1.0025, 1.0012, 1.0010, 1.0109, 1.0043, 1.0007,\n",
      "         1.0013, 1.0021, 1.0004, 1.0016, 1.0012, 1.0009, 1.0039, 1.0050, 1.0007,\n",
      "         1.0009, 1.0010, 1.0056, 1.0054, 1.0011, 1.0064, 1.0008, 1.0062, 1.0004,\n",
      "         1.0032, 1.0018, 1.0004, 1.0010, 1.0043, 1.0044, 1.0082, 1.0223],\n",
      "        [0.0000, 1.0024, 1.0024, 1.0022, 1.0250, 1.0143, 1.0298, 1.0074, 1.0042,\n",
      "         1.0267, 1.0495, 1.0009, 1.0122, 1.0137, 1.0287, 1.0264, 1.0018, 1.0007,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0007, 1.0051, 1.0202, 1.0048, 1.0011, 1.0006, 1.0008,\n",
      "         1.0015, 1.0014, 1.0025, 1.0002, 1.0011, 1.0038, 1.0005, 1.0015, 1.0004,\n",
      "         1.0003, 1.0024, 1.0039, 1.0006, 1.0037, 1.0008, 1.0018, 1.0038, 1.0206,\n",
      "         1.0012, 1.0008, 1.0159, 1.0024, 1.0019, 1.0046, 1.0031, 1.0070],\n",
      "        [0.0000, 1.0138, 1.0012, 1.0072, 1.0042, 1.0089, 1.0017, 1.0057, 1.0011,\n",
      "         1.0047, 1.0036, 1.0028, 1.0037, 1.0071, 1.0006, 1.0065, 1.0024, 1.0007,\n",
      "         1.0006, 1.0066, 1.0013, 1.0007, 1.0081, 1.0138, 1.0012, 1.0137, 1.0010,\n",
      "         1.0002, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0072, 1.0058, 1.0012, 1.0015, 1.0126, 1.0219, 1.0011, 1.0078,\n",
      "         1.0074, 1.0147, 1.0035, 1.0024, 1.0019, 1.0060, 1.0074, 1.0008, 1.0098,\n",
      "         1.0192, 1.0136, 1.0088, 1.0099, 1.0014, 1.0004],\n",
      "        [0.0000, 1.0333, 1.0034, 1.0372, 1.0481, 1.0217, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 47, 47])  attentions_grads shape: torch.Size([3, 2, 12, 47, 47])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 47, 47])\n",
      "joint_attentions shape: torch.Size([2, 47, 47])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0009, 1.0170, 1.0040, 1.0106, 1.0007, 1.0047, 1.0379, 1.0034,\n",
      "         1.0027, 1.0008, 1.0010, 1.0128, 1.0104, 1.0062, 1.0055, 1.0029, 1.0008,\n",
      "         1.0007, 1.0103, 1.0008, 1.0022, 1.0310, 1.0050, 1.0191, 1.0028, 1.0015,\n",
      "         1.0046, 1.0099, 1.0093, 1.0005, 1.0100, 1.0069, 1.0010, 1.0019, 1.0195,\n",
      "         1.0024, 1.0005, 1.0002, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0076, 1.0072, 1.0014, 1.0011, 1.0023, 1.0035, 1.0011, 1.0026,\n",
      "         1.0229, 1.0102, 1.0413, 1.0014, 1.0019, 1.0074, 1.0012, 1.0027, 1.0022,\n",
      "         1.0067, 1.0113, 1.0010, 1.0007, 1.0023, 1.0009, 1.0030, 1.0025, 1.0141,\n",
      "         1.0018, 1.0042, 1.0019, 1.0129, 1.0006, 1.0020, 1.0052, 1.0005, 1.0044,\n",
      "         1.0030, 1.0003, 1.0004, 1.0003, 1.0039, 1.0026, 1.0003, 1.0006, 1.0034,\n",
      "         1.0003, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0040, 1.0015, 1.0024, 1.0021, 1.0010, 1.0001, 1.0081, 1.0008,\n",
      "         1.0023, 1.0043, 1.0010, 1.0044, 1.0009, 1.0016, 1.0002, 1.0108, 1.0012,\n",
      "         1.0022, 1.0024, 1.0010, 1.0010, 1.0017, 1.0018, 1.0010, 1.0003, 1.0424,\n",
      "         1.0020, 1.0002, 1.0010, 1.0061, 1.0006, 1.0007, 1.0025, 1.0003, 1.0011,\n",
      "         1.0098, 1.0002, 1.0056, 1.0188],\n",
      "        [0.0000, 1.0006, 1.0254, 1.0009, 1.0005, 1.0128, 1.0062, 1.0011, 1.0057,\n",
      "         1.0042, 1.0022, 1.0029, 1.0085, 1.0025, 1.0022, 1.0018, 1.0151, 1.0044,\n",
      "         1.0266, 1.0052, 1.0024, 1.0032, 1.0014, 1.0014, 1.0382, 1.0043, 1.0007,\n",
      "         1.0083, 1.0009, 1.0030, 1.0005, 1.0039, 1.0011, 1.0046, 1.0021, 1.0056,\n",
      "         1.0005, 1.0002, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0027, 1.0135, 1.0004, 1.0159, 1.0032, 1.0277, 1.0007, 1.0014,\n",
      "         1.0022, 1.0033, 1.0002, 1.0021, 1.0020, 1.0011, 1.0018, 1.0049, 1.0035,\n",
      "         1.0019, 1.0022, 1.0089, 1.0011, 1.0005, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0011, 1.0032, 1.0122, 1.0051, 1.0009, 1.0013, 1.0018, 1.0064,\n",
      "         1.0155, 1.0009, 1.0024, 1.0109, 1.0045, 1.0107, 1.0033, 1.0434, 1.0031,\n",
      "         1.0006, 1.0007, 1.0130, 1.0027, 1.0036, 1.0050, 1.0012, 1.0041, 1.0253,\n",
      "         1.0078, 1.0383, 1.0014, 1.0026, 1.0095, 1.0051, 1.0017, 1.0020, 1.0005,\n",
      "         1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0159, 1.0101, 1.0007, 1.0013, 1.0022, 1.0174, 1.0039, 1.0014,\n",
      "         1.0015, 1.0037, 1.0009, 1.0008, 1.0008, 1.0019, 1.0012, 1.0010, 1.0038,\n",
      "         1.0058, 1.0014, 1.0006, 1.0019, 1.0004, 1.0005, 1.0029, 1.0273, 1.0003,\n",
      "         1.0000, 1.0131, 1.0045],\n",
      "        [0.0000, 1.0042, 1.0339, 1.0014, 1.0012, 1.0073, 1.0079, 1.0093, 1.0203,\n",
      "         1.0386, 1.0010, 1.0204, 1.0609, 1.0078, 1.0013, 1.0038, 1.0007, 1.0217,\n",
      "         1.0199, 1.0014, 1.0011, 1.0004, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 49, 49])  attentions_grads shape: torch.Size([3, 2, 12, 49, 49])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 49, 49])\n",
      "joint_attentions shape: torch.Size([2, 49, 49])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0014, 1.0008, 1.0058, 1.0249, 1.0151, 1.0005, 1.0048, 1.0164,\n",
      "         1.0046, 1.0022, 1.0011, 1.0076, 1.0060, 1.0005, 1.0004, 1.0035, 1.0042,\n",
      "         1.0005, 1.0004, 1.0224, 1.0148, 1.0043, 1.0036, 1.0011, 1.0142, 1.0052,\n",
      "         1.0004, 1.0005, 1.0005, 1.0005, 1.0013, 1.0034, 1.0005, 1.0004, 1.0037,\n",
      "         1.0020, 1.0108, 1.0156, 1.0021, 1.0043, 1.0164, 1.0022, 1.0011, 1.0005,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0037, 1.0176, 1.0016, 1.0072, 1.0016, 1.0005, 1.0045, 1.0005,\n",
      "         1.0005, 1.0247, 1.0037, 1.0072, 1.0018, 1.0022, 1.0184, 1.0003, 1.0006,\n",
      "         1.0155, 1.0021, 1.0022, 1.0008, 1.0208, 1.0005, 1.0201, 1.0095, 1.0029,\n",
      "         1.0021, 1.0023, 1.0039, 1.0013, 1.0052, 1.0012, 1.0025, 1.0007, 1.0007,\n",
      "         1.0054, 1.0005, 1.0034, 1.0006, 1.0024, 1.0003, 1.0028, 1.0090, 1.0106,\n",
      "         1.0008, 1.0119, 1.0003, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0022, 1.0317, 1.0034, 1.0025, 1.0094, 1.0020, 1.0042, 1.0041,\n",
      "         1.0009, 1.0120, 1.0043, 1.0010, 1.0159, 1.0284, 1.0009, 1.0032, 1.0148,\n",
      "         1.0077, 1.0072, 1.0006, 1.0179, 1.0053, 1.0029, 1.0020, 1.0126, 1.0006,\n",
      "         1.0007, 1.0043, 1.0007, 1.0008, 1.0286, 1.0046, 1.0006, 1.0091, 1.0004,\n",
      "         1.0003],\n",
      "        [0.0000, 1.0050, 1.0027, 1.0042, 1.0557, 1.0655, 1.0030, 1.0031, 1.0005,\n",
      "         1.0008, 1.0022, 1.0011, 1.0056, 1.0010, 1.0026, 1.0003, 1.0054, 1.0025,\n",
      "         1.0007, 1.0215, 1.0026, 1.0026, 1.0272, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0162, 1.0116, 1.0775, 1.0207, 1.0320, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0025, 1.0026, 1.0014, 1.0036, 1.0010, 1.0017, 1.0017, 1.0010,\n",
      "         1.0175, 1.0224, 1.0040, 1.0077, 1.0009, 1.0013, 1.0006, 1.0060, 1.0017,\n",
      "         1.0022, 1.0061, 1.0101, 1.0103, 1.0136, 1.0042, 1.0002, 1.0041, 1.0036,\n",
      "         1.0068, 1.0030]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0826, 1.0208, 1.0054, 1.0017, 1.0231, 1.0130, 1.0056, 1.0009,\n",
      "         1.0089, 1.0030, 1.0050, 1.0089, 1.0016, 1.0013, 1.0156, 1.0095, 1.0150,\n",
      "         1.0017, 1.0011, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0046, 1.0009, 1.0052, 1.0009, 1.0008, 1.0029, 1.0025, 1.0026,\n",
      "         1.0048, 1.0019, 1.0021, 1.0035, 1.0064, 1.0003, 1.0016, 1.0017, 1.0008,\n",
      "         1.0039, 1.0017, 1.0014, 1.0018, 1.0005, 1.0020, 1.0002, 1.0004, 1.0010,\n",
      "         1.0015, 1.0001, 1.0007, 1.0014, 1.0017, 1.0042, 1.0491, 1.0034, 1.0057]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0020, 1.0309, 1.0278, 1.0003, 1.0033, 1.0331, 1.0230, 1.0006,\n",
      "         1.0042, 1.0007, 1.0185, 1.0034, 1.0074, 1.0044, 1.0237, 1.0032, 1.0133,\n",
      "         1.0153, 1.0067, 1.0036, 1.0017, 1.0245, 1.0023, 1.0012, 1.0065, 1.0004,\n",
      "         1.0010, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0018, 1.0012, 1.0008, 1.0008, 1.0015, 1.0035, 1.0031, 1.0009,\n",
      "         1.0037, 1.0005, 1.0060, 1.0033, 1.0040, 1.0046, 1.0128, 1.0012, 1.0006,\n",
      "         1.0080, 1.0320, 1.0006, 1.0052, 1.0058, 1.0026, 1.0024, 1.0006, 1.0094,\n",
      "         1.0018, 1.0011, 1.0007, 1.0031, 1.0054, 1.0018, 1.0023, 1.0078, 1.0012,\n",
      "         1.0005, 1.0088, 1.0006, 1.0006, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0068, 1.0128, 1.0037, 1.0193, 1.0052, 1.0481, 1.0051, 1.0234,\n",
      "         1.0204, 1.0126, 1.0015, 1.0154, 1.0021, 1.0545, 1.0142, 1.0025, 1.0039,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0213, 1.0094, 1.0031, 1.0075, 1.0027, 1.0329, 1.0015, 1.0037,\n",
      "         1.0016, 1.0014, 1.0052, 1.0092, 1.0098, 1.0073, 1.0030, 1.0023, 1.0024,\n",
      "         1.0090, 1.0154]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0042, 1.0326, 1.0025, 1.0020, 1.0110, 1.0160, 1.0144, 1.0020,\n",
      "         1.0343, 1.0028, 1.0281, 1.0027, 1.0223, 1.0070, 1.0088, 1.0578, 1.0020,\n",
      "         1.0011, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0004, 1.0027, 1.0070, 1.0028, 1.0143, 1.0151, 1.0213, 1.0047,\n",
      "         1.0045, 1.0010, 1.0097, 1.0057, 1.0143, 1.0045, 1.0023, 1.0007, 1.0051,\n",
      "         1.0060, 1.0199, 1.0009, 1.0015, 1.0027, 1.0178]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0249, 1.0042, 1.0095, 1.0163, 1.0009, 1.0419, 1.0116, 1.0005,\n",
      "         1.0007, 1.0045, 1.0057, 1.0006, 1.0299, 1.0028, 1.0007, 1.0013, 1.0015,\n",
      "         1.0104, 1.0010, 1.0025, 1.0008, 1.0037, 1.0052, 1.0085, 1.0264, 1.0094,\n",
      "         1.0053, 1.0035, 1.0006, 1.0002, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0326, 1.0008, 1.0109, 1.0008, 1.0061, 1.0056, 1.0009, 1.0203,\n",
      "         1.0050, 1.0004, 1.0004, 1.0023, 1.0026, 1.0047, 1.0095, 1.0027, 1.0005,\n",
      "         1.0002, 1.0026, 1.0120, 1.0026, 1.0043, 1.0077, 1.0003, 1.0103, 1.0017,\n",
      "         1.0370, 1.0058, 1.0006, 1.0033, 1.0058, 1.0283, 1.0098, 1.0004, 1.0001]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0135, 1.0625, 1.0029, 1.0034, 1.0008, 1.0088, 1.0039, 1.0015,\n",
      "         1.0427, 1.0003, 1.0113, 1.0212, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0195, 1.0010, 1.0105, 1.0146, 1.0013, 1.0117, 1.0011, 1.0120,\n",
      "         1.0016, 1.0020, 1.0089, 1.0008, 1.0017, 1.0008, 1.0014, 1.0028, 1.0041,\n",
      "         1.0032, 1.0007, 1.0005, 1.0008, 1.0200, 1.0201, 1.0012, 1.0011, 1.0046,\n",
      "         1.0091, 1.0040, 1.0024, 1.0062, 1.0118, 1.0020, 1.0049, 1.0024, 1.0071,\n",
      "         1.0187, 1.0032, 1.0006, 1.0007]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0008, 1.0145, 1.0002, 1.0029, 1.0025, 1.0022, 1.0007, 1.0009,\n",
      "         1.0052, 1.0027, 1.0028, 1.0181, 1.0012, 1.0025, 1.1174, 1.0022, 1.0005,\n",
      "         1.0017, 1.0028, 1.0031, 1.0235, 1.0018, 1.0022, 1.0234, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0022, 1.0523, 1.0058, 1.0239, 1.0042, 1.0007, 1.0019, 1.0230,\n",
      "         1.0037, 1.0001, 1.0011, 1.0039, 1.0011, 1.0039, 1.0015, 1.0140, 1.0017,\n",
      "         1.0015, 1.0032, 1.0060, 1.0011, 1.0005, 1.0007, 1.0004, 1.0127, 1.0024,\n",
      "         1.0013]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0031, 1.0023, 1.0067, 1.0022, 1.0055, 1.0329, 1.0000, 1.0082,\n",
      "         1.0012, 1.0006, 1.0036, 1.0041, 1.0249, 1.0017, 1.0022, 1.0069, 1.0455],\n",
      "        [0.0000, 1.0089, 1.0027, 1.0028, 1.0045, 1.0553, 1.0341, 1.0154, 1.0040,\n",
      "         1.0192, 1.0379, 1.0250, 1.0331, 1.0036, 1.0014, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0022, 1.0033, 1.0243, 1.0008, 1.0164, 1.0008, 1.0121,\n",
      "         1.0015, 1.0006, 1.0135, 1.0008, 1.0025, 1.0007, 1.0005, 1.0250, 1.0011,\n",
      "         1.0036, 1.0129, 1.0100, 1.0009, 1.0115, 1.0028, 1.0006, 1.0093, 1.0248,\n",
      "         1.0016, 1.0049, 1.0012, 1.0006, 1.0029, 1.0010, 1.0008, 1.0008, 1.0003,\n",
      "         1.0001],\n",
      "        [0.0000, 1.0531, 1.0203, 1.0057, 1.0060, 1.0306, 1.0117, 1.0032, 1.0010,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0037, 1.0012, 1.0017, 1.0161, 1.0005, 1.0051, 1.0030,\n",
      "         1.0021, 1.0335, 1.0005, 1.0035, 1.0007, 1.0054, 1.0008, 1.0164, 1.0090,\n",
      "         1.0200, 1.0153, 1.0011, 1.0006, 1.0024, 1.0119, 1.0009, 1.0081, 1.0007,\n",
      "         1.0049, 1.0230, 1.0010, 1.0006, 1.0152, 1.0035, 1.0007, 1.0016, 1.0181,\n",
      "         1.0012, 1.0010, 1.0004, 1.0001, 1.0000],\n",
      "        [0.0000, 1.0010, 1.0012, 1.0246, 1.0035, 1.0172, 1.0006, 1.0121, 1.0049,\n",
      "         1.0006, 1.0093, 1.0018, 1.0007, 1.0006, 1.0017, 1.0012, 1.0014, 1.0032,\n",
      "         1.0003, 1.0059, 1.0030, 1.0007, 1.0037, 1.0512, 1.0045, 1.0071, 1.0011,\n",
      "         1.0015, 1.0009, 1.0169, 1.0103, 1.0003, 1.0005, 1.0032, 1.0044, 1.0044,\n",
      "         1.0225, 1.0003, 1.0036, 1.0005, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0779, 1.0021, 1.0025, 1.0120, 1.0077, 1.0074, 1.0035, 1.0020,\n",
      "         1.0214, 1.0094, 1.0105, 1.0021, 1.0153, 1.0119, 1.0010, 1.0009, 1.0056,\n",
      "         1.0023, 1.0112, 1.0100, 1.0054, 1.0054, 1.0129, 1.0043, 1.0025, 1.0005],\n",
      "        [0.0000, 1.0020, 1.0152, 1.0118, 1.0032, 1.0035, 1.0746, 1.0053, 1.0210,\n",
      "         1.0020, 1.0017, 1.0131, 1.0074, 1.0041, 1.0238, 1.0025, 1.0178, 1.0060,\n",
      "         1.0030, 1.0011, 1.0011, 1.0005, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0058, 1.0009, 1.0016, 1.0013, 1.0064, 1.0355, 1.0047, 1.0009,\n",
      "         1.0047, 1.0359, 1.0005, 1.0064, 1.0103, 1.0003, 1.0179, 1.0023, 1.0029,\n",
      "         1.0011, 1.0003, 1.0174, 1.0000, 1.0019, 1.0005, 1.0003, 1.0026, 1.0035],\n",
      "        [0.0000, 1.0030, 1.0145, 1.0178, 1.0015, 1.0019, 1.0128, 1.0065, 1.0325,\n",
      "         1.0035, 1.0014, 1.0006, 1.0033, 1.0012, 1.0247, 1.0009, 1.0106, 1.0145,\n",
      "         1.0017, 1.0007, 1.0082, 1.0069, 1.0152, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0009, 1.0130, 1.0014, 1.0007, 1.0012, 1.0042, 1.0099, 1.0008,\n",
      "         1.0075, 1.0007, 1.0014, 1.0007, 1.0052, 1.0008, 1.0004, 1.0006, 1.0322,\n",
      "         1.0014, 1.0439, 1.0163, 1.0005, 1.0007, 1.0022, 1.0013, 1.0158, 1.0197,\n",
      "         1.0181, 1.0007, 1.0010, 1.0036, 1.0010, 1.0039, 1.0005, 1.0173, 1.0118,\n",
      "         1.0064, 1.0052, 1.0005, 1.0002],\n",
      "        [0.0000, 1.0122, 1.0057, 1.0065, 1.0600, 1.0030, 1.0025, 1.0117, 1.0000,\n",
      "         1.0027, 1.0035, 1.0092, 1.0040, 1.0004, 1.0753, 1.0006, 1.0079, 1.0050,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0292, 1.0120, 1.0062, 1.0112, 1.0034, 1.0709, 1.0042, 1.0185,\n",
      "         1.0362, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0046, 1.0009, 1.0050, 1.0011, 1.0011, 1.0059, 1.0100, 1.0011,\n",
      "         1.0011, 1.0008, 1.0033, 1.0026, 1.0046, 1.0009, 1.0017, 1.0016, 1.0157,\n",
      "         1.0082, 1.0025, 1.0465, 1.0004, 1.0004, 1.0024, 1.0005, 1.0015, 1.0007,\n",
      "         1.0012, 1.0011, 1.0101, 1.0020, 1.0001, 1.0002, 1.0019, 1.0002, 1.0011,\n",
      "         1.0104, 1.0054, 1.0002, 1.0038, 1.0054]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 10, 10])  attentions_grads shape: torch.Size([3, 2, 12, 10, 10])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 10, 10])\n",
      "joint_attentions shape: torch.Size([2, 10, 10])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0121, 1.1141, 1.0121, 1.0477, 1.0067, 1.0136, 1.0151, 1.0021,\n",
      "         1.0051],\n",
      "        [0.0000, 1.0108, 1.0085, 1.0238, 1.1032, 1.0207, 1.0438, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0020, 1.0019, 1.0054, 1.0022, 1.0022, 1.0028, 1.0003, 1.0007,\n",
      "         1.0001, 1.0016, 1.0012, 1.0332, 1.0019, 1.0003, 1.0005, 1.0314, 1.0054,\n",
      "         1.0145, 1.0008, 1.0051, 1.0071, 1.0004, 1.0017, 1.0027, 1.0015, 1.0090,\n",
      "         1.0006, 1.0084, 1.0015, 1.0004, 1.0007, 1.0004, 1.0011, 1.0015],\n",
      "        [0.0000, 1.0129, 1.0056, 1.0013, 1.0021, 1.0044, 1.0026, 1.0009, 1.0063,\n",
      "         1.0111, 1.0054, 1.0007, 1.0204, 1.0164, 1.0061, 1.0101, 1.0107, 1.0014,\n",
      "         1.0011, 1.0001, 1.0099, 1.0201, 1.0008, 1.0002, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0064, 1.0028, 1.0030, 1.0026, 1.0167, 1.0026, 1.0364, 1.0081,\n",
      "         1.0187, 1.0166, 1.0077, 1.0066, 1.0278, 1.0124, 1.0030, 1.0011, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0044, 1.0060, 1.0161, 1.0021, 1.0287, 1.0251, 1.0013, 1.0004,\n",
      "         1.0032, 1.0039, 1.0115, 1.0058, 1.0014, 1.0050, 1.0008, 1.0085, 1.0039,\n",
      "         1.0080, 1.0013, 1.0043, 1.0032, 1.0073]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0165, 1.0026, 1.0193, 1.0233, 1.0083, 1.0030, 1.0015, 1.0012,\n",
      "         1.0208, 1.0189, 1.0110, 1.0190, 1.0055, 1.0020, 1.0009, 1.0331, 1.0080,\n",
      "         1.0019, 1.0028, 1.0179, 1.0012, 1.0006, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0079, 1.0036, 1.0068, 1.0034, 1.0032, 1.0008, 1.0065, 1.0031,\n",
      "         1.0009, 1.0081, 1.0008, 1.0038, 1.0065, 1.0173, 1.0035, 1.0005, 1.0007,\n",
      "         1.0043, 1.0093, 1.0306, 1.0021, 1.0195, 1.0029, 1.0008, 1.0010, 1.0025,\n",
      "         1.0005, 1.0058, 1.0470, 1.0008, 1.0041, 1.0032, 1.0012, 1.0065, 1.0005,\n",
      "         1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0063, 1.0324, 1.0029, 1.0049, 1.0078, 1.0020, 1.0865, 1.0067,\n",
      "         1.0070, 1.0010, 1.0035, 1.0272, 1.0107, 1.0247, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0131, 1.0017, 1.0016, 1.0165, 1.0105, 1.0026, 1.0567, 1.0013,\n",
      "         1.0019, 1.0333, 1.0085, 1.0075, 1.0141, 1.0034, 1.0177, 1.0010, 1.0017,\n",
      "         1.0039, 1.0032, 1.0079, 1.0004, 1.0033, 1.0010, 1.0004]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0205, 1.0055, 1.0023, 1.0238, 1.0028, 1.0004, 1.0074, 1.0011,\n",
      "         1.0011, 1.0020, 1.0014, 1.0021, 1.0020, 1.0026, 1.0008, 1.0016, 1.0019,\n",
      "         1.0049, 1.0037, 1.0017, 1.0040, 1.0016, 1.0009, 1.0060, 1.0033, 1.0029,\n",
      "         1.0018, 1.0052, 1.0035, 1.0007, 1.0004, 1.0014, 1.0415],\n",
      "        [0.0000, 1.0072, 1.0049, 1.0013, 1.0390, 1.0007, 1.0077, 1.0331, 1.0060,\n",
      "         1.0031, 1.0037, 1.0072, 1.0007, 1.0029, 1.0004, 1.0051, 1.0104, 1.0183,\n",
      "         1.0017, 1.0024, 1.0095, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 17, 17])  attentions_grads shape: torch.Size([3, 2, 12, 17, 17])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 17, 17])\n",
      "joint_attentions shape: torch.Size([2, 17, 17])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0145, 1.0227, 1.0028, 1.0736, 1.0164, 1.0742, 1.0166, 1.0490,\n",
      "         1.0037, 1.0034, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0199, 1.0071, 1.0031, 1.0031, 1.0022, 1.0312, 1.0248, 1.0013,\n",
      "         1.0172, 1.0221, 1.0237, 1.0026, 1.0224, 1.0070, 1.0026, 1.0048]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 19, 19])  attentions_grads shape: torch.Size([3, 2, 12, 19, 19])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 19, 19])\n",
      "joint_attentions shape: torch.Size([2, 19, 19])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0067, 1.0089, 1.0273, 1.0257, 1.0026, 1.0033, 1.0058, 1.0098,\n",
      "         1.0071, 1.0131, 1.0055, 1.0148, 1.0081, 1.0176, 1.0021, 1.0011, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0020, 1.0120, 1.0138, 1.0075, 1.0012, 1.0022, 1.0035, 1.0025,\n",
      "         1.0038, 1.0027, 1.0161, 1.0014, 1.0376, 1.0099, 1.0044, 1.0322, 1.0017,\n",
      "         1.0005]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0038, 1.0162, 1.0018, 1.0288, 1.0046, 1.0020, 1.0171,\n",
      "         1.0011, 1.0040, 1.0008, 1.0119, 1.0144, 1.0034, 1.0013, 1.0008, 1.0221,\n",
      "         1.0035, 1.0100, 1.0112, 1.0107, 1.0008, 1.0206, 1.0496, 1.0008, 1.0005],\n",
      "        [0.0000, 1.0016, 1.0032, 1.0022, 1.0121, 1.0084, 1.0048, 1.0014, 1.0464,\n",
      "         1.0087, 1.0111, 1.0010, 1.0015, 1.0025, 1.0054, 1.0018, 1.0171, 1.0025,\n",
      "         1.0136, 1.0119, 1.0035, 1.0113, 1.0033, 1.0017, 1.0013, 1.0012, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0030, 1.0859, 1.0037, 1.0182, 1.0412, 1.0041, 1.0036, 1.0070,\n",
      "         1.0095, 1.0081, 1.0071, 1.0214, 1.0012, 1.0054, 1.0040, 1.0075, 1.0020,\n",
      "         1.0054, 1.0073, 1.0170, 1.0009, 1.0005, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0018, 1.0062, 1.0054, 1.0011, 1.0012, 1.0110, 1.0010, 1.0235,\n",
      "         1.0271, 1.0026, 1.0105, 1.0131, 1.0058, 1.0193, 1.0005, 1.0006, 1.0272,\n",
      "         1.0040, 1.0009, 1.0011, 1.0010, 1.0010, 1.0059, 1.0013, 1.0102, 1.0192,\n",
      "         1.0024, 1.0015, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0030, 1.0038, 1.0071, 1.0648, 1.0008, 1.0052, 1.0061, 1.0025,\n",
      "         1.0017, 1.0043, 1.0046, 1.0017, 1.0013, 1.0252, 1.0002, 1.0010, 1.0014,\n",
      "         1.0008, 1.0011, 1.0004, 1.0028, 1.0006, 1.0015, 1.0007, 1.0005, 1.0008,\n",
      "         1.0078, 1.0001, 1.0009, 1.0029, 1.0394, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0034, 1.0031, 1.0000, 1.0004, 1.0004, 1.0012, 1.0004, 1.0008,\n",
      "         1.0005, 1.0000, 1.0008, 1.0008, 1.0013, 1.0011, 1.0351, 1.0005, 1.0006,\n",
      "         1.0005, 1.0004, 1.0008, 1.0002, 1.0041, 1.0000, 1.0000, 1.0009, 1.0007,\n",
      "         1.0288, 1.0005, 1.0038, 1.0004, 1.0028, 1.0001, 1.0000, 1.0006, 1.0004,\n",
      "         1.0044, 1.0002, 1.0004, 1.0005, 1.0006, 1.0006, 1.0017, 1.0002]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0011, 1.0013, 1.0030, 1.0029, 1.0284, 1.0045, 1.0077, 1.0005,\n",
      "         1.0096, 1.0094, 1.0005, 1.0058, 1.0027, 1.0078, 1.0242, 1.0099, 1.0013,\n",
      "         1.0009, 1.0063, 1.0045, 1.0007, 1.0038, 1.0019, 1.0025, 1.0086, 1.0130,\n",
      "         1.0058, 1.0028, 1.0363, 1.0011, 1.0005, 1.0018, 1.0308, 1.0148, 1.0003,\n",
      "         1.0003],\n",
      "        [0.0000, 1.0054, 1.0043, 1.0080, 1.0021, 1.0009, 1.0032, 1.0027, 1.0100,\n",
      "         1.0001, 1.0014, 1.0099, 1.0057, 1.0013, 1.0088, 1.0246, 1.0179, 1.0049,\n",
      "         1.0011, 1.0031, 1.0190, 1.0006, 1.0037, 1.0084, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0213, 1.0316, 1.0022, 1.0086, 1.0064, 1.0300, 1.0103, 1.0018,\n",
      "         1.0054, 1.0051, 1.0120, 1.0206, 1.0078, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0102, 1.0120, 1.0146, 1.0015, 1.0120, 1.0013, 1.0012, 1.0071,\n",
      "         1.0028, 1.0008, 1.0045, 1.0006, 1.0123, 1.0119, 1.0087, 1.0023, 1.0046,\n",
      "         1.0015, 1.0048, 1.0037, 1.0130, 1.0005, 1.0009, 1.0059, 1.0008, 1.0008,\n",
      "         1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 19, 19])  attentions_grads shape: torch.Size([3, 2, 12, 19, 19])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 19, 19])\n",
      "joint_attentions shape: torch.Size([2, 19, 19])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0267, 1.0112, 1.0106, 1.0103, 1.0024, 1.0022, 1.0245, 1.0021,\n",
      "         1.0198, 1.0166, 1.0247, 1.0341, 1.0218, 1.0100, 1.0007, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0075, 1.0164, 1.0048, 1.0071, 1.0003, 1.0014, 1.0031, 1.0055,\n",
      "         1.0077, 1.0009, 1.0179, 1.0031, 1.0020, 1.0023, 1.0029, 1.0154, 1.0041,\n",
      "         1.0484]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 38, 38])  attentions_grads shape: torch.Size([3, 2, 12, 38, 38])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 38, 38])\n",
      "joint_attentions shape: torch.Size([2, 38, 38])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0006, 1.0680, 1.0024, 1.0045, 1.0136, 1.0010, 1.0009, 1.0224,\n",
      "         1.0010, 1.0012, 1.0042, 1.0046, 1.0033, 1.0025, 1.0009, 1.0055, 1.0008,\n",
      "         1.0088, 1.0039, 1.0006, 1.0007, 1.0019, 1.0008, 1.0113, 1.0026, 1.0065,\n",
      "         1.0056, 1.0041, 1.0005, 1.0015, 1.0033, 1.0005, 1.0013, 1.0220, 1.0067,\n",
      "         1.0003, 1.0002],\n",
      "        [0.0000, 1.0021, 1.0153, 1.0262, 1.0016, 1.0449, 1.0023, 1.0030, 1.0031,\n",
      "         1.0007, 1.0006, 1.0017, 1.0005, 1.0006, 1.0011, 1.0030, 1.0010, 1.0067,\n",
      "         1.0140, 1.0010, 1.0008, 1.0003, 1.0016, 1.0016, 1.0023, 1.0134, 1.0058,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0039, 1.0012, 1.0040, 1.0132, 1.0010, 1.0009, 1.0055, 1.0021,\n",
      "         1.0005, 1.0006, 1.0070, 1.0015, 1.0014, 1.0041, 1.0059, 1.0008, 1.0009,\n",
      "         1.0065, 1.0189, 1.0009, 1.0050, 1.0006, 1.0035, 1.0014, 1.0211, 1.0120,\n",
      "         1.0046],\n",
      "        [0.0000, 1.0026, 1.0215, 1.0180, 1.0168, 1.0034, 1.0022, 1.0563, 1.0282,\n",
      "         1.0055, 1.0331, 1.0205, 1.0171, 1.0025, 1.0035, 1.0062, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 42, 42])  attentions_grads shape: torch.Size([3, 2, 12, 42, 42])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 42, 42])\n",
      "joint_attentions shape: torch.Size([2, 42, 42])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0009, 1.0106, 1.0007, 1.0010, 1.0008, 1.0012, 1.0005, 1.0111,\n",
      "         1.0034, 1.0021, 1.0008, 1.0097, 1.0129, 1.0223, 1.0009, 1.0239, 1.0004,\n",
      "         1.0006, 1.0607, 1.0048, 1.0004, 1.0029, 1.0114, 1.0062, 1.0018, 1.0024,\n",
      "         1.0004, 1.0048, 1.0043, 1.0012, 1.0005, 1.0105, 1.0037, 1.0006, 1.0031,\n",
      "         1.0072, 1.0004, 1.0018, 1.0020, 1.0006, 1.0001],\n",
      "        [0.0000, 1.0461, 1.0390, 1.0168, 1.0108, 1.0035, 1.0170, 1.0299, 1.0019,\n",
      "         1.0048, 1.0050, 1.0477, 1.0015, 1.0010, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0017, 1.0026, 1.0040, 1.0025, 1.0025, 1.0289, 1.0023, 1.0013,\n",
      "         1.0034, 1.0005, 1.0019, 1.0029, 1.0014, 1.0508, 1.0011, 1.0256, 1.0068,\n",
      "         1.0027, 1.0012],\n",
      "        [0.0000, 1.0133, 1.0026, 1.0359, 1.0046, 1.0066, 1.0074, 1.0087, 1.0204,\n",
      "         1.0090, 1.0066, 1.0244, 1.0108, 1.0251, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0096, 1.0220, 1.0029, 1.0134, 1.0037, 1.0043, 1.0205, 1.0128,\n",
      "         1.0033, 1.0099, 1.0109, 1.0091, 1.0126, 1.0221, 1.0052, 1.0087, 1.0014,\n",
      "         1.0280, 1.0014, 1.0029, 1.0294, 1.0034, 1.0015, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0075, 1.0009, 1.0029, 1.0138, 1.0069, 1.0012, 1.0007, 1.0027,\n",
      "         1.0050, 1.0014, 1.0025, 1.0228, 1.0098, 1.0015, 1.0289, 1.0015, 1.0079,\n",
      "         1.0128, 1.0056, 1.0025, 1.0105, 1.0011, 1.0022, 1.0020, 1.0081, 1.0006,\n",
      "         1.0025, 1.0006, 1.0013, 1.0004, 1.0010, 1.0009, 1.0174]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 48, 48])  attentions_grads shape: torch.Size([3, 2, 12, 48, 48])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 48, 48])\n",
      "joint_attentions shape: torch.Size([2, 48, 48])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0516, 1.0119, 1.0032, 1.0029, 1.0232, 1.0015, 1.0043,\n",
      "         1.0094, 1.0167, 1.0007, 1.0077, 1.0533, 1.0245, 1.0004, 1.0059, 1.0105,\n",
      "         1.0008, 1.0021, 1.0031, 1.0008, 1.0057, 1.0208, 1.0048, 1.0010, 1.0112,\n",
      "         1.0013, 1.0002, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0016, 1.0079, 1.0062, 1.0169, 1.0020, 1.0025, 1.0067, 1.0015,\n",
      "         1.0005, 1.0039, 1.0011, 1.0029, 1.0009, 1.0007, 1.0088, 1.0004, 1.0113,\n",
      "         1.0067, 1.0107, 1.0014, 1.0033, 1.0021, 1.0064, 1.0022, 1.0002, 1.0023,\n",
      "         1.0006, 1.0011, 1.0003, 1.0002, 1.0001, 1.0024, 1.0005, 1.0007, 1.0009,\n",
      "         1.0015, 1.0003, 1.0008, 1.0020, 1.0008, 1.0031, 1.0003, 1.0003, 1.0132,\n",
      "         1.0006, 1.0017, 1.0184]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0009, 1.0036, 1.0143, 1.0048, 1.0094, 1.0048, 1.0112, 1.0113,\n",
      "         1.0020, 1.0014, 1.0043, 1.0047, 1.0039, 1.0320, 1.0005, 1.0111, 1.0123,\n",
      "         1.0015, 1.0018, 1.0031, 1.0028, 1.0009, 1.0010, 1.0033, 1.0307, 1.0010,\n",
      "         1.0303, 1.0008, 1.0002],\n",
      "        [0.0000, 1.0003, 1.0003, 1.0020, 1.0132, 1.0011, 1.0009, 1.0014, 1.0007,\n",
      "         1.0123, 1.0019, 1.0058, 1.0125, 1.0047, 1.0051, 1.0093, 1.0176, 1.0013,\n",
      "         1.0138, 1.0015, 1.0466, 1.0010, 1.0017, 1.0097, 1.0049, 1.0028, 1.0011,\n",
      "         1.0342, 1.0006, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 11, 11])  attentions_grads shape: torch.Size([3, 2, 12, 11, 11])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 11, 11])\n",
      "joint_attentions shape: torch.Size([2, 11, 11])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0172, 1.0201, 1.0705, 1.0403, 1.0262, 1.0035, 1.0106, 1.0229,\n",
      "         1.0050, 1.0021],\n",
      "        [0.0000, 1.0049, 1.0433, 1.0277, 1.0349, 1.0108, 1.0768, 1.0358, 1.0017,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0033, 1.0911, 1.0063, 1.0007, 1.0091, 1.0052, 1.0025, 1.0062,\n",
      "         1.0022, 1.0075, 1.0074, 1.0031, 1.0014, 1.0009, 1.0014, 1.0004, 1.0020,\n",
      "         1.0031, 1.0019, 1.0053, 1.0053, 1.0011, 1.0062, 1.0248, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0010, 1.0037, 1.0058, 1.0041, 1.0080, 1.0023, 1.0022, 1.0316,\n",
      "         1.0011, 1.0033, 1.0143, 1.0065, 1.0023, 1.0277, 1.0010, 1.0344, 1.0067,\n",
      "         1.0017, 1.0008, 1.0110, 1.0408, 1.0312, 1.0050, 1.0026, 1.0135, 1.0026,\n",
      "         1.0013, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0023, 1.0013, 1.0033, 1.0059, 1.0002, 1.0027, 1.0081, 1.0037,\n",
      "         1.0016, 1.0021, 1.0005, 1.0009, 1.0065, 1.0030, 1.0004, 1.0008, 1.0016,\n",
      "         1.0023, 1.0007, 1.0005, 1.0026, 1.0037, 1.0010, 1.0012, 1.0072, 1.0129,\n",
      "         1.0345, 1.0008, 1.0094, 1.0042],\n",
      "        [0.0000, 1.0276, 1.1135, 1.0008, 1.0032, 1.0104, 1.0128, 1.0056, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 12, 12])  attentions_grads shape: torch.Size([3, 2, 12, 12, 12])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 12, 12])\n",
      "joint_attentions shape: torch.Size([2, 12, 12])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0201, 1.0250, 1.0063, 1.0196, 1.0255, 1.0125, 1.0031, 1.0222,\n",
      "         1.0068, 1.0051, 1.0020],\n",
      "        [0.0000, 1.0088, 1.0061, 1.0839, 1.0864, 1.0090, 1.0203, 1.0034, 1.0169,\n",
      "         1.0153, 1.0024, 1.0012]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0134, 1.0073, 1.0090, 1.1165, 1.0122, 1.0131, 1.0024, 1.0110,\n",
      "         1.0172, 1.0092, 1.0024, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0190, 1.0070, 1.0060, 1.0020, 1.0015, 1.0007, 1.0029, 1.0046,\n",
      "         1.0046, 1.0033, 1.0028, 1.0016, 1.0004, 1.0039, 1.0003, 1.0022, 1.0040,\n",
      "         1.0106, 1.0037, 1.0029, 1.0009, 1.0024, 1.0010, 1.0002, 1.0007, 1.0003,\n",
      "         1.0006, 1.0005, 1.0402, 1.0282, 1.0015, 1.0011, 1.0310]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0042, 1.0063, 1.0022, 1.0083, 1.0024, 1.0036, 1.0001, 1.0018,\n",
      "         1.0006, 1.0012, 1.0020, 1.0024, 1.0060, 1.0007, 1.0003, 1.0015, 1.0008,\n",
      "         1.0019, 1.0008, 1.0002, 1.0000, 1.0012, 1.0024, 1.0025, 1.0106, 1.0018,\n",
      "         1.0005],\n",
      "        [0.0000, 1.0088, 1.0037, 1.0225, 1.0007, 1.0071, 1.0024, 1.0045, 1.0057,\n",
      "         1.0128, 1.0026, 1.0183, 1.0026, 1.0013, 1.0023, 1.0016, 1.0081, 1.0018,\n",
      "         1.0004, 1.0085, 1.0074, 1.0178, 1.0034, 1.0253, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0012, 1.0013, 1.0397, 1.0006, 1.0013, 1.0020, 1.0033, 1.0007,\n",
      "         1.0005, 1.0005, 1.0021, 1.0020, 1.0127, 1.0069, 1.0031, 1.0068, 1.0022,\n",
      "         1.0015, 1.0010, 1.0104, 1.0022, 1.0006, 1.0009, 1.0031, 1.0309, 1.0064,\n",
      "         1.0025, 1.0055, 1.0009, 1.0271, 1.0032, 1.0010, 1.0015, 1.0239, 1.0004,\n",
      "         1.0001],\n",
      "        [0.0000, 1.0048, 1.0004, 1.0018, 1.0001, 1.0050, 1.0132, 1.0027, 1.0048,\n",
      "         1.0027, 1.0021, 1.0006, 1.0027, 1.0011, 1.0020, 1.0014, 1.0416, 1.0042,\n",
      "         1.0075, 1.0019, 1.0013, 1.0007, 1.0112, 1.0038, 1.0002, 1.0010, 1.0032,\n",
      "         1.0030, 1.0026, 1.0250, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0048, 1.0037, 1.0023, 1.0008, 1.0000, 1.0013, 1.0001, 1.0118,\n",
      "         1.0029, 1.0018, 1.0012, 1.0001, 1.0004, 1.0030, 1.0020, 1.0008, 1.0033,\n",
      "         1.0032, 1.0653, 1.0016, 1.0010, 1.0011, 1.0013, 1.0012, 1.0008],\n",
      "        [0.0000, 1.0103, 1.0210, 1.0113, 1.1063, 1.0297, 1.0116, 1.0132, 1.0053,\n",
      "         1.0259, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0109, 1.0054, 1.0060, 1.0449, 1.0043, 1.0216, 1.0255, 1.0029,\n",
      "         1.0027, 1.0238, 1.0046, 1.0032, 1.0262, 1.0020, 1.0030, 1.0063, 1.0289,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0428, 1.0020, 1.0312, 1.0007, 1.0017, 1.0083, 1.0011, 1.0001,\n",
      "         1.0052, 1.0028, 1.0027, 1.0041, 1.0002, 1.0012, 1.0322, 1.0008, 1.0126,\n",
      "         1.0102, 1.0040, 1.0011, 1.0006, 1.0007, 1.0031, 1.0211]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0270, 1.0026, 1.0041, 1.0087, 1.0305, 1.0013, 1.0047, 1.0005,\n",
      "         1.0161, 1.0049, 1.0014, 1.0002, 1.0017, 1.0034, 1.0026, 1.0014, 1.0114,\n",
      "         1.0013, 1.0006, 1.0006, 1.0031, 1.0068, 1.0002, 1.0046, 1.0080, 1.0288,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0008, 1.0013, 1.0074, 1.0089, 1.0033, 1.0043, 1.0007, 1.0603,\n",
      "         1.0125, 1.0176, 1.0043, 1.0198, 1.0006, 1.0111, 1.0130, 1.0037, 1.0315,\n",
      "         1.0078, 1.0008, 1.0049, 1.0045, 1.0249, 1.0006, 1.0187, 1.0018, 1.0009,\n",
      "         1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0072, 1.0280, 1.0045, 1.0255, 1.0979, 1.0149, 1.0388, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0068, 1.0043, 1.0042, 1.0042, 1.0116, 1.0196, 1.0009, 1.0010,\n",
      "         1.0057, 1.0009, 1.0006, 1.0010, 1.0041, 1.0068, 1.0012, 1.0016, 1.0014,\n",
      "         1.0126, 1.0014, 1.0053, 1.0003, 1.0012, 1.0050, 1.0031, 1.0044, 1.0023,\n",
      "         1.0004, 1.0035, 1.0016, 1.0010, 1.0008, 1.0025, 1.0020, 1.0009, 1.0010,\n",
      "         1.0018, 1.0038, 1.0399]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0026, 1.0035, 1.0036, 1.0033, 1.0477, 1.0056, 1.0019, 1.0261,\n",
      "         1.0026, 1.0041, 1.0026, 1.0041, 1.0011, 1.0012, 1.0070, 1.0015, 1.0006,\n",
      "         1.0295, 1.0321, 1.0020, 1.0276, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0059, 1.0013, 1.0013, 1.0353, 1.0048, 1.0011, 1.0126, 1.0071,\n",
      "         1.0194, 1.0035, 1.0098, 1.0008, 1.0035, 1.0256, 1.0008, 1.0213, 1.0057,\n",
      "         1.0004, 1.0030, 1.0567, 1.0201, 1.0017, 1.0011, 1.0011, 1.0016, 1.0080,\n",
      "         1.0262, 1.0011, 1.0004]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0114, 1.0080, 1.0040, 1.0019, 1.0094, 1.0038, 1.0159, 1.0007,\n",
      "         1.0216, 1.0018, 1.0037, 1.0009, 1.0392, 1.0009, 1.0005, 1.0251, 1.0072,\n",
      "         1.0018, 1.0015, 1.0100, 1.0038, 1.0013, 1.0021, 1.0003, 1.0005, 1.0046,\n",
      "         1.0008, 1.0111, 1.0074, 1.0005, 1.0193, 1.0173, 1.0006, 1.0002],\n",
      "        [0.0000, 1.0054, 1.0049, 1.0139, 1.0119, 1.0056, 1.0161, 1.0221, 1.0144,\n",
      "         1.0142, 1.0066, 1.0116, 1.0028, 1.0123, 1.0123, 1.0094, 1.0162, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 38, 38])  attentions_grads shape: torch.Size([3, 2, 12, 38, 38])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 38, 38])\n",
      "joint_attentions shape: torch.Size([2, 38, 38])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0154, 1.0039, 1.0027, 1.0208, 1.0023, 1.0030, 1.0080, 1.0007,\n",
      "         1.0000, 1.0095, 1.0010, 1.0024, 1.0002, 1.0004, 1.0015, 1.0046, 1.0006,\n",
      "         1.0415, 1.0021, 1.0086, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0018, 1.0059, 1.0004, 1.0016, 1.0145, 1.0105, 1.0028, 1.0019,\n",
      "         1.0025, 1.0003, 1.0003, 1.0374, 1.0069, 1.0003, 1.0004, 1.0009, 1.0012,\n",
      "         1.0017, 1.0136, 1.0006, 1.0005, 1.0007, 1.0015, 1.0007, 1.0010, 1.0055,\n",
      "         1.0008, 1.0049, 1.0009, 1.0081, 1.0024, 1.0004, 1.0006, 1.0024, 1.0066,\n",
      "         1.0003, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0160, 1.0011, 1.0034, 1.0184, 1.0029, 1.0169, 1.0598, 1.0047,\n",
      "         1.0025, 1.0014, 1.0037, 1.0007, 1.0018, 1.0022, 1.0056, 1.0011, 1.0054,\n",
      "         1.0069, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0024, 1.0335, 1.0008, 1.0205, 1.0133, 1.0013, 1.0125, 1.0192,\n",
      "         1.0012, 1.0132, 1.0045, 1.0049, 1.0108, 1.0004, 1.0012, 1.0314, 1.0015,\n",
      "         1.0141, 1.0112, 1.0007, 1.0004]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0177, 1.0060, 1.0043, 1.0037, 1.0030, 1.0101, 1.0020, 1.0013,\n",
      "         1.0057, 1.0045, 1.0308, 1.0116, 1.0065, 1.0103, 1.0022, 1.0013, 1.0062,\n",
      "         1.0056, 1.0014, 1.0049, 1.0230],\n",
      "        [0.0000, 1.0103, 1.0073, 1.0068, 1.0361, 1.0054, 1.0262, 1.0028, 1.0015,\n",
      "         1.0170, 1.0220, 1.0091, 1.0238, 1.0036, 1.0056, 1.0519, 1.0167, 1.0052,\n",
      "         1.0228, 1.0014, 1.0008, 1.0006]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0180, 1.0175, 1.0060, 1.0168, 1.0017, 1.0017, 1.0027, 1.0051,\n",
      "         1.0088, 1.0008, 1.0023, 1.0017, 1.0044, 1.0045, 1.0063, 1.0069, 1.0066,\n",
      "         1.0121, 1.0017, 1.0012, 1.0012, 1.0045, 1.0017, 1.0082, 1.0190, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0032, 1.0081, 1.0012, 1.0210, 1.0026, 1.0020, 1.0015, 1.0005,\n",
      "         1.0131, 1.0200, 1.0012, 1.0068, 1.0151, 1.0077, 1.0027, 1.0433, 1.0037,\n",
      "         1.0134, 1.0276, 1.0122, 1.0008, 1.0009, 1.0024, 1.0306, 1.0020, 1.0052,\n",
      "         1.0008, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 47, 47])  attentions_grads shape: torch.Size([3, 2, 12, 47, 47])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 47, 47])\n",
      "joint_attentions shape: torch.Size([2, 47, 47])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0014, 1.0143, 1.0306, 1.0008, 1.0004, 1.0012, 1.0017, 1.0046,\n",
      "         1.0016, 1.0073, 1.0018, 1.0013, 1.0138, 1.0005, 1.0073, 1.0072, 1.0013,\n",
      "         1.0004, 1.0119, 1.0003, 1.0013, 1.0014, 1.0006, 1.0025, 1.0015, 1.0032,\n",
      "         1.0006, 1.0026, 1.0026, 1.0015, 1.0005, 1.0003, 1.0072, 1.0003, 1.0008,\n",
      "         1.0042, 1.0005, 1.0001, 1.0188, 1.0004, 1.0023, 1.0004, 1.0003, 1.0026,\n",
      "         1.0020, 1.0061],\n",
      "        [0.0000, 1.0109, 1.0016, 1.0110, 1.0453, 1.0234, 1.0045, 1.0064, 1.0271,\n",
      "         1.0026, 1.0428, 1.0018, 1.0241, 1.0104, 1.0040, 1.0348, 1.0046, 1.0016,\n",
      "         1.0080, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0038, 1.0079, 1.0024, 1.0010, 1.0012, 1.0041, 1.0037, 1.0013,\n",
      "         1.0219, 1.0046, 1.0504, 1.0110, 1.0007, 1.0092, 1.0012, 1.0379, 1.0007,\n",
      "         1.0021, 1.0078, 1.0057, 1.0022, 1.0087, 1.0005, 1.0324, 1.0102, 1.0034,\n",
      "         1.0009, 1.0317, 1.0009, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0116, 1.0019, 1.0022, 1.0020, 1.0021, 1.0010, 1.0021, 1.0033,\n",
      "         1.0013, 1.0004, 1.0009, 1.0002, 1.0010, 1.0011, 1.0095, 1.0013, 1.0024,\n",
      "         1.0096, 1.0038, 1.0012, 1.0005, 1.0005, 1.0015, 1.0056, 1.0005, 1.0019,\n",
      "         1.0012, 1.0007, 1.0011, 1.0005, 1.0005, 1.0365, 1.0005, 1.0000, 1.0072,\n",
      "         1.0018, 1.0014, 1.0013]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0169, 1.0010, 1.0057, 1.0126, 1.0060, 1.0071, 1.0097, 1.0027,\n",
      "         1.0183, 1.0072, 1.0082, 1.0055, 1.0068, 1.0068, 1.0144, 1.0070, 1.0172],\n",
      "        [0.0000, 1.1107, 1.0327, 1.0165, 1.0182, 1.0068, 1.0068, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0044, 1.0053, 1.0058, 1.0021, 1.0007, 1.0072, 1.0016,\n",
      "         1.0007, 1.0037, 1.0023, 1.0027, 1.0041, 1.0042, 1.0125, 1.0034, 1.0024,\n",
      "         1.0092, 1.0067, 1.0047, 1.0153, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0017, 1.0006, 1.0141, 1.0162, 1.0014, 1.0003, 1.0006, 1.0012,\n",
      "         1.0452, 1.0017, 1.0050, 1.0003, 1.0005, 1.0057, 1.0007, 1.0019, 1.0001,\n",
      "         1.0013, 1.0023, 1.0016, 1.0005, 1.0004, 1.0002, 1.0005, 1.0096, 1.0034,\n",
      "         1.0026, 1.0010, 1.0005, 1.0001, 1.0009, 1.0005, 1.0002, 1.0011, 1.0016,\n",
      "         1.0101, 1.0059, 1.0102, 1.0016]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0024, 1.0035, 1.0165, 1.0048, 1.0002, 1.0027, 1.0016, 1.0062,\n",
      "         1.0031, 1.0039, 1.0040, 1.0033, 1.0014, 1.0082, 1.0046, 1.0010, 1.0012,\n",
      "         1.0157, 1.0027, 1.0027, 1.0262, 1.0006, 1.0035, 1.0034, 1.0048],\n",
      "        [0.0000, 1.0103, 1.0145, 1.0029, 1.0149, 1.0095, 1.0030, 1.0124, 1.0065,\n",
      "         1.0015, 1.0164, 1.0054, 1.0154, 1.0043, 1.1114, 1.0011, 1.0014, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0017, 1.1056, 1.0173, 1.0226, 1.0012, 1.0236, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0068, 1.0004, 1.0181, 1.0084, 1.0001, 1.0088, 1.0127, 1.0043,\n",
      "         1.0010, 1.0143, 1.0028, 1.0035, 1.0013, 1.0021, 1.0024, 1.0029, 1.0024,\n",
      "         1.0167, 1.0040, 1.0064, 1.0036, 1.0032]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0061, 1.0023, 1.0017, 1.0017, 1.0004, 1.0013, 1.0039, 1.0100,\n",
      "         1.0025, 1.0019, 1.0105, 1.0032, 1.0467, 1.0095, 1.0003, 1.0037, 1.0037,\n",
      "         1.0037, 1.0026, 1.0073, 1.0036, 1.0005, 1.0038, 1.0018, 1.0019, 1.0021,\n",
      "         1.0026, 1.0038],\n",
      "        [0.0000, 1.0012, 1.0005, 1.0448, 1.0020, 1.0027, 1.0045, 1.0003, 1.0030,\n",
      "         1.0008, 1.0303, 1.0014, 1.0032, 1.0003, 1.0006, 1.0498, 1.0035, 1.0012,\n",
      "         1.0008, 1.0016, 1.0002, 1.0006, 1.0003, 1.0038, 1.0264, 1.0013, 1.0056,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 53, 53])  attentions_grads shape: torch.Size([3, 2, 12, 53, 53])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 53, 53])\n",
      "joint_attentions shape: torch.Size([2, 53, 53])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0093, 1.0193, 1.0011, 1.0081, 1.0006, 1.0092, 1.0014, 1.0005,\n",
      "         1.0014, 1.0087, 1.0006, 1.0004, 1.0005, 1.0003, 1.0018, 1.0057, 1.0031,\n",
      "         1.0003, 1.0001, 1.0005, 1.0069, 1.0009, 1.0004, 1.0003, 1.0007, 1.0011,\n",
      "         1.0003, 1.0006, 1.0005, 1.0012, 1.0056, 1.0003, 1.0066, 1.0041, 1.0136,\n",
      "         1.0008, 1.0016, 1.0006, 1.0001, 1.0004, 1.0011, 1.0009, 1.0003, 1.0024,\n",
      "         1.0011, 1.0005, 1.0099, 1.0004, 1.0017, 1.0002, 1.0013, 1.0033],\n",
      "        [0.0000, 1.0238, 1.0141, 1.0174, 1.0250, 1.0177, 1.0080, 1.0205, 1.0430,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0017, 1.0162, 1.0100, 1.0025, 1.0008, 1.0044, 1.0014, 1.0029,\n",
      "         1.0009, 1.0013, 1.0031, 1.0029, 1.0018, 1.0019, 1.0004, 1.0127, 1.0009,\n",
      "         1.0043, 1.0013, 1.0004, 1.0012, 1.0138, 1.0029, 1.0008, 1.0083, 1.0011,\n",
      "         1.0014, 1.0018, 1.0019, 1.0017, 1.0140],\n",
      "        [0.0000, 1.0070, 1.0074, 1.0010, 1.0017, 1.0087, 1.0094, 1.0037, 1.0015,\n",
      "         1.0015, 1.0033, 1.0031, 1.0014, 1.0022, 1.0009, 1.0091, 1.0044, 1.0304,\n",
      "         1.0141, 1.0053, 1.0051, 1.0197, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0027, 1.0006, 1.0017, 1.0014, 1.0106, 1.0191, 1.0052, 1.0083,\n",
      "         1.0033, 1.0010, 1.0021, 1.0380, 1.0047, 1.0000, 1.0036, 1.0454, 1.0008,\n",
      "         1.0119, 1.0044, 1.0006, 1.0041, 1.0004, 1.0005, 1.0003, 1.0018, 1.0010],\n",
      "        [0.0000, 1.0010, 1.0134, 1.0016, 1.0125, 1.0016, 1.0036, 1.0008, 1.0002,\n",
      "         1.0007, 1.0118, 1.0131, 1.0188, 1.0025, 1.0019, 1.0085, 1.0010, 1.0122,\n",
      "         1.0022, 1.0021, 1.0025, 1.0092, 1.0035, 1.0162, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0010, 1.0008, 1.0008, 1.0094, 1.0023, 1.0008, 1.0286,\n",
      "         1.0015, 1.0241, 1.0074, 1.0008, 1.0096, 1.0049, 1.0081, 1.0066, 1.0032,\n",
      "         1.0022, 1.0016, 1.0008, 1.0007, 1.0008, 1.0098, 1.0012, 1.0040, 1.0022,\n",
      "         1.0231, 1.0022, 1.0035, 1.0684, 1.0169, 1.0012, 1.0012, 1.0030, 1.0004,\n",
      "         1.0005],\n",
      "        [0.0000, 1.0041, 1.0012, 1.0047, 1.0106, 1.0017, 1.0005, 1.0035, 1.0016,\n",
      "         1.0089, 1.0223, 1.0252, 1.0177, 1.0280, 1.0023, 1.0114, 1.0066, 1.0042,\n",
      "         1.0012, 1.0008, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0051, 1.0170, 1.0018, 1.0017, 1.0013, 1.0008, 1.0059, 1.0136,\n",
      "         1.0023, 1.0098, 1.0205, 1.0000, 1.0024, 1.0028, 1.0090, 1.0015, 1.0015,\n",
      "         1.0019, 1.0009, 1.0032, 1.0008, 1.0202, 1.0084, 1.0036, 1.0178, 1.0023,\n",
      "         1.0002, 1.0051, 1.0107],\n",
      "        [0.0000, 1.0146, 1.0104, 1.0027, 1.0370, 1.0005, 1.0024, 1.0702, 1.0277,\n",
      "         1.0037, 1.0105, 1.0434, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0004, 1.0033, 1.0038, 1.0083, 1.0065, 1.0006, 1.0006, 1.0168,\n",
      "         1.0008, 1.0012, 1.0018, 1.0037, 1.0008, 1.0009, 1.0048, 1.0009, 1.0704,\n",
      "         1.0035, 1.0006, 1.0003, 1.0015, 1.0016, 1.0011, 1.0051, 1.0070, 1.0057,\n",
      "         1.0073],\n",
      "        [0.0000, 1.0012, 1.0388, 1.0028, 1.0022, 1.0030, 1.0060, 1.0267, 1.0016,\n",
      "         1.0107, 1.0059, 1.0065, 1.0091, 1.0007, 1.0124, 1.0171, 1.0012, 1.0006,\n",
      "         1.0184, 1.0015, 1.0010, 1.0018, 1.0028, 1.0220, 1.0068, 1.0009, 1.0004,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0037, 1.0053, 1.0033, 1.0198, 1.0031, 1.0025, 1.0036, 1.0020,\n",
      "         1.0027, 1.0008, 1.0073, 1.0051, 1.0062, 1.0218, 1.0046, 1.0088, 1.0026,\n",
      "         1.0353, 1.0037, 1.0086, 1.0045, 1.0007, 1.0021, 1.0037, 1.0024, 1.0010,\n",
      "         1.0020, 1.0207, 1.0013, 1.0014, 1.0006, 1.0032, 1.0409, 1.0019, 1.0385,\n",
      "         1.0005, 1.0001, 1.0000],\n",
      "        [0.0000, 1.0120, 1.0027, 1.0008, 1.0107, 1.0019, 1.0018, 1.0058, 1.0015,\n",
      "         1.0031, 1.0009, 1.0014, 1.0020, 1.0006, 1.0006, 1.0031, 1.0022, 1.0021,\n",
      "         1.0064, 1.0035, 1.0017, 1.0042, 1.0212, 1.0023, 1.0007, 1.0007, 1.0054,\n",
      "         1.0003, 1.0052, 1.0004, 1.0011, 1.0008, 1.0250, 1.0124, 1.0056, 1.0016,\n",
      "         1.0006, 1.0031, 1.0072]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0015, 1.0020, 1.0072, 1.0039, 1.0024, 1.0021, 1.0004,\n",
      "         1.0011, 1.0039, 1.0048, 1.0039, 1.0007, 1.0017, 1.0157, 1.0034, 1.0066,\n",
      "         1.0030, 1.0015, 1.0029, 1.0009, 1.0522, 1.0069, 1.0041, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0061, 1.0050, 1.0000, 1.0056, 1.0006, 1.0004, 1.0004, 1.0001,\n",
      "         1.0023, 1.0004, 1.0408, 1.0104, 1.0004, 1.0022, 1.0004, 1.0031, 1.0016,\n",
      "         1.0206, 1.0010, 1.0098, 1.0000, 1.0041, 1.0011, 1.0016, 1.0001, 1.0000,\n",
      "         1.0020, 1.0080, 1.0009, 1.0001, 1.0007, 1.0184]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0034, 1.0109, 1.0091, 1.0050, 1.0059, 1.0129, 1.0016, 1.0017,\n",
      "         1.0007, 1.0044, 1.0009, 1.0009, 1.0007, 1.0005, 1.0038, 1.0012, 1.0081,\n",
      "         1.0037, 1.0276, 1.0023, 1.0007, 1.0306, 1.0008, 1.0032, 1.0462, 1.0023,\n",
      "         1.0025, 1.0155],\n",
      "        [0.0000, 1.0177, 1.0238, 1.0791, 1.0089, 1.0069, 1.0198, 1.0393, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0032, 1.0022, 1.0101, 1.0015, 1.0035, 1.0074, 1.0008, 1.0017,\n",
      "         1.0026, 1.0056, 1.0016, 1.0020, 1.0016, 1.0068, 1.0348, 1.0054, 1.0022,\n",
      "         1.0026, 1.0352, 1.0007, 1.0029, 1.0058, 1.0007, 1.0011, 1.0024, 1.0240],\n",
      "        [0.0000, 1.0051, 1.0088, 1.0052, 1.0490, 1.0047, 1.0997, 1.0465, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0098, 1.0046, 1.0242, 1.0571, 1.0100, 1.0041, 1.0376, 1.0126,\n",
      "         1.0056, 1.0070, 1.0029, 1.0022, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0064, 1.0018, 1.0047, 1.0055, 1.0012, 1.0031, 1.0117, 1.0029,\n",
      "         1.0262, 1.0014, 1.0398, 1.0165, 1.0018, 1.0083, 1.0014, 1.0016, 1.0491,\n",
      "         1.0057, 1.0300, 1.0165, 1.0026, 1.0022, 1.0004]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0005, 1.0052, 1.0036, 1.0059, 1.0001, 1.0104, 1.0010, 1.0006,\n",
      "         1.0016, 1.0015, 1.0194, 1.0022, 1.0039, 1.0023, 1.0011, 1.0062, 1.0073,\n",
      "         1.0000, 1.0039, 1.0132, 1.0021, 1.0123, 1.0006, 1.0008, 1.0090, 1.0040],\n",
      "        [0.0000, 1.0079, 1.0038, 1.0792, 1.0155, 1.0044, 1.0024, 1.0079, 1.0066,\n",
      "         1.0152, 1.0049, 1.0080, 1.0282, 1.0047, 1.0047, 1.0007, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 12, 12])  attentions_grads shape: torch.Size([3, 2, 12, 12, 12])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 12, 12])\n",
      "joint_attentions shape: torch.Size([2, 12, 12])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0048, 1.0199, 1.0050, 1.0135, 1.0806, 1.0038, 1.0012, 1.0099,\n",
      "         1.0414, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0114, 1.0103, 1.0253, 1.0040, 1.0040, 1.0045, 1.0132, 1.0054,\n",
      "         1.0724, 1.0068, 1.0013]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0086, 1.0042, 1.0052, 1.0088, 1.0010, 1.0196, 1.0200, 1.0005,\n",
      "         1.0311, 1.0108, 1.0075, 1.0073, 1.0061, 1.0125, 1.0022, 1.0020, 1.0352,\n",
      "         1.0165, 1.0010, 1.0087, 1.0004, 1.0009, 1.0093, 1.0011, 1.0014, 1.0174,\n",
      "         1.0067, 1.0008, 1.0052, 1.0006, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0037, 1.0031, 1.0006, 1.0092, 1.0091, 1.0011, 1.0007, 1.0163,\n",
      "         1.0034, 1.0125, 1.0084, 1.0013, 1.0291, 1.0084, 1.0009, 1.0078, 1.0426,\n",
      "         1.0007, 1.0059, 1.0008, 1.0005, 1.0009, 1.0068, 1.0010, 1.0035, 1.0009,\n",
      "         1.0016, 1.0052, 1.0004, 1.0059, 1.0009, 1.0009, 1.0096, 1.0005, 1.0001]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 16, 16])  attentions_grads shape: torch.Size([3, 2, 12, 16, 16])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 16, 16])\n",
      "joint_attentions shape: torch.Size([2, 16, 16])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0251, 1.0039, 1.0034, 1.0055, 1.0093, 1.0159, 1.0031, 1.0007,\n",
      "         1.0122, 1.0514, 1.0035, 1.0070, 1.0060, 1.0062, 1.0031],\n",
      "        [0.0000, 1.0081, 1.0440, 1.0433, 1.0199, 1.0171, 1.0057, 1.0034, 1.0291,\n",
      "         1.0290, 1.0098, 1.0006, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0042, 1.0027, 1.0029, 1.0033, 1.0025, 1.0074, 1.0129, 1.0040,\n",
      "         1.0044, 1.0027, 1.0005, 1.0030, 1.0003, 1.0014, 1.0022, 1.0071, 1.0010,\n",
      "         1.0023, 1.0181, 1.0004, 1.0148, 1.0097, 1.0264, 1.0014, 1.0014, 1.0011,\n",
      "         1.0007, 1.0019, 1.0024, 1.0102],\n",
      "        [0.0000, 1.0022, 1.0028, 1.0023, 1.0079, 1.0030, 1.0224, 1.0009, 1.0102,\n",
      "         1.0131, 1.0078, 1.0066, 1.0048, 1.0021, 1.0013, 1.0175, 1.0026, 1.0141,\n",
      "         1.0287, 1.0059, 1.0283, 1.0010, 1.0053, 1.0198, 1.0027, 1.0039, 1.0212,\n",
      "         1.0007, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 43, 43])  attentions_grads shape: torch.Size([3, 2, 12, 43, 43])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 43, 43])\n",
      "joint_attentions shape: torch.Size([2, 43, 43])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0089, 1.0043, 1.0130, 1.0015, 1.0330, 1.0047, 1.0191, 1.0029,\n",
      "         1.0043, 1.0068, 1.0013, 1.0118, 1.0053, 1.0068, 1.0267, 1.0010, 1.0011,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0043, 1.0025, 1.0054, 1.0094, 1.0030, 1.0010, 1.0341, 1.0033,\n",
      "         1.0018, 1.0057, 1.0028, 1.0046, 1.0005, 1.0023, 1.0021, 1.0038, 1.0026,\n",
      "         1.0009, 1.0041, 1.0032, 1.0002, 1.0028, 1.0012, 1.0045, 1.0010, 1.0021,\n",
      "         1.0000, 1.0007, 1.0068, 1.0001, 1.0010, 1.0006, 1.0030, 1.0000, 1.0016,\n",
      "         1.0049, 1.0001, 1.0008, 1.0012, 1.0061, 1.0037, 1.0053]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0075, 1.0013, 1.0011, 1.0033, 1.0123, 1.0035, 1.0241, 1.0029,\n",
      "         1.0008, 1.0006, 1.0021, 1.0012, 1.0112, 1.0021, 1.0023, 1.0103, 1.0012,\n",
      "         1.0023, 1.0021, 1.0093, 1.0018, 1.0016, 1.0016, 1.0206, 1.0068, 1.0081,\n",
      "         1.0227, 1.0216, 1.0245, 1.0086, 1.0005, 1.0003, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0112, 1.0083, 1.0029, 1.0017, 1.0083, 1.0018, 1.0003, 1.0015,\n",
      "         1.0033, 1.0024, 1.0003, 1.0062, 1.0050, 1.0019, 1.0003, 1.0016, 1.0003,\n",
      "         1.0010, 1.0051, 1.0007, 1.0027, 1.0035, 1.0009, 1.0005, 1.0013, 1.0001,\n",
      "         1.0007, 1.0012, 1.0006, 1.0033, 1.0004, 1.0063, 1.0013, 1.0012, 1.0071,\n",
      "         1.0005, 1.0030, 1.0034, 1.0016, 1.0105, 1.0010, 1.0028, 1.0276]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0171, 1.0013, 1.0021, 1.0023, 1.0056, 1.0083, 1.0009, 1.0008,\n",
      "         1.0008, 1.0281, 1.0004, 1.0022, 1.0012, 1.0018, 1.0002, 1.0036, 1.0042,\n",
      "         1.0006, 1.0004, 1.0017, 1.0045, 1.0015, 1.0001, 1.0056, 1.0055, 1.0290,\n",
      "         1.0003, 1.0001, 1.0004, 1.0019, 1.0139],\n",
      "        [0.0000, 1.0014, 1.0102, 1.0064, 1.0188, 1.0106, 1.0040, 1.0327, 1.0021,\n",
      "         1.0070, 1.0256, 1.0056, 1.0040, 1.0017, 1.0032, 1.0158, 1.0032, 1.0429,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0126, 1.0162, 1.0005, 1.0142, 1.0018, 1.0026, 1.0036, 1.0024,\n",
      "         1.0015, 1.0102, 1.0022, 1.0029, 1.0214, 1.0028, 1.0040, 1.0019, 1.0012,\n",
      "         1.0043, 1.0035, 1.0034, 1.0046, 1.0029, 1.0005, 1.0008, 1.0006, 1.0006,\n",
      "         1.0012, 1.0022, 1.0006, 1.0037, 1.0438],\n",
      "        [0.0000, 1.0329, 1.0064, 1.0023, 1.0020, 1.0142, 1.0009, 1.0188, 1.0026,\n",
      "         1.0007, 1.0043, 1.0057, 1.0051, 1.0271, 1.0140, 1.0230, 1.0010, 1.0196,\n",
      "         1.0009, 1.0304, 1.0100, 1.0069, 1.0109, 1.0011, 1.0005, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0081, 1.0026, 1.0405, 1.0014, 1.0326, 1.0041, 1.0061, 1.0010,\n",
      "         1.0124, 1.0008, 1.0126, 1.0050, 1.0012, 1.0063, 1.0060, 1.0009, 1.0312,\n",
      "         1.0011, 1.0053, 1.0153, 1.0019, 1.0038, 1.0115, 1.0014, 1.0189, 1.0123,\n",
      "         1.0006, 1.0006],\n",
      "        [0.0000, 1.0161, 1.0060, 1.0499, 1.0185, 1.0155, 1.0059, 1.0061, 1.0035,\n",
      "         1.0173, 1.0095, 1.0262, 1.0037, 1.0314, 1.0056, 1.0022, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0389, 1.0098, 1.0071, 1.0126, 1.0048, 1.0521, 1.0029, 1.0045,\n",
      "         1.0169, 1.0119, 1.0053, 1.0467, 1.0243, 1.0020, 1.0031, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0016, 1.0052, 1.0166, 1.0208, 1.0086, 1.0332, 1.0023, 1.0014,\n",
      "         1.0036, 1.0076, 1.0986, 1.0143, 1.0160, 1.0336, 1.0031, 1.0026, 1.0073]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0037, 1.0057, 1.0022, 1.0083, 1.0024, 1.0457, 1.0065, 1.0187,\n",
      "         1.0037, 1.1098, 1.0027, 1.0020, 1.0260, 1.0086, 1.0258, 1.0020, 1.0032],\n",
      "        [0.0000, 1.0049, 1.0267, 1.0049, 1.0096, 1.1249, 1.0007, 1.0061, 1.0023,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0008, 1.0044, 1.0008, 1.0014, 1.0025, 1.0168, 1.0005,\n",
      "         1.0009, 1.0050, 1.0017, 1.0117, 1.0005, 1.0134, 1.0019, 1.0062, 1.0024,\n",
      "         1.0466, 1.0047, 1.0004, 1.0011, 1.0059, 1.0116, 1.0038, 1.0066, 1.0056,\n",
      "         1.0105, 1.0424, 1.0082, 1.0022, 1.0013, 1.0007, 1.0178, 1.0015, 1.0027,\n",
      "         1.0097, 1.0004, 1.0002],\n",
      "        [0.0000, 1.0263, 1.0042, 1.0077, 1.0051, 1.0029, 1.0041, 1.0011, 1.0330,\n",
      "         1.0130, 1.0206, 1.0021, 1.0006, 1.0079, 1.0082, 1.0008, 1.0070, 1.0165,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0008, 1.0084, 1.0049, 1.0351, 1.0014, 1.0013, 1.0258, 1.0007,\n",
      "         1.0088, 1.0074, 1.0009, 1.0080, 1.0005, 1.0029, 1.0047, 1.0041, 1.0029,\n",
      "         1.0037, 1.0003, 1.0008, 1.0001],\n",
      "        [0.0000, 1.0155, 1.0742, 1.0014, 1.0076, 1.0029, 1.0040, 1.0023, 1.0182,\n",
      "         1.0014, 1.0000, 1.0124, 1.0201, 1.0022, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0070, 1.0084, 1.0121, 1.0045, 1.0341, 1.0055, 1.0036, 1.0406,\n",
      "         1.0249, 1.0270, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0313, 1.0033, 1.0018, 1.0031, 1.0116, 1.0736, 1.0037, 1.0191,\n",
      "         1.0082, 1.0018, 1.0077, 1.0209, 1.0432, 1.0212, 1.0020, 1.0008, 1.0010]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0103, 1.0288, 1.0015, 1.0058, 1.0044, 1.0043, 1.0023, 1.0237,\n",
      "         1.0111, 1.0009, 1.0150, 1.0026, 1.0095, 1.0062, 1.0291, 1.0006, 1.0011,\n",
      "         1.0165, 1.0006, 1.0130, 1.0138, 1.0062, 1.0024, 1.0020, 1.0005],\n",
      "        [0.0000, 1.0284, 1.0513, 1.0064, 1.0109, 1.0060, 1.0216, 1.0404, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0076, 1.0201, 1.0014, 1.0250, 1.0011, 1.0010, 1.0012, 1.0028,\n",
      "         1.0011, 1.0048, 1.0010, 1.0080, 1.0108, 1.0019, 1.0012, 1.0052, 1.0008,\n",
      "         1.0277, 1.0030, 1.0121, 1.0055, 1.0171, 1.0261, 1.0011, 1.0530, 1.0316,\n",
      "         1.0014, 1.0005],\n",
      "        [0.0000, 1.0240, 1.0038, 1.0087, 1.0089, 1.0099, 1.0234, 1.0146, 1.0022,\n",
      "         1.0026, 1.0206, 1.0006, 1.0189, 1.0109, 1.0090, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0059, 1.0058, 1.0007, 1.0015, 1.0066, 1.0006, 1.0014, 1.0085,\n",
      "         1.0408, 1.0439, 1.0118, 1.0013, 1.0146, 1.0013, 1.0025, 1.0008, 1.0193,\n",
      "         1.0164, 1.0026, 1.0040, 1.0024, 1.0009, 1.0006, 1.0215, 1.0029, 1.0058,\n",
      "         1.0088, 1.0208, 1.0007, 1.0002],\n",
      "        [0.0000, 1.0406, 1.0029, 1.0140, 1.0166, 1.0090, 1.0083, 1.0169, 1.0043,\n",
      "         1.0029, 1.0143, 1.0269, 1.0253, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0034, 1.0111, 1.0052, 1.0033, 1.0015, 1.0366, 1.0012, 1.0197,\n",
      "         1.0398, 1.0010, 1.0024, 1.0016, 1.0448, 1.0058, 1.0158, 1.0014, 1.0017,\n",
      "         1.0041, 1.0080, 1.0034, 1.0090, 1.0043, 1.0011],\n",
      "        [0.0000, 1.0025, 1.0300, 1.0014, 1.0034, 1.0149, 1.0127, 1.0156, 1.0014,\n",
      "         1.0027, 1.0179, 1.0179, 1.0227, 1.0016, 1.0019, 1.0218, 1.0026, 1.0013,\n",
      "         1.0085, 1.0239, 1.0013, 1.0020, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0013, 1.0109, 1.0033, 1.0146, 1.0025, 1.0058, 1.0267, 1.0006,\n",
      "         1.0041, 1.0142, 1.0003, 1.0035, 1.0001, 1.0166, 1.0009, 1.0011, 1.0041,\n",
      "         1.0010, 1.0002, 1.0073, 1.0007, 1.0006, 1.0016, 1.0020, 1.0081, 1.0004,\n",
      "         1.0021, 1.0012, 1.0010, 1.0010, 1.0364],\n",
      "        [0.0000, 1.0057, 1.0013, 1.0226, 1.0045, 1.0299, 1.0013, 1.0030, 1.0017,\n",
      "         1.0212, 1.0040, 1.0036, 1.0270, 1.0012, 1.0117, 1.0013, 1.0005, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 42, 42])  attentions_grads shape: torch.Size([3, 2, 12, 42, 42])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 42, 42])\n",
      "joint_attentions shape: torch.Size([2, 42, 42])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0026, 1.0165, 1.0023, 1.0020, 1.0011, 1.0050, 1.0019, 1.0005,\n",
      "         1.0059, 1.0033, 1.0038, 1.0044, 1.0183, 1.0066, 1.0004, 1.0034, 1.0302,\n",
      "         1.0024, 1.0097, 1.0009, 1.0150, 1.0019, 1.0055, 1.0050, 1.0066, 1.0049,\n",
      "         1.0139, 1.0007, 1.0029, 1.0005, 1.0032, 1.0021, 1.0010, 1.0049, 1.0024,\n",
      "         1.0043, 1.0065, 1.0030, 1.0339, 1.0013, 1.0002],\n",
      "        [0.0000, 1.0145, 1.0003, 1.0051, 1.0107, 1.0013, 1.0055, 1.0013, 1.0053,\n",
      "         1.0047, 1.0017, 1.0061, 1.0011, 1.0027, 1.0140, 1.0095, 1.0007, 1.0079,\n",
      "         1.0069, 1.0044, 1.0131, 1.0329, 1.0253, 1.0058, 1.0001, 1.0046, 1.0052,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0040, 1.0012, 1.0133, 1.0026, 1.0029, 1.0009, 1.0714,\n",
      "         1.0014, 1.0009, 1.0030, 1.0043, 1.0011, 1.0020, 1.0040, 1.0003, 1.0059,\n",
      "         1.0022, 1.0104, 1.0010, 1.0120, 1.0281, 1.0057, 1.0018, 1.0064],\n",
      "        [0.0000, 1.0635, 1.0058, 1.0021, 1.0091, 1.0005, 1.0423, 1.0470, 1.0056,\n",
      "         1.0183, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0041, 1.0107, 1.0013, 1.0036, 1.0019, 1.0042, 1.0006, 1.0223,\n",
      "         1.0025, 1.0336, 1.0033, 1.0149, 1.0010, 1.0050, 1.0320, 1.0221, 1.0026,\n",
      "         1.0006, 1.0267, 1.0008, 1.0004, 1.0010, 1.0016, 1.0187, 1.0019, 1.0008,\n",
      "         1.0047, 1.0044, 1.0012, 1.0005, 1.0060, 1.0008, 1.0090, 1.0005, 1.0001],\n",
      "        [0.0000, 1.0128, 1.0008, 1.0017, 1.0033, 1.0094, 1.0009, 1.0009, 1.0003,\n",
      "         1.0021, 1.0015, 1.0044, 1.0021, 1.0038, 1.0072, 1.0007, 1.0054, 1.0015,\n",
      "         1.0016, 1.0042, 1.0039, 1.0050, 1.0012, 1.0010, 1.0063, 1.0010, 1.0052,\n",
      "         1.0006, 1.0035, 1.0236, 1.0019, 1.0016, 1.0215, 1.0021, 1.0022, 1.0167]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0506, 1.0298, 1.0061, 1.0350, 1.0045, 1.0075, 1.0276, 1.0217,\n",
      "         1.0031, 1.0145, 1.0254, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0075, 1.0086, 1.0043, 1.0014, 1.0094, 1.0031, 1.0020, 1.0020,\n",
      "         1.0020, 1.0033, 1.0017, 1.0027, 1.0017, 1.0045, 1.0027, 1.0026, 1.0027,\n",
      "         1.0031, 1.0081, 1.0039, 1.0104, 1.0016, 1.0034, 1.0044, 1.0038, 1.0023,\n",
      "         1.0026, 1.0339]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0177, 1.1114, 1.0372, 1.0375, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0020, 1.1055, 1.0235, 1.0010, 1.0096, 1.0027, 1.0066, 1.0001,\n",
      "         1.0125, 1.0020, 1.0023, 1.0010, 1.0003, 1.0000, 1.0011, 1.0040, 1.0041,\n",
      "         1.0023, 1.0025, 1.0014, 1.0013, 1.0042, 1.0090, 1.0006, 1.0052, 1.0119]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 28, 28])  attentions_grads shape: torch.Size([3, 2, 12, 28, 28])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 28, 28])\n",
      "joint_attentions shape: torch.Size([2, 28, 28])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0016, 1.0069, 1.0021, 1.0003, 1.0033, 1.0161, 1.0082, 1.0584,\n",
      "         1.0018, 1.0007, 1.0014, 1.0010, 1.0008, 1.0016, 1.0080, 1.0032, 1.0009,\n",
      "         1.0007, 1.0012, 1.0014, 1.0039, 1.0007, 1.0029, 1.0090, 1.0012, 1.0027,\n",
      "         1.0113],\n",
      "        [0.0000, 1.0192, 1.0263, 1.0019, 1.0130, 1.0192, 1.0269, 1.0265, 1.0048,\n",
      "         1.0225, 1.0043, 1.0306, 1.0028, 1.0051, 1.0023, 1.0104, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0109, 1.0082, 1.0217, 1.0070, 1.0068, 1.0020, 1.0017,\n",
      "         1.0339, 1.0049, 1.0026, 1.0015, 1.0214, 1.0097, 1.0016, 1.0004, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0045, 1.0109, 1.0098, 1.0124, 1.0092, 1.0095, 1.0473, 1.0052,\n",
      "         1.0087, 1.0085, 1.0015, 1.0099, 1.0121, 1.0081, 1.0022, 1.0020, 1.0017,\n",
      "         1.0079, 1.0223]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0037, 1.0715, 1.0309, 1.0851, 1.0245, 1.0060, 1.0094, 1.0034,\n",
      "         1.0059, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0013, 1.0060, 1.0043, 1.0025, 1.0216, 1.0130, 1.0025, 1.0023,\n",
      "         1.0017, 1.0093, 1.0015, 1.0016, 1.0021, 1.0010, 1.0524, 1.0028, 1.0013,\n",
      "         1.0060, 1.0008, 1.0040, 1.0023, 1.0016]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0062, 1.0032, 1.0008, 1.0007, 1.0096, 1.0013, 1.0021, 1.0029,\n",
      "         1.0020, 1.0036, 1.0099, 1.0006, 1.0078, 1.0010, 1.0055, 1.0103, 1.0147,\n",
      "         1.0042, 1.0036, 1.0034, 1.0306, 1.0036, 1.0014, 1.0013, 1.0121, 1.0057,\n",
      "         1.0006, 1.0088, 1.0053, 1.0055, 1.0129, 1.0009, 1.0005],\n",
      "        [0.0000, 1.0038, 1.0022, 1.0060, 1.0296, 1.0022, 1.0131, 1.0004, 1.0067,\n",
      "         1.0019, 1.0012, 1.0046, 1.0023, 1.0050, 1.0604, 1.0001, 1.0016, 1.0019,\n",
      "         1.0049, 1.0020, 1.0031, 1.0091, 1.0125, 1.0005, 1.0003, 1.0005, 1.0002,\n",
      "         1.0018, 1.0155, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0056, 1.0096, 1.0084, 1.1109, 1.0006, 1.0327, 1.0040, 1.0039,\n",
      "         1.0033, 1.0056, 1.0123, 1.0084, 1.0045, 1.0036, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0026, 1.0063, 1.0042, 1.0192, 1.0029, 1.0011, 1.0231,\n",
      "         1.0010, 1.0072, 1.0105, 1.0011, 1.0027, 1.0059, 1.0007, 1.0013, 1.0038,\n",
      "         1.0008, 1.0013, 1.0503, 1.0007, 1.0043, 1.0214, 1.0013, 1.0022, 1.0110,\n",
      "         1.0044, 1.0195, 1.0065, 1.0005, 1.0004]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 38, 38])  attentions_grads shape: torch.Size([3, 2, 12, 38, 38])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 38, 38])\n",
      "joint_attentions shape: torch.Size([2, 38, 38])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0014, 1.0017, 1.0005, 1.0289, 1.0075, 1.0009, 1.0039, 1.0008,\n",
      "         1.0027, 1.0007, 1.0046, 1.0264, 1.0010, 1.0020, 1.0092, 1.0010, 1.0260,\n",
      "         1.0012, 1.0012, 1.0120, 1.0007, 1.0100, 1.0009, 1.0103, 1.0084, 1.0026,\n",
      "         1.0055, 1.0006, 1.0132, 1.0015, 1.0007, 1.0032, 1.0005, 1.0006, 1.0193,\n",
      "         1.0004, 1.0002],\n",
      "        [0.0000, 1.0045, 1.0063, 1.0260, 1.0020, 1.0111, 1.0086, 1.0154, 1.0358,\n",
      "         1.0012, 1.0125, 1.0008, 1.0011, 1.0191, 1.0008, 1.0047, 1.0031, 1.0155,\n",
      "         1.0009, 1.0016, 1.0116, 1.0160, 1.0054, 1.0017, 1.0039, 1.0166, 1.0015,\n",
      "         1.0004, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 19, 19])  attentions_grads shape: torch.Size([3, 2, 12, 19, 19])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 19, 19])\n",
      "joint_attentions shape: torch.Size([2, 19, 19])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0045, 1.0044, 1.0048, 1.0152, 1.0150, 1.0098, 1.0049, 1.0014,\n",
      "         1.0179, 1.0096, 1.0018, 1.0035, 1.0628, 1.0247, 1.0135, 1.0105, 1.0029,\n",
      "         1.0005],\n",
      "        [0.0000, 1.1348, 1.0248, 1.0092, 1.0237, 1.0199, 1.0207, 1.0044, 1.0044,\n",
      "         1.0010, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0057, 1.0032, 1.0067, 1.0440, 1.0061, 1.0006, 1.0091, 1.0123,\n",
      "         1.0029, 1.0016, 1.0081, 1.0050, 1.0042, 1.0121, 1.0011, 1.0036, 1.0222,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0228, 1.0016, 1.0020, 1.0077, 1.0186, 1.0121, 1.0165, 1.0006,\n",
      "         1.0163, 1.0081, 1.0030, 1.0221, 1.0061, 1.0036, 1.0011, 1.0084, 1.0607,\n",
      "         1.0296, 1.0025, 1.0008]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0079, 1.0042, 1.0022, 1.0086, 1.0485, 1.0052, 1.0004, 1.0411,\n",
      "         1.0033, 1.0030, 1.0014, 1.0111, 1.0046, 1.0076, 1.0142, 1.0055, 1.0067,\n",
      "         1.0010, 1.0062, 1.0047, 1.0133, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0029, 1.0055, 1.0054, 1.0008, 1.0009, 1.0007, 1.0134, 1.0007,\n",
      "         1.0012, 1.0006, 1.0316, 1.0019, 1.0209, 1.0036, 1.0201, 1.0009, 1.0205,\n",
      "         1.0003, 1.0005, 1.0094, 1.0058, 1.0078, 1.0008, 1.0007, 1.0008, 1.0039,\n",
      "         1.0018, 1.0006, 1.0072, 1.0006, 1.0014, 1.0199, 1.0020, 1.0108, 1.0006,\n",
      "         1.0088, 1.0075, 1.0017, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0028, 1.0150, 1.0026, 1.0010, 1.0123, 1.0015, 1.0018, 1.0003,\n",
      "         1.0107, 1.0016, 1.0076, 1.0032, 1.0726, 1.0051, 1.0028, 1.0025, 1.0009,\n",
      "         1.0005, 1.0110, 1.0124, 1.0331, 1.0020, 1.0014, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0019, 1.0361, 1.0004, 1.0011, 1.0201, 1.0002, 1.0012, 1.0054,\n",
      "         1.0019, 1.0031, 1.0008, 1.0219, 1.0020, 1.0004, 1.0006, 1.0071, 1.0013,\n",
      "         1.0053, 1.0109, 1.0076, 1.0015, 1.0005, 1.0024, 1.0022, 1.0058, 1.0032,\n",
      "         1.0010, 1.0013, 1.0008, 1.0032, 1.0031, 1.0011, 1.0011, 1.0051, 1.0005,\n",
      "         1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0051, 1.0133, 1.0044, 1.0089, 1.0014, 1.0141, 1.0082, 1.0014,\n",
      "         1.0192, 1.0041, 1.0017, 1.0054, 1.0022, 1.0136, 1.0060, 1.0041, 1.0011,\n",
      "         1.0282, 1.0196, 1.0006],\n",
      "        [0.0000, 1.0725, 1.0561, 1.0039, 1.0681, 1.0040, 1.0195, 1.0039, 1.0010,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0025, 1.0017, 1.0015, 1.0012, 1.0033, 1.0010, 1.0048,\n",
      "         1.0060, 1.0070, 1.0008, 1.0055, 1.0001, 1.0078, 1.0034, 1.0083, 1.0001,\n",
      "         1.0001, 1.0043, 1.0005, 1.0008, 1.0017, 1.0183, 1.0007, 1.0005, 1.0006,\n",
      "         1.0059, 1.0031, 1.0153, 1.0041, 1.0291],\n",
      "        [0.0000, 1.0177, 1.0159, 1.0014, 1.0031, 1.0183, 1.0066, 1.0088, 1.0089,\n",
      "         1.0017, 1.0233, 1.0011, 1.0028, 1.0091, 1.0014, 1.0018, 1.0215, 1.0031,\n",
      "         1.0213, 1.0087, 1.0012, 1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0072, 1.0007, 1.0150, 1.0008, 1.0116, 1.0022, 1.0354,\n",
      "         1.0004, 1.0015, 1.0008, 1.0008, 1.0364, 1.0018, 1.0005, 1.0068, 1.0110,\n",
      "         1.0008, 1.0002, 1.0010, 1.0015, 1.0010, 1.0092, 1.0025, 1.0017, 1.0023,\n",
      "         1.0006, 1.0110, 1.0007, 1.0050, 1.0008, 1.0013, 1.0012, 1.0016, 1.0004,\n",
      "         1.0002],\n",
      "        [0.0000, 1.0320, 1.0081, 1.0087, 1.0249, 1.0141, 1.0086, 1.0271, 1.0036,\n",
      "         1.0321, 1.0129, 1.0033, 1.0509, 1.0067, 1.0053, 1.0070, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0292, 1.0111, 1.0012, 1.0108, 1.0068, 1.0288, 1.0051,\n",
      "         1.0226, 1.0008, 1.0069, 1.0033, 1.0362, 1.0087, 1.0024, 1.0017, 1.0019,\n",
      "         1.0072, 1.0177, 1.0064, 1.0064, 1.0079, 1.0022, 1.0005],\n",
      "        [0.0000, 1.0051, 1.0014, 1.0220, 1.0145, 1.0256, 1.0153, 1.0264, 1.0006,\n",
      "         1.0020, 1.0313, 1.0011, 1.0145, 1.0391, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0140, 1.0015, 1.0102, 1.0032, 1.0013, 1.0120, 1.0291, 1.0120,\n",
      "         1.0016, 1.0020, 1.0006, 1.0016, 1.0014, 1.0055, 1.0052, 1.0015, 1.0015,\n",
      "         1.0164, 1.0039, 1.0002, 1.0043, 1.0042, 1.0103, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0119, 1.0076, 1.0009, 1.0385, 1.0013, 1.0015, 1.0020, 1.0040,\n",
      "         1.0074, 1.0013, 1.0015, 1.0004, 1.0166, 1.0017, 1.0007, 1.0054, 1.0006,\n",
      "         1.0058, 1.0024, 1.0161, 1.0010, 1.0145, 1.0010, 1.0157, 1.0098, 1.0056,\n",
      "         1.0235, 1.0111, 1.0010, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0018, 1.0100, 1.0031, 1.0282, 1.0020, 1.0064, 1.0037, 1.0158,\n",
      "         1.0182, 1.0069, 1.0029, 1.0182, 1.0021, 1.0401, 1.0023, 1.0021, 1.0033,\n",
      "         1.0023, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0085, 1.0064, 1.0082, 1.0008, 1.0191, 1.0019, 1.0113, 1.0295,\n",
      "         1.0035, 1.0053, 1.0077, 1.0158, 1.0006, 1.0067, 1.0008, 1.0027, 1.0061,\n",
      "         1.0147, 1.0006, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0197, 1.0256, 1.0127, 1.0104, 1.0108, 1.0042, 1.0073, 1.0090,\n",
      "         1.0376, 1.0035, 1.0060, 1.0099, 1.0087, 1.0051, 1.0205, 1.0426, 1.0016,\n",
      "         1.0045, 1.0000],\n",
      "        [0.0000, 1.0024, 1.0024, 1.0028, 1.0526, 1.0218, 1.0137, 1.0087, 1.0046,\n",
      "         1.0041, 1.0018, 1.0028, 1.0121, 1.0037, 1.0042, 1.0010, 1.0020, 1.0103,\n",
      "         1.0066, 1.0355]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0024, 1.0024, 1.0021, 1.0047, 1.0011, 1.0021, 1.0008, 1.0108,\n",
      "         1.0011, 1.0024, 1.0013, 1.0006, 1.0022, 1.0008, 1.0005, 1.0035, 1.0018,\n",
      "         1.0019, 1.0035, 1.0070, 1.0003, 1.0066, 1.0000, 1.0005, 1.0058, 1.0322,\n",
      "         1.0003, 1.0173, 1.0003, 1.0256, 1.0058, 1.0009, 1.0022],\n",
      "        [0.0000, 1.0110, 1.0020, 1.0077, 1.0019, 1.0150, 1.0120, 1.0007, 1.0107,\n",
      "         1.0183, 1.0010, 1.0026, 1.0028, 1.0038, 1.0015, 1.0007, 1.0016, 1.0027,\n",
      "         1.0063, 1.0012, 1.0266, 1.0012, 1.0006, 1.0055, 1.0002, 1.0128, 1.0047,\n",
      "         1.0186, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0081, 1.0046, 1.0019, 1.0020, 1.0061, 1.0239, 1.0022, 1.0126,\n",
      "         1.0035, 1.0075, 1.0045, 1.0197, 1.0183, 1.0053, 1.0447, 1.0042, 1.0271,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0013, 1.0240, 1.0063, 1.0022, 1.0085, 1.0007, 1.0014, 1.0007,\n",
      "         1.0041, 1.0070, 1.0006, 1.0108, 1.0011, 1.0029, 1.0003, 1.0128, 1.0016,\n",
      "         1.0117, 1.0110, 1.0018, 1.0016, 1.0039, 1.0028, 1.0105, 1.0015, 1.0012,\n",
      "         1.0011, 1.0024, 1.0037, 1.0006, 1.0043, 1.0029, 1.0018, 1.0022, 1.0123,\n",
      "         1.0014, 1.0003, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0020, 1.0014, 1.0008, 1.0026, 1.0032, 1.0003, 1.0018, 1.0007,\n",
      "         1.0001, 1.0009, 1.0033, 1.0007, 1.0027, 1.0029, 1.0013, 1.0005, 1.0160,\n",
      "         1.0134, 1.0072, 1.0029, 1.0003, 1.0012, 1.0620, 1.0003, 1.0011, 1.0005,\n",
      "         1.0016, 1.0049, 1.0167, 1.0064, 1.0076, 1.0005, 1.0013, 1.0002, 1.0009,\n",
      "         1.0008, 1.0022, 1.0009],\n",
      "        [0.0000, 1.0057, 1.0063, 1.0012, 1.0463, 1.0050, 1.0112, 1.0046, 1.0011,\n",
      "         1.0094, 1.0027, 1.0240, 1.0009, 1.0020, 1.0027, 1.0033, 1.0027, 1.0017,\n",
      "         1.0020, 1.0544, 1.0003, 1.0041, 1.0183, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0975, 1.0101, 1.0036, 1.0009, 1.0293, 1.0092, 1.0015, 1.0116,\n",
      "         1.0310, 1.0071, 1.0039, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0088, 1.0044, 1.0011, 1.0052, 1.0028, 1.0022, 1.0025, 1.0139,\n",
      "         1.0022, 1.0026, 1.0152, 1.0018, 1.0010, 1.0010, 1.0030, 1.0011, 1.0127,\n",
      "         1.0100, 1.0029, 1.0026, 1.0017, 1.0067, 1.0008, 1.0056, 1.0006, 1.0025,\n",
      "         1.0002, 1.0016, 1.0013]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0006, 1.0065, 1.0287, 1.0083, 1.0013, 1.0090, 1.0025, 1.0018,\n",
      "         1.0123, 1.0140, 1.0046, 1.0012, 1.0063, 1.0220, 1.0105, 1.0043, 1.0040,\n",
      "         1.0021, 1.0008, 1.0008, 1.0049, 1.0038, 1.0005, 1.0085, 1.0470, 1.0128,\n",
      "         1.0008, 1.0013, 1.0024, 1.0022, 1.0155, 1.0007, 1.0003],\n",
      "        [0.0000, 1.0111, 1.0024, 1.0141, 1.0118, 1.0020, 1.0251, 1.0090, 1.0105,\n",
      "         1.0029, 1.0123, 1.0033, 1.0013, 1.0011, 1.0609, 1.0208, 1.0016, 1.0015,\n",
      "         1.0035, 1.0010, 1.0010, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 43, 43])  attentions_grads shape: torch.Size([3, 2, 12, 43, 43])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 43, 43])\n",
      "joint_attentions shape: torch.Size([2, 43, 43])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0045, 1.0128, 1.0035, 1.0009, 1.0007, 1.0026, 1.0139, 1.0049,\n",
      "         1.0016, 1.0320, 1.0087, 1.0211, 1.0006, 1.0043, 1.0053, 1.0020, 1.0034,\n",
      "         1.0068, 1.0058, 1.0015, 1.0005, 1.0231, 1.0035, 1.0013, 1.0006, 1.0201,\n",
      "         1.0070, 1.0005, 1.0016, 1.0008, 1.0111, 1.0004, 1.0011, 1.0107, 1.0003,\n",
      "         1.0002, 1.0253, 1.0007, 1.0004, 1.0003, 1.0003, 1.0001],\n",
      "        [0.0000, 1.0026, 1.0031, 1.0037, 1.0222, 1.0066, 1.0266, 1.0084, 1.0043,\n",
      "         1.0365, 1.0130, 1.0027, 1.0007, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0013, 1.0046, 1.0017, 1.0028, 1.0023, 1.0065, 1.0022, 1.0001,\n",
      "         1.0068, 1.0243, 1.0013, 1.0004, 1.0013, 1.0148, 1.0012, 1.0018, 1.0075,\n",
      "         1.0501, 1.0179, 1.0021, 1.0042, 1.0013, 1.0002, 1.0180, 1.0104, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0017, 1.0021, 1.0019, 1.0004, 1.0040, 1.0010, 1.0013, 1.0177,\n",
      "         1.0023, 1.0009, 1.0017, 1.0004, 1.0018, 1.0010, 1.0118, 1.0045, 1.0006,\n",
      "         1.0013, 1.0002, 1.0007, 1.0002, 1.0009, 1.0011, 1.0010, 1.0023, 1.0044,\n",
      "         1.0004, 1.0008, 1.0166, 1.0050, 1.0002, 1.0073, 1.0006, 1.0021, 1.0015,\n",
      "         1.0004, 1.0055, 1.0019, 1.0137, 1.0007, 1.0009, 1.0023, 1.0005]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 49, 49])  attentions_grads shape: torch.Size([3, 2, 12, 49, 49])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 49, 49])\n",
      "joint_attentions shape: torch.Size([2, 49, 49])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0023, 1.0024, 1.0022, 1.0014, 1.0007, 1.0014, 1.0034, 1.0007,\n",
      "         1.0029, 1.0087, 1.0107, 1.0030, 1.0029, 1.0091, 1.0094, 1.0082, 1.0066,\n",
      "         1.0134, 1.0061, 1.0021, 1.0074, 1.0056, 1.0010, 1.0049, 1.0121, 1.0101,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0016, 1.0056, 1.0034, 1.0025, 1.0030, 1.0085, 1.0002, 1.0017,\n",
      "         1.0024, 1.0013, 1.0004, 1.0029, 1.0253, 1.0010, 1.0007, 1.0082, 1.0014,\n",
      "         1.0024, 1.0000, 1.0005, 1.0004, 1.0001, 1.0006, 1.0000, 1.0032, 1.0024,\n",
      "         1.0023, 1.0003, 1.0006, 1.0017, 1.0220, 1.0015, 1.0011, 1.0008, 1.0006,\n",
      "         1.0017, 1.0000, 1.0010, 1.0014, 1.0001, 1.0049, 1.0008, 1.0022, 1.0144,\n",
      "         1.0013, 1.0092, 1.0014, 1.0196]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0074, 1.0321, 1.0011, 1.0091, 1.0148, 1.0014, 1.0030,\n",
      "         1.0011, 1.0000, 1.0007, 1.0005, 1.0058, 1.0053, 1.0005, 1.0033, 1.0008,\n",
      "         1.0129, 1.0009, 1.0189, 1.0010, 1.0002, 1.0021, 1.0016, 1.0026, 1.0075],\n",
      "        [0.0000, 1.0054, 1.0025, 1.0031, 1.0004, 1.0024, 1.0133, 1.0018, 1.0180,\n",
      "         1.0041, 1.0012, 1.0344, 1.0209, 1.0006, 1.0014, 1.0011, 1.0331, 1.0005,\n",
      "         1.0024, 1.0018, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 16, 16])  attentions_grads shape: torch.Size([3, 2, 12, 16, 16])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 16, 16])\n",
      "joint_attentions shape: torch.Size([2, 16, 16])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0229, 1.0134, 1.0056, 1.0015, 1.0161, 1.0262, 1.0513, 1.0465,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0608, 1.0004, 1.0058, 1.0035, 1.0510, 1.0213, 1.0053, 1.0016,\n",
      "         1.0037, 1.0003, 1.0041, 1.0003, 1.0096, 1.0097, 1.0111]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0043, 1.0010, 1.0125, 1.0175, 1.0041, 1.0107, 1.0133, 1.0039,\n",
      "         1.0172, 1.0057, 1.0020, 1.0734, 1.0086, 1.0073, 1.0056, 1.0059, 1.0147,\n",
      "         1.0046, 1.0059, 1.0012, 1.0109, 1.0012, 1.0015],\n",
      "        [0.0000, 1.0052, 1.0019, 1.0038, 1.0056, 1.0027, 1.0214, 1.0110, 1.0012,\n",
      "         1.0064, 1.0299, 1.0019, 1.0154, 1.0009, 1.0060, 1.0014, 1.0079, 1.0040,\n",
      "         1.0070, 1.0069, 1.0012, 1.0208, 1.0066, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0332, 1.0257, 1.0021, 1.0037, 1.0026, 1.0154, 1.0030, 1.0009,\n",
      "         1.0043, 1.0012, 1.0141, 1.0005, 1.0276, 1.0080, 1.0015, 1.0056, 1.0078,\n",
      "         1.0016, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0026, 1.0013, 1.0006, 1.0013, 1.0219, 1.0009, 1.0194, 1.0016,\n",
      "         1.0011, 1.0324, 1.0068, 1.0026, 1.0016, 1.0525, 1.0052, 1.0009, 1.0076,\n",
      "         1.0014, 1.0006, 1.0003, 1.0008, 1.0013, 1.0088, 1.0020, 1.0197]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0079, 1.0171, 1.0086, 1.0332, 1.0028, 1.0412, 1.0026, 1.0097,\n",
      "         1.0128, 1.0380, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0019, 1.0026, 1.0161, 1.0146, 1.0020, 1.0020, 1.0028, 1.0003,\n",
      "         1.0011, 1.0055, 1.0025, 1.0035, 1.0022, 1.0229, 1.0032, 1.0003, 1.0155,\n",
      "         1.0008, 1.0112, 1.0007, 1.0029, 1.0000, 1.0006, 1.0034, 1.0069, 1.0060,\n",
      "         1.0035, 1.0039, 1.0199]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 37, 37])  attentions_grads shape: torch.Size([3, 2, 12, 37, 37])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 37, 37])\n",
      "joint_attentions shape: torch.Size([2, 37, 37])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0008, 1.0020, 1.0023, 1.0191, 1.0132, 1.0331, 1.0047, 1.0334,\n",
      "         1.0045, 1.0023, 1.0278, 1.0112, 1.0009, 1.0018, 1.0426, 1.0007, 1.0021,\n",
      "         1.0025, 1.0191, 1.0006, 1.0190, 1.0125, 1.0018, 1.0006, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0032, 1.0012, 1.0024, 1.0488, 1.0008, 1.0144, 1.0003, 1.0011,\n",
      "         1.0029, 1.0014, 1.0109, 1.0051, 1.0017, 1.0021, 1.0002, 1.0254, 1.0001,\n",
      "         1.0000, 1.0005, 1.0002, 1.0006, 1.0023, 1.0044, 1.0004, 1.0009, 1.0003,\n",
      "         1.0307, 1.0066, 1.0092, 1.0069, 1.0002, 1.0002, 1.0026, 1.0032, 1.0019,\n",
      "         1.0273]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0059, 1.0020, 1.0027, 1.0113, 1.0020, 1.0012, 1.0125, 1.0017,\n",
      "         1.0040, 1.0001, 1.0021, 1.0011, 1.0019, 1.0004, 1.0030, 1.0039, 1.0220,\n",
      "         1.0195, 1.0022, 1.0021, 1.0008, 1.0003, 1.0016, 1.0030, 1.0007, 1.0053,\n",
      "         1.0002, 1.0004, 1.0031, 1.0008, 1.0004, 1.0003, 1.0013, 1.0030, 1.0001,\n",
      "         1.0038, 1.0010, 1.0096, 1.0101],\n",
      "        [0.0000, 1.0012, 1.0083, 1.0036, 1.0301, 1.0023, 1.0669, 1.0019, 1.0134,\n",
      "         1.0083, 1.0017, 1.0011, 1.0091, 1.0086, 1.0050, 1.0289, 1.0020, 1.0010,\n",
      "         1.0013, 1.0063, 1.0010, 1.0146, 1.0011, 1.0287, 1.0011, 1.0007, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0048, 1.0040, 1.0973, 1.0218, 1.0025, 1.0047, 1.0174, 1.0317,\n",
      "         1.0136, 1.0252, 1.0118, 1.0242, 1.0029, 1.0008, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0053, 1.0170, 1.0034, 1.0047, 1.0009, 1.0029, 1.0161, 1.0009,\n",
      "         1.0045, 1.0022, 1.0035, 1.0073, 1.0043, 1.0035, 1.0014, 1.0155, 1.0101,\n",
      "         1.0123, 1.0010, 1.0051, 1.0149, 1.0020, 1.0041, 1.0434]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0127, 1.0085, 1.0008, 1.0095, 1.0080, 1.0071, 1.0013, 1.0009,\n",
      "         1.0120, 1.0007, 1.0009, 1.0091, 1.0014, 1.0012, 1.0072, 1.0032, 1.0170,\n",
      "         1.0009, 1.0320, 1.0014, 1.0209, 1.0039, 1.0207, 1.0018, 1.0011, 1.0194,\n",
      "         1.0147, 1.0014, 1.0002],\n",
      "        [0.0000, 1.0022, 1.0114, 1.0240, 1.0198, 1.0126, 1.0179, 1.0494, 1.0430,\n",
      "         1.0032, 1.0390, 1.0024, 1.0020, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0273, 1.0001, 1.0009, 1.0013, 1.0006, 1.0014, 1.0007, 1.0011,\n",
      "         1.0164, 1.0058, 1.0008, 1.0026, 1.0005, 1.0006, 1.0003, 1.0012, 1.0176,\n",
      "         1.0184, 1.0023, 1.0021, 1.0001, 1.0145, 1.0006, 1.0002, 1.0004, 1.0097,\n",
      "         1.0030, 1.0046, 1.0005, 1.0122, 1.0026, 1.0011, 1.0001, 1.0006, 1.0008,\n",
      "         1.0004, 1.0011, 1.0052, 1.0063],\n",
      "        [0.0000, 1.0099, 1.0025, 1.0023, 1.0093, 1.0099, 1.0020, 1.0015, 1.0017,\n",
      "         1.0094, 1.0031, 1.0224, 1.0053, 1.0266, 1.0034, 1.0015, 1.0013, 1.0495,\n",
      "         1.0016, 1.0078, 1.0073, 1.0025, 1.0061, 1.0085, 1.0156, 1.0189, 1.0015,\n",
      "         1.0004, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 26, 26])  attentions_grads shape: torch.Size([3, 2, 12, 26, 26])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 26, 26])\n",
      "joint_attentions shape: torch.Size([2, 26, 26])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0059, 1.0047, 1.0074, 1.0089, 1.0054, 1.0346, 1.0037, 1.0062,\n",
      "         1.0251, 1.0217, 1.0033, 1.0024, 1.0097, 1.0070, 1.0052, 1.0065, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0003, 1.0369, 1.0043, 1.0013, 1.0019, 1.0131, 1.0099, 1.0275,\n",
      "         1.0126, 1.0004, 1.0010, 1.0030, 1.0017, 1.0019, 1.0032, 1.0048, 1.0037,\n",
      "         1.0010, 1.0017, 1.0026, 1.0105, 1.0091, 1.0039, 1.0010, 1.0004]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0030, 1.0151, 1.0056, 1.0074, 1.0039, 1.0047, 1.0009, 1.0010,\n",
      "         1.0020, 1.0008, 1.0095, 1.0155, 1.0010, 1.0052, 1.0096, 1.0275, 1.0100,\n",
      "         1.0032, 1.0030, 1.0004, 1.0030, 1.0509, 1.0006, 1.0026, 1.0398, 1.0015,\n",
      "         1.0023, 1.0012, 1.0322, 1.0006, 1.0005],\n",
      "        [0.0000, 1.0118, 1.0021, 1.0060, 1.0331, 1.0017, 1.0036, 1.0025, 1.0014,\n",
      "         1.0022, 1.0024, 1.0036, 1.0262, 1.0016, 1.0025, 1.0092, 1.0067, 1.0038,\n",
      "         1.0031, 1.0065, 1.0017, 1.0050, 1.0010, 1.0171, 1.0005, 1.0008, 1.0002,\n",
      "         1.0013, 1.0057, 1.0024, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0173, 1.0056, 1.0016, 1.0009, 1.0039, 1.0242, 1.0042, 1.0038,\n",
      "         1.0245, 1.0037, 1.0014, 1.0090, 1.0062, 1.0046, 1.0253, 1.0004, 1.0034,\n",
      "         1.0100, 1.0034, 1.0074, 1.0052, 1.0043, 1.0013, 1.0005, 1.0015, 1.0002],\n",
      "        [0.0000, 1.0034, 1.0243, 1.0051, 1.0018, 1.0333, 1.0024, 1.0040, 1.0032,\n",
      "         1.0345, 1.0017, 1.0063, 1.0025, 1.0012, 1.0059, 1.0068, 1.0113, 1.0090,\n",
      "         1.0009, 1.0010, 1.0058, 1.0110, 1.0342, 1.0013, 1.0004, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0175, 1.0071, 1.0020, 1.0007, 1.0011, 1.0014, 1.0021, 1.0034,\n",
      "         1.0018, 1.0017, 1.0005, 1.0025, 1.0001, 1.0021, 1.0005, 1.0023, 1.0021,\n",
      "         1.0000, 1.0045, 1.0058, 1.0011, 1.0034, 1.0477, 1.0002, 1.0002, 1.0018,\n",
      "         1.0010, 1.0159, 1.0010, 1.0008, 1.0013, 1.0032, 1.0013, 1.0001, 1.0012,\n",
      "         1.0026, 1.0025, 1.0312],\n",
      "        [0.0000, 1.0183, 1.0133, 1.0045, 1.0014, 1.0238, 1.0134, 1.0039, 1.0004,\n",
      "         1.0045, 1.0240, 1.0728, 1.0379, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0257, 1.0064, 1.0012, 1.0014, 1.0015, 1.0014, 1.0013, 1.0064,\n",
      "         1.0083, 1.0067, 1.0071, 1.0166, 1.0060, 1.0003, 1.0009, 1.0039, 1.0352,\n",
      "         1.0022, 1.0093, 1.0274, 1.0128, 1.0336, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0037, 1.0022, 1.0040, 1.0006, 1.0003, 1.0026, 1.0015, 1.0007,\n",
      "         1.0021, 1.0023, 1.0003, 1.0038, 1.0148, 1.0005, 1.0008, 1.0109, 1.0004,\n",
      "         1.0007, 1.0056, 1.0009, 1.0015, 1.0010, 1.0101, 1.0038, 1.0046, 1.0017,\n",
      "         1.0001, 1.0028, 1.0088, 1.0030, 1.0010, 1.0002, 1.0019, 1.0238]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0041, 1.0044, 1.0108, 1.0156, 1.0800, 1.0033, 1.0003, 1.0089,\n",
      "         1.0077, 1.0053, 1.0124, 1.0120, 1.0021, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0041, 1.0006, 1.0014, 1.0049, 1.0022, 1.0017, 1.0007, 1.0024,\n",
      "         1.0022, 1.0014, 1.0058, 1.0010, 1.0023, 1.0050, 1.0010, 1.0000, 1.0019,\n",
      "         1.0015, 1.0014, 1.0021, 1.0003, 1.0149, 1.0007, 1.0647, 1.0017, 1.0010]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0008, 1.0030, 1.0018, 1.0014, 1.0015, 1.0142, 1.0015, 1.0018,\n",
      "         1.0146, 1.0072, 1.0009, 1.0000, 1.0086, 1.0004, 1.0146, 1.0014, 1.0090,\n",
      "         1.0232, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0058, 1.0035, 1.0162, 1.0126, 1.0066, 1.0130, 1.0124, 1.0012,\n",
      "         1.0009, 1.0474, 1.0032, 1.0007, 1.0265, 1.0074, 1.0038, 1.0092, 1.0016,\n",
      "         1.0097, 1.0010, 1.0013, 1.0198, 1.0009, 1.0003]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0027, 1.0041, 1.0047, 1.0050, 1.0716, 1.0200, 1.0461, 1.0267,\n",
      "         1.0288, 1.0138, 1.0031, 1.0064, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0063, 1.0124, 1.0025, 1.0021, 1.0342, 1.0060, 1.0144, 1.0271,\n",
      "         1.0013, 1.0060, 1.0173, 1.0060, 1.0076, 1.0035, 1.0066, 1.0361, 1.0094,\n",
      "         1.0009, 1.0024, 1.0237, 1.0316, 1.0016, 1.0005]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0143, 1.0108, 1.0018, 1.0017, 1.0136, 1.0008, 1.0308, 1.0021,\n",
      "         1.0035, 1.0052, 1.0013, 1.0043, 1.0023, 1.0020, 1.0046, 1.0052, 1.0040,\n",
      "         1.0012, 1.0087, 1.0077, 1.0010, 1.0126, 1.0053, 1.0152, 1.0068, 1.0058,\n",
      "         1.0016, 1.0017, 1.0039, 1.0017, 1.0015, 1.0161, 1.0028, 1.0197, 1.0014,\n",
      "         1.0005, 1.0005, 1.0044, 1.0004, 1.0002],\n",
      "        [0.0000, 1.0355, 1.0126, 1.1191, 1.0105, 1.0053, 1.0047, 1.0263, 1.0144,\n",
      "         1.0032, 1.0008, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 51, 51])  attentions_grads shape: torch.Size([3, 2, 12, 51, 51])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 51, 51])\n",
      "joint_attentions shape: torch.Size([2, 51, 51])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0018, 1.0007, 1.0006, 1.0006, 1.0018, 1.0013, 1.0077, 1.0006,\n",
      "         1.0098, 1.0004, 1.0019, 1.0007, 1.0093, 1.0099, 1.0012, 1.0019, 1.0265,\n",
      "         1.0014, 1.0049, 1.0010, 1.0040, 1.0018, 1.0014, 1.0004, 1.0014, 1.0145,\n",
      "         1.0021, 1.0006, 1.0003, 1.0013, 1.0004, 1.0084, 1.0005, 1.0025, 1.0006,\n",
      "         1.0006, 1.0017, 1.0051, 1.0007, 1.0092, 1.0009, 1.0020, 1.0002, 1.0024,\n",
      "         1.0009, 1.0014, 1.0004, 1.0017, 1.0002, 1.0001],\n",
      "        [0.0000, 1.0024, 1.0077, 1.0024, 1.0019, 1.0008, 1.0163, 1.0084, 1.0037,\n",
      "         1.0029, 1.0030, 1.0069, 1.0013, 1.0028, 1.0004, 1.0011, 1.0022, 1.0070,\n",
      "         1.0016, 1.0005, 1.0045, 1.0007, 1.0015, 1.0003, 1.0004, 1.0003, 1.0016,\n",
      "         1.0003, 1.0033, 1.0042, 1.0027, 1.0016, 1.0010, 1.0012, 1.0084, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 23, 23])  attentions_grads shape: torch.Size([3, 2, 12, 23, 23])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 23, 23])\n",
      "joint_attentions shape: torch.Size([2, 23, 23])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0490, 1.0093, 1.0237, 1.0947, 1.0135, 1.0030, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0030, 1.0011, 1.0757, 1.0023, 1.0009, 1.0047, 1.0033, 1.0007,\n",
      "         1.0132, 1.0008, 1.0043, 1.0013, 1.0024, 1.0034, 1.0012, 1.0030, 1.0107,\n",
      "         1.0014, 1.0008, 1.0141, 1.0021, 1.0014]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0370, 1.0471, 1.0061, 1.0139, 1.0984, 1.0066, 1.0047, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0014, 1.0008, 1.0010, 1.0085, 1.0030, 1.0058, 1.0117, 1.0006,\n",
      "         1.0009, 1.0084, 1.0028, 1.0009, 1.0014, 1.0166, 1.0005, 1.0307, 1.0009,\n",
      "         1.0033, 1.0004, 1.0023, 1.0045, 1.0348, 1.0210, 1.0007, 1.0062, 1.0076,\n",
      "         1.0069, 1.0046, 1.0107, 1.0013, 1.0007, 1.0083, 1.0229, 1.0004, 1.0001]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0014, 1.0225, 1.0042, 1.0027, 1.0229, 1.0010, 1.0008, 1.0014,\n",
      "         1.0388, 1.0023, 1.0009, 1.0025, 1.0017, 1.0033, 1.0041, 1.0010, 1.0012,\n",
      "         1.0071, 1.0280, 1.0030, 1.0091, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0006, 1.0072, 1.0084, 1.0002, 1.0003, 1.0041, 1.0112, 1.0006,\n",
      "         1.0023, 1.0004, 1.0005, 1.0086, 1.0007, 1.0064, 1.0056, 1.0540, 1.0012,\n",
      "         1.0013, 1.0037, 1.0017, 1.0148, 1.0021, 1.0038, 1.0086, 1.0009, 1.0033,\n",
      "         1.0202, 1.0149, 1.0007, 1.0005, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0030, 1.0074, 1.0000, 1.0054, 1.0128, 1.0055, 1.0093,\n",
      "         1.0130, 1.0064, 1.0055, 1.0068, 1.0084, 1.0015, 1.0010, 1.0025, 1.0017,\n",
      "         1.0012, 1.0021, 1.0014, 1.0039, 1.0004, 1.0186, 1.0013, 1.0121, 1.0006,\n",
      "         1.0048, 1.0053, 1.0017, 1.0023, 1.0031, 1.0137],\n",
      "        [0.0000, 1.0224, 1.0106, 1.0125, 1.0044, 1.0162, 1.0327, 1.0058, 1.0102,\n",
      "         1.0406, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0266, 1.0018, 1.0020, 1.0060, 1.0046, 1.0026, 1.0016, 1.0083,\n",
      "         1.0019, 1.0006, 1.0009, 1.0009, 1.0059, 1.0070, 1.0019, 1.0051, 1.0022,\n",
      "         1.0035, 1.0037, 1.0008, 1.0025, 1.0039, 1.0038, 1.0008, 1.0072, 1.0073,\n",
      "         1.0029, 1.0009, 1.0034, 1.0030, 1.0405],\n",
      "        [0.0000, 1.0036, 1.0012, 1.0019, 1.0362, 1.0015, 1.0097, 1.0019, 1.0551,\n",
      "         1.0217, 1.0087, 1.0170, 1.0021, 1.0222, 1.0018, 1.0016, 1.0065, 1.0245,\n",
      "         1.0013, 1.0034, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0056, 1.0073, 1.0116, 1.0088, 1.0142, 1.0297, 1.0038, 1.0171,\n",
      "         1.0210, 1.0186, 1.0188, 1.0303, 1.0323, 1.0045, 1.0010, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0032, 1.0049, 1.0050, 1.0025, 1.0322, 1.0138, 1.0019, 1.0039,\n",
      "         1.0085, 1.0049, 1.0153, 1.0051, 1.0039, 1.0160, 1.0370, 1.0545, 1.0012,\n",
      "         1.0290, 1.0120, 1.0036, 1.0011]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0138, 1.0361, 1.0199, 1.0187, 1.0104, 1.0370, 1.0333, 1.0457,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0041, 1.0013, 1.0023, 1.0003, 1.0021, 1.0004, 1.0005, 1.0012,\n",
      "         1.0006, 1.0001, 1.0005, 1.0095, 1.0010, 1.0024, 1.0016, 1.0077, 1.0064,\n",
      "         1.0006, 1.0005, 1.0040, 1.0009, 1.0056, 1.0007, 1.0009, 1.0002, 1.0003,\n",
      "         1.0160, 1.0009, 1.0002, 1.0047, 1.0014, 1.0010, 1.0104, 1.0006, 1.0006,\n",
      "         1.0003, 1.0010, 1.0123, 1.0015, 1.0009]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 49, 49])  attentions_grads shape: torch.Size([3, 2, 12, 49, 49])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 49, 49])\n",
      "joint_attentions shape: torch.Size([2, 49, 49])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0032, 1.0021, 1.0067, 1.0004, 1.0082, 1.0194, 1.0004, 1.0066,\n",
      "         1.0009, 1.0009, 1.0032, 1.0019, 1.0005, 1.0003, 1.0121, 1.0019, 1.0676,\n",
      "         1.0009, 1.0050, 1.0004, 1.0025, 1.0117, 1.0104, 1.0063, 1.0016, 1.0007,\n",
      "         1.0030, 1.0021, 1.0031, 1.0010, 1.0148, 1.0010, 1.0007, 1.0016, 1.0101,\n",
      "         1.0035, 1.0022, 1.0007, 1.0027, 1.0052, 1.0004, 1.0212, 1.0004, 1.0006,\n",
      "         1.0019, 1.0054, 1.0003, 1.0001],\n",
      "        [0.0000, 1.0013, 1.0059, 1.0018, 1.0035, 1.0017, 1.0075, 1.0008, 1.0030,\n",
      "         1.0123, 1.0248, 1.0016, 1.0061, 1.0006, 1.0021, 1.0021, 1.0061, 1.0007,\n",
      "         1.0020, 1.0010, 1.0009, 1.0000, 1.0012, 1.0005, 1.0005, 1.0003, 1.0004,\n",
      "         1.0002, 1.0025, 1.0008, 1.0082, 1.0015, 1.0003, 1.0023, 1.0022, 1.0031,\n",
      "         1.0322, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 42, 42])  attentions_grads shape: torch.Size([3, 2, 12, 42, 42])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 42, 42])\n",
      "joint_attentions shape: torch.Size([2, 42, 42])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0007, 1.0010, 1.0022, 1.0023, 1.0007, 1.0069, 1.0053, 1.0004,\n",
      "         1.0005, 1.0055, 1.0014, 1.0034, 1.0012, 1.0007, 1.0003, 1.0028, 1.0003,\n",
      "         1.0014, 1.0038, 1.0035, 1.0005, 1.0018, 1.0082, 1.0014, 1.0048, 1.0191,\n",
      "         1.0015, 1.0003, 1.0028, 1.0022, 1.0032, 1.0073, 1.0014, 1.0005, 1.0010,\n",
      "         1.0032, 1.0004, 1.0006, 1.0005, 1.0004, 1.0001],\n",
      "        [0.0000, 1.0198, 1.0228, 1.0041, 1.0019, 1.0029, 1.0060, 1.0060, 1.0009,\n",
      "         1.0212, 1.0008, 1.0055, 1.0018, 1.0038, 1.0014, 1.0060, 1.0027, 1.0063,\n",
      "         1.0163, 1.0288, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 48, 48])  attentions_grads shape: torch.Size([3, 2, 12, 48, 48])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 48, 48])\n",
      "joint_attentions shape: torch.Size([2, 48, 48])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0066, 1.0072, 1.0033, 1.0027, 1.0402, 1.0054, 1.0097, 1.0349,\n",
      "         1.0206, 1.0024, 1.0306, 1.0160, 1.0627, 1.0060, 1.0138, 1.0017, 1.0040,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0094, 1.0037, 1.0008, 1.0064, 1.0117, 1.0082, 1.0077, 1.0006,\n",
      "         1.0011, 1.0043, 1.0008, 1.0016, 1.0008, 1.0042, 1.0136, 1.0179, 1.0014,\n",
      "         1.0009, 1.0024, 1.0007, 1.0063, 1.0007, 1.0112, 1.0018, 1.0024, 1.0161,\n",
      "         1.0094, 1.0005, 1.0078, 1.0007, 1.0005, 1.0015, 1.0005, 1.0012, 1.0005,\n",
      "         1.0128, 1.0035, 1.0005, 1.0163, 1.0005, 1.0113, 1.0040, 1.0057, 1.0007,\n",
      "         1.0040, 1.0003, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0224, 1.0028, 1.0045, 1.0229, 1.0153, 1.0079, 1.0104, 1.0230,\n",
      "         1.0087, 1.0069, 1.0069, 1.0057, 1.0039, 1.0078, 1.0066, 1.0031, 1.0000],\n",
      "        [0.0000, 1.0687, 1.0007, 1.0038, 1.0013, 1.0099, 1.0321, 1.0010, 1.0031,\n",
      "         1.0008, 1.0002, 1.0138, 1.0022, 1.0181, 1.0026, 1.0061, 1.0020, 1.0100]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0020, 1.0018, 1.0064, 1.0043, 1.0012, 1.0092, 1.0058, 1.0030,\n",
      "         1.0032, 1.0090, 1.0019, 1.0021, 1.0012, 1.0120, 1.0033, 1.0004, 1.0006,\n",
      "         1.0123, 1.0008, 1.0014, 1.0035, 1.0006, 1.0006, 1.0083, 1.0114, 1.0015,\n",
      "         1.0372, 1.0008, 1.0013, 1.0257, 1.0008, 1.0458, 1.0005, 1.0002],\n",
      "        [0.0000, 1.0017, 1.0073, 1.0088, 1.0021, 1.0206, 1.0225, 1.0038, 1.0154,\n",
      "         1.0023, 1.0014, 1.0609, 1.0046, 1.0484, 1.0014, 1.0186, 1.0463, 1.0015,\n",
      "         1.0009, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0009, 1.0605, 1.0133, 1.0024, 1.0021, 1.0019, 1.0031, 1.0078,\n",
      "         1.0054, 1.0006, 1.0080, 1.0012, 1.0181, 1.0151, 1.0006, 1.0009, 1.0245,\n",
      "         1.0264, 1.0044, 1.0057, 1.0342, 1.0015, 1.0033, 1.0016, 1.0042, 1.0087,\n",
      "         1.0060, 1.0009, 1.0009],\n",
      "        [0.0000, 1.0037, 1.0287, 1.0320, 1.0170, 1.0055, 1.0017, 1.0060, 1.0097,\n",
      "         1.0408, 1.0016, 1.0088, 1.0096, 1.0186, 1.0188, 1.0123, 1.0253, 1.0071,\n",
      "         1.0018, 1.0005, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0060, 1.0204, 1.0087, 1.0086, 1.0555, 1.0067, 1.0034, 1.0005,\n",
      "         1.0215, 1.0089, 1.0100, 1.0170, 1.0023, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0013, 1.0010, 1.0005, 1.0011, 1.0021, 1.0005, 1.0012, 1.0004,\n",
      "         1.0006, 1.0002, 1.0022, 1.0029, 1.0003, 1.0023, 1.0127, 1.0082, 1.0017,\n",
      "         1.0026, 1.0031, 1.0074, 1.0030, 1.0006, 1.0027, 1.0024, 1.0002, 1.0018,\n",
      "         1.0012, 1.0032, 1.0036, 1.0022, 1.0002, 1.0010, 1.0015, 1.0057, 1.0271,\n",
      "         1.0061, 1.0014, 1.0006, 1.0030, 1.0010]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0123, 1.0065, 1.0061, 1.0030, 1.0074, 1.0025, 1.0030, 1.0100,\n",
      "         1.0012, 1.0006, 1.0135, 1.0046, 1.0131, 1.0102, 1.0253, 1.0093, 1.0007,\n",
      "         1.0007, 1.0073, 1.0194, 1.0011, 1.0024, 1.0038, 1.0013, 1.0071, 1.0023,\n",
      "         1.0008, 1.0052, 1.0019, 1.0003, 1.0021, 1.0158, 1.0240, 1.0005, 1.0004],\n",
      "        [0.0000, 1.0202, 1.0092, 1.0058, 1.0081, 1.1067, 1.0555, 1.0046, 1.0025,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 29, 29])  attentions_grads shape: torch.Size([3, 2, 12, 29, 29])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 29, 29])\n",
      "joint_attentions shape: torch.Size([2, 29, 29])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0232, 1.0017, 1.0020, 1.0014, 1.0045, 1.0042, 1.0004, 1.0221,\n",
      "         1.0014, 1.0290, 1.0014, 1.0016, 1.0025, 1.0046, 1.0031, 1.0016, 1.0020,\n",
      "         1.0190, 1.0034, 1.0010, 1.0011, 1.0024, 1.0026, 1.0196, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000],\n",
      "        [0.0000, 1.0455, 1.0045, 1.0517, 1.0073, 1.0006, 1.0242, 1.0019, 1.0028,\n",
      "         1.0254, 1.0123, 1.0034, 1.0008, 1.0026, 1.0256, 1.0002, 1.0004, 1.0007,\n",
      "         1.0033, 1.0037, 1.0004, 1.0021, 1.0142, 1.0008, 1.0018, 1.0010, 1.0004,\n",
      "         1.0009, 1.0128]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0175, 1.0086, 1.0013, 1.0124, 1.0023, 1.0034, 1.0047, 1.0029,\n",
      "         1.0010, 1.0023, 1.0040, 1.0031, 1.0368, 1.0024, 1.0045, 1.0020, 1.0049,\n",
      "         1.0024, 1.0045, 1.0021, 1.0011, 1.0151, 1.0021, 1.0006, 1.0012, 1.0043,\n",
      "         1.0000, 1.0039, 1.0063, 1.0107, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0018, 1.0269, 1.0262, 1.0059, 1.0047, 1.0063, 1.0008, 1.0011,\n",
      "         1.0079, 1.0026, 1.0008, 1.0050, 1.0486, 1.0198, 1.0006, 1.0052, 1.0088,\n",
      "         1.0011, 1.0013, 1.0010, 1.0228, 1.0035, 1.0061, 1.0010, 1.0042, 1.0031,\n",
      "         1.0099, 1.0082, 1.0108, 1.0056, 1.0053, 1.0007, 1.0003]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 22, 22])  attentions_grads shape: torch.Size([3, 2, 12, 22, 22])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 22, 22])\n",
      "joint_attentions shape: torch.Size([2, 22, 22])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0163, 1.0041, 1.0031, 1.0136, 1.0217, 1.0100, 1.0408, 1.0738,\n",
      "         1.0023, 1.0228, 1.0142, 1.0028, 1.0039, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0415, 1.0015, 1.0106, 1.0055, 1.0044, 1.0026, 1.0091, 1.0046,\n",
      "         1.0021, 1.0024, 1.0063, 1.0138, 1.0041, 1.0101, 1.0046, 1.0006, 1.0033,\n",
      "         1.0563, 1.0101, 1.0044, 1.0190]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 15, 15])  attentions_grads shape: torch.Size([3, 2, 12, 15, 15])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 15, 15])\n",
      "joint_attentions shape: torch.Size([2, 15, 15])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0020, 1.0411, 1.0451, 1.0169, 1.0228, 1.0122, 1.0025, 1.0167,\n",
      "         1.0292, 1.0133, 1.0161, 1.0177, 1.0032, 1.0017],\n",
      "        [0.0000, 1.0181, 1.0159, 1.0432, 1.0204, 1.0074, 1.0125, 1.0280, 1.0051,\n",
      "         1.0027, 1.0115, 1.0128, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0022, 1.0032, 1.0023, 1.0036, 1.0069, 1.0033, 1.0032, 1.0030,\n",
      "         1.0032, 1.0021, 1.0107, 1.0049, 1.0055, 1.0060, 1.0062, 1.0010, 1.0081,\n",
      "         1.0010, 1.0050, 1.0007, 1.0014, 1.0028, 1.0073, 1.0043, 1.0014, 1.0339,\n",
      "         1.0082, 1.0073, 1.0046, 1.0011, 1.0006, 1.0205, 1.0020, 1.0005, 1.0002],\n",
      "        [0.0000, 1.0031, 1.0051, 1.0029, 1.0013, 1.0574, 1.0101, 1.0008, 1.0146,\n",
      "         1.0036, 1.0022, 1.0008, 1.0067, 1.0031, 1.0229, 1.0012, 1.0033, 1.0256,\n",
      "         1.0021, 1.0030, 1.0523, 1.0019, 1.0064, 1.0034, 1.0007, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0102, 1.0069, 1.0163, 1.0065, 1.0108, 1.0016, 1.0015, 1.0423,\n",
      "         1.0163, 1.0032, 1.0013, 1.0012, 1.0053, 1.0362, 1.0675, 1.0054, 1.0053,\n",
      "         1.0022, 1.0012, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0071, 1.0007, 1.0101, 1.0053, 1.0224, 1.0005, 1.0012, 1.0014,\n",
      "         1.0006, 1.0012, 1.0088, 1.0057, 1.0484, 1.0124, 1.0493, 1.0011, 1.0009,\n",
      "         1.0013, 1.0014, 1.0018, 1.0026, 1.0090, 1.0018, 1.0189, 1.0012, 1.0002]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0043, 1.0011, 1.0002, 1.0014, 1.0029, 1.0494, 1.0038, 1.0005,\n",
      "         1.0004, 1.0016, 1.0021, 1.0042, 1.0006, 1.0011, 1.0007, 1.0043, 1.0011,\n",
      "         1.0063, 1.0018, 1.0080, 1.0031, 1.0007, 1.0014, 1.0030, 1.0299, 1.0012,\n",
      "         1.0037, 1.0144, 1.0262, 1.0003, 1.0008, 1.0019, 1.0003, 1.0006, 1.0134,\n",
      "         1.0002, 1.0009, 1.0049],\n",
      "        [0.0000, 1.0026, 1.0065, 1.0313, 1.0007, 1.0061, 1.0065, 1.0009, 1.0021,\n",
      "         1.0038, 1.0066, 1.0017, 1.0172, 1.0047, 1.0039, 1.0027, 1.0071, 1.0412,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0006, 1.0007, 1.0029, 1.0028, 1.0340, 1.0093, 1.0005,\n",
      "         1.0016, 1.0060, 1.0119, 1.0119, 1.0026, 1.0043, 1.0043, 1.0006, 1.0019,\n",
      "         1.0010, 1.0030, 1.0102, 1.0081, 1.0031, 1.0038, 1.0119, 1.0368, 1.0005,\n",
      "         1.0011, 1.0013, 1.0004, 1.0018, 1.0022, 1.0003, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0027, 1.0026, 1.0027, 1.0244, 1.0010, 1.0021, 1.0019, 1.0141,\n",
      "         1.0000, 1.0010, 1.0114, 1.0012, 1.0013, 1.0012, 1.0007, 1.0023, 1.0015,\n",
      "         1.0440, 1.0007, 1.0041, 1.0004, 1.0007, 1.0012, 1.0017, 1.0168, 1.0016,\n",
      "         1.0003, 1.0003, 1.0018, 1.0094, 1.0004, 1.0004, 1.0008, 1.0010, 1.0015,\n",
      "         1.0144, 1.0001, 1.0011, 1.0021, 1.0004, 1.0072, 1.0010, 1.0020]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 46, 46])  attentions_grads shape: torch.Size([3, 2, 12, 46, 46])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 46, 46])\n",
      "joint_attentions shape: torch.Size([2, 46, 46])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0326, 1.0295, 1.0247, 1.0150, 1.0479, 1.0249, 1.0454, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0014, 1.0015, 1.0035, 1.0008, 1.0192, 1.0015, 1.0800, 1.0082,\n",
      "         1.0000, 1.0001, 1.0010, 1.0003, 1.0012, 1.0013, 1.0010, 1.0004, 1.0130,\n",
      "         1.0004, 1.0004, 1.0027, 1.0001, 1.0001, 1.0015, 1.0000, 1.0002, 1.0002,\n",
      "         1.0001, 1.0002, 1.0004, 1.0002, 1.0005, 1.0002, 1.0023, 1.0006, 1.0004,\n",
      "         1.0012, 1.0003, 1.0019, 1.0001, 1.0004, 1.0011, 1.0001, 1.0010, 1.0008,\n",
      "         1.0005]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0208, 1.0053, 1.0078, 1.0804, 1.0224, 1.0271, 1.0119, 1.0032,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0009, 1.0010, 1.0009, 1.0030, 1.0100, 1.0359, 1.0022, 1.0088,\n",
      "         1.0009, 1.0030, 1.0009, 1.0044, 1.0220, 1.0008, 1.0251, 1.0014, 1.0161,\n",
      "         1.0007, 1.0117, 1.0040, 1.0083, 1.0130, 1.0224, 1.0119, 1.0010, 1.0009,\n",
      "         1.0210, 1.0006, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0063, 1.0177, 1.0243, 1.0044, 1.0023, 1.0019, 1.0095, 1.0012,\n",
      "         1.0018, 1.0052, 1.0123, 1.0043, 1.0153, 1.0133, 1.0204, 1.0074, 1.0055,\n",
      "         1.0009, 1.0006, 1.0012, 1.0051, 1.0094, 1.0010, 1.0162, 1.0113, 1.0014,\n",
      "         1.0003, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0013, 1.0034, 1.0012, 1.0372, 1.0005, 1.0034, 1.0016, 1.0187,\n",
      "         1.0006, 1.0001, 1.0013, 1.0129, 1.0004, 1.0008, 1.0138, 1.0067, 1.0097,\n",
      "         1.0025, 1.0023, 1.0008, 1.0004, 1.0002, 1.0009, 1.0000, 1.0118, 1.0002,\n",
      "         1.0008, 1.0005, 1.0013, 1.0005, 1.0201, 1.0003, 1.0022, 1.0009, 1.0031]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 42, 42])  attentions_grads shape: torch.Size([3, 2, 12, 42, 42])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 42, 42])\n",
      "joint_attentions shape: torch.Size([2, 42, 42])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0010, 1.0021, 1.0081, 1.0133, 1.0197, 1.0019, 1.0012, 1.0021,\n",
      "         1.0023, 1.0098, 1.0036, 1.0042, 1.0063, 1.0142, 1.0006, 1.0035, 1.0007,\n",
      "         1.0217, 1.0012, 1.0018, 1.0055, 1.0087, 1.0029, 1.0119, 1.0012, 1.0019,\n",
      "         1.0156, 1.0057, 1.0090, 1.0054, 1.0048, 1.0012, 1.0006, 1.0001, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0025, 1.0016, 1.0023, 1.0710, 1.0007, 1.0018, 1.0008, 1.0013,\n",
      "         1.0047, 1.0007, 1.0058, 1.0049, 1.0006, 1.0006, 1.0015, 1.0003, 1.0045,\n",
      "         1.0008, 1.0020, 1.0221, 1.0009, 1.0014, 1.0002, 1.0022, 1.0013, 1.0001,\n",
      "         1.0091, 1.0003, 1.0150, 1.0000, 1.0072, 1.0083, 1.0012, 1.0067, 1.0023,\n",
      "         1.0011, 1.0003, 1.0044, 1.0005, 1.0015, 1.0033]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0016, 1.0002, 1.0401, 1.0013, 1.0060, 1.0009, 1.0022, 1.0000,\n",
      "         1.0000, 1.0020, 1.0016, 1.0011, 1.0015, 1.0006, 1.0084, 1.0006, 1.0067,\n",
      "         1.0013, 1.0001, 1.0066, 1.0009, 1.0010, 1.0149, 1.0021, 1.0219, 1.0012,\n",
      "         1.0007, 1.0394, 1.0098, 1.0008, 1.0001, 1.0008, 1.0013, 1.0003, 1.0075,\n",
      "         1.0010, 1.0021, 1.0033, 1.0025, 1.0080],\n",
      "        [0.0000, 1.0016, 1.0290, 1.0083, 1.0379, 1.0035, 1.0386, 1.0186, 1.0052,\n",
      "         1.0020, 1.0241, 1.0032, 1.0225, 1.0055, 1.0268, 1.0023, 1.0064, 1.0051,\n",
      "         1.0096, 1.0006, 1.0004, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 54, 54])  attentions_grads shape: torch.Size([3, 2, 12, 54, 54])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 54, 54])\n",
      "joint_attentions shape: torch.Size([2, 54, 54])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0017, 1.0022, 1.0008, 1.0012, 1.0040, 1.0003, 1.0015, 1.0002,\n",
      "         1.0009, 1.0010, 1.0014, 1.0019, 1.0014, 1.0010, 1.0167, 1.0002, 1.0078,\n",
      "         1.0005, 1.0025, 1.0007, 1.0037, 1.0008, 1.0007, 1.0004, 1.0002, 1.0007,\n",
      "         1.0050, 1.0002, 1.0001, 1.0011, 1.0010, 1.0001, 1.0004, 1.0011, 1.0078,\n",
      "         1.0013, 1.0006, 1.0003, 1.0023, 1.0172, 1.0004, 1.0026, 1.0000, 1.0455,\n",
      "         1.0024, 1.0149, 1.0006, 1.0054, 1.0002, 1.0006, 1.0001, 1.0007, 1.0005],\n",
      "        [0.0000, 1.0045, 1.0003, 1.0180, 1.0036, 1.0041, 1.0014, 1.0093, 1.0237,\n",
      "         1.0041, 1.0118, 1.0069, 1.0017, 1.0104, 1.0022, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0047, 1.0044, 1.0428, 1.0080, 1.0128, 1.0156, 1.0077, 1.0091,\n",
      "         1.0033, 1.0021, 1.0009, 1.0088, 1.0008, 1.0160, 1.0085, 1.0027, 1.0025,\n",
      "         1.0128, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0040, 1.0053, 1.0057, 1.0083, 1.0109, 1.0043, 1.0003, 1.0055,\n",
      "         1.0008, 1.0027, 1.0035, 1.0086, 1.0002, 1.0042, 1.0061, 1.0004, 1.0039,\n",
      "         1.0010, 1.0007, 1.0006, 1.0126, 1.0005, 1.0017, 1.0040, 1.0153, 1.0004,\n",
      "         1.0024, 1.0008, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 47, 47])  attentions_grads shape: torch.Size([3, 2, 12, 47, 47])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 47, 47])\n",
      "joint_attentions shape: torch.Size([2, 47, 47])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0034, 1.0008, 1.0008, 1.0012, 1.0007, 1.0046, 1.0008, 1.0021,\n",
      "         1.0012, 1.0099, 1.0011, 1.0009, 1.0018, 1.0010, 1.0005, 1.0018, 1.0014,\n",
      "         1.0014, 1.0010, 1.0011, 1.0004, 1.0099, 1.0006, 1.0023, 1.0020, 1.0013,\n",
      "         1.0002, 1.0178, 1.0014, 1.0011, 1.0006, 1.0002, 1.0054, 1.0003, 1.0044,\n",
      "         1.0006, 1.0017, 1.0001, 1.0021, 1.0005, 1.0004, 1.0009, 1.0019, 1.0009,\n",
      "         1.0028, 1.0128],\n",
      "        [0.0000, 1.0000, 1.0014, 1.0012, 1.0033, 1.0014, 1.0003, 1.0001, 1.0004,\n",
      "         1.0037, 1.0035, 1.0038, 1.0003, 1.0008, 1.0012, 1.0037, 1.0007, 1.0019,\n",
      "         1.0038, 1.0017, 1.0016, 1.0002, 1.0051, 1.0121, 1.0013, 1.0003, 1.0007,\n",
      "         1.0105, 1.0015, 1.0005, 1.0003, 1.0003, 1.0010, 1.0110, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0237, 1.0159, 1.0108, 1.0021, 1.0033, 1.0031, 1.0634, 1.0064,\n",
      "         1.0148, 1.0025, 1.0071, 1.0168, 1.0103, 1.0173, 1.0188, 1.0008, 1.0043,\n",
      "         1.0121, 1.0033, 1.0007],\n",
      "        [0.0000, 1.0170, 1.0070, 1.0645, 1.0271, 1.0054, 1.0546, 1.0097, 1.0275,\n",
      "         1.0057, 1.0123, 1.0031, 1.0043, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0649, 1.0031, 1.0512, 1.0011, 1.0152, 1.0331, 1.0014, 1.0010,\n",
      "         1.0123, 1.0423, 1.0023, 1.0004, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0009, 1.0259, 1.0017, 1.0138, 1.0062, 1.0041, 1.0011, 1.0329,\n",
      "         1.0006, 1.0034, 1.0016, 1.0054, 1.0032, 1.0016, 1.0245, 1.0015, 1.0517,\n",
      "         1.0008, 1.0258, 1.0007, 1.0005, 1.0035, 1.0018, 1.0007, 1.0073, 1.0016,\n",
      "         1.0006, 1.0006, 1.0075, 1.0116, 1.0006, 1.0002]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 44, 44])  attentions_grads shape: torch.Size([3, 2, 12, 44, 44])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 44, 44])\n",
      "joint_attentions shape: torch.Size([2, 44, 44])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0023, 1.0048, 1.0031, 1.0444, 1.0444, 1.0107, 1.0288, 1.0054,\n",
      "         1.0222, 1.0200, 1.0053, 1.0058, 1.0404, 1.0065, 1.0026, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0031, 1.0001, 1.0009, 1.0040, 1.0019, 1.0007, 1.0012, 1.0052,\n",
      "         1.0018, 1.0021, 1.0046, 1.0010, 1.0008, 1.0020, 1.0024, 1.0005, 1.0007,\n",
      "         1.0110, 1.0020, 1.0000, 1.0009, 1.0001, 1.0014, 1.1155, 1.0011, 1.0002,\n",
      "         1.0006, 1.0038, 1.0190, 1.0024, 1.0003, 1.0008, 1.0014, 1.0003, 1.0003,\n",
      "         1.0015, 1.0022, 1.0003, 1.0073, 1.0003, 1.0003, 1.0035, 1.0019]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0026, 1.0423, 1.0095, 1.0229, 1.0014, 1.0037, 1.0067, 1.0038,\n",
      "         1.0011, 1.0341, 1.0013, 1.0127, 1.0047, 1.0137, 1.0179, 1.0006, 1.0061,\n",
      "         1.0050, 1.0161, 1.0173, 1.0010, 1.0004, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0058, 1.0007, 1.0018, 1.0068, 1.0016, 1.0001, 1.0029, 1.0027,\n",
      "         1.0038, 1.0040, 1.0007, 1.0020, 1.0007, 1.0045, 1.0011, 1.0094, 1.0018,\n",
      "         1.0015, 1.0023, 1.0010, 1.0053, 1.0030, 1.0008, 1.0005, 1.0007, 1.0020,\n",
      "         1.0005, 1.0011, 1.0012, 1.0030, 1.0146, 1.0011, 1.0060]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0028, 1.0108, 1.0023, 1.0192, 1.0091, 1.0093, 1.0071, 1.0029,\n",
      "         1.0045, 1.0009, 1.0020, 1.0039, 1.0009, 1.0681, 1.0029, 1.0012, 1.0006,\n",
      "         1.0029, 1.0017, 1.0043, 1.0026, 1.0000, 1.0097, 1.0173],\n",
      "        [0.0000, 1.0047, 1.1044, 1.0444, 1.0052, 1.0045, 1.0074, 1.0438, 1.0239,\n",
      "         1.0038, 1.0012, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 25, 25])  attentions_grads shape: torch.Size([3, 2, 12, 25, 25])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 25, 25])\n",
      "joint_attentions shape: torch.Size([2, 25, 25])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0012, 1.0118, 1.0092, 1.0038, 1.0271, 1.0055, 1.0022, 1.0015,\n",
      "         1.0360, 1.0026, 1.0010, 1.0081, 1.0145, 1.0103, 1.0011, 1.0016, 1.0102,\n",
      "         1.0002, 1.0154, 1.0022, 1.0105, 1.0279, 1.0017, 1.0004],\n",
      "        [0.0000, 1.0039, 1.0413, 1.0540, 1.0144, 1.0759, 1.0130, 1.0144, 1.0135,\n",
      "         1.0121, 1.0023, 1.0041, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 39, 39])  attentions_grads shape: torch.Size([3, 2, 12, 39, 39])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 39, 39])\n",
      "joint_attentions shape: torch.Size([2, 39, 39])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0019, 1.0152, 1.0015, 1.0018, 1.0078, 1.0016, 1.0000, 1.0014,\n",
      "         1.0026, 1.0015, 1.0051, 1.0243, 1.0025, 1.0150, 1.0010, 1.0007, 1.0047,\n",
      "         1.0029, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0064, 1.0163, 1.0026, 1.0063, 1.0008, 1.0015, 1.0157, 1.0019,\n",
      "         1.0006, 1.0039, 1.0004, 1.0011, 1.0014, 1.0046, 1.0007, 1.0222, 1.0016,\n",
      "         1.0004, 1.0008, 1.0294, 1.0073, 1.0018, 1.0001, 1.0058, 1.0024, 1.0058,\n",
      "         1.0011, 1.0066, 1.0005, 1.0008, 1.0269, 1.0051, 1.0007, 1.0134, 1.0007,\n",
      "         1.0007, 1.0014, 1.0006]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0035, 1.0014, 1.0010, 1.0242, 1.0005, 1.0011, 1.0131, 1.0028,\n",
      "         1.0061, 1.0018, 1.0037, 1.0547, 1.0006, 1.0014, 1.0029, 1.0358, 1.0070,\n",
      "         1.0077, 1.0043, 1.0181, 1.0128, 1.0007, 1.0042, 1.0132, 1.0019, 1.0005,\n",
      "         1.0046, 1.0049, 1.0020, 1.0004],\n",
      "        [0.0000, 1.0189, 1.0203, 1.0118, 1.0031, 1.0130, 1.0203, 1.0072, 1.0021,\n",
      "         1.0239, 1.0058, 1.0111, 1.0016, 1.0316, 1.0047, 1.0014, 1.0033, 1.0888,\n",
      "         1.0018, 1.0011, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0115, 1.0081, 1.0161, 1.0069, 1.0580, 1.0084, 1.0245, 1.0205,\n",
      "         1.0356, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0056, 1.0038, 1.0038, 1.0310, 1.0223, 1.0098, 1.0085, 1.0179,\n",
      "         1.0021, 1.0046, 1.0018, 1.0234, 1.0059, 1.0130, 1.0491, 1.0014, 1.0008]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0030, 1.0046, 1.0025, 1.0040, 1.0171, 1.0010, 1.0047, 1.0036,\n",
      "         1.0033, 1.0023, 1.0047, 1.0109, 1.0047, 1.0034, 1.0032, 1.0074, 1.0035,\n",
      "         1.0025, 1.0044, 1.0025, 1.0005, 1.0114, 1.0004, 1.0097, 1.0071, 1.0386,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0291, 1.0007, 1.0083, 1.0211, 1.0020, 1.0008, 1.0045, 1.0011,\n",
      "         1.0277, 1.0047, 1.0251, 1.0007, 1.0100, 1.0053, 1.0146, 1.0008, 1.0025,\n",
      "         1.0170, 1.0112, 1.0095, 1.0033, 1.0028, 1.0064, 1.0378, 1.0174, 1.0004,\n",
      "         1.0015, 1.0015, 1.0051, 1.0006, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0313, 1.0966, 1.0027, 1.0027, 1.0028, 1.0143, 1.0025,\n",
      "         1.0160, 1.0096, 1.0213, 1.0030, 1.0038, 1.0065, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0137, 1.0130, 1.0021, 1.0014, 1.0067, 1.0035, 1.0026, 1.0038,\n",
      "         1.0061, 1.0315, 1.0016, 1.0083, 1.0002, 1.0022, 1.0144, 1.0030, 1.0009,\n",
      "         1.0011, 1.0055, 1.0056, 1.0009, 1.0010, 1.0005, 1.0012, 1.0023, 1.0004,\n",
      "         1.0004, 1.0016, 1.0022, 1.0174]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 43, 43])  attentions_grads shape: torch.Size([3, 2, 12, 43, 43])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 43, 43])\n",
      "joint_attentions shape: torch.Size([2, 43, 43])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0044, 1.0015, 1.0016, 1.0035, 1.0010, 1.0054, 1.0009, 1.0005,\n",
      "         1.0107, 1.0066, 1.0014, 1.0011, 1.0023, 1.0015, 1.0013, 1.0346, 1.0027,\n",
      "         1.0008, 1.0006, 1.0039, 1.0005, 1.0134, 1.0017, 1.0007, 1.0016, 1.0021,\n",
      "         1.0112, 1.0007, 1.0022, 1.0019, 1.0057, 1.0004, 1.0029, 1.0005, 1.0009,\n",
      "         1.0021, 1.0041, 1.0005, 1.0022, 1.0002, 1.0043, 1.0271],\n",
      "        [0.0000, 1.0162, 1.0186, 1.0134, 1.0161, 1.0154, 1.0043, 1.0375, 1.0230,\n",
      "         1.0251, 1.0208, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0071, 1.0229, 1.0024, 1.0010, 1.0092, 1.0015, 1.0044, 1.0057,\n",
      "         1.0156, 1.0018, 1.0206, 1.0227, 1.0039, 1.0071, 1.0108, 1.0014, 1.0011,\n",
      "         1.0391, 1.0016, 1.0015, 1.0057, 1.0008, 1.0024, 1.0022, 1.0028, 1.0081,\n",
      "         1.0109, 1.0120, 1.0025, 1.0009, 1.0003],\n",
      "        [0.0000, 1.0091, 1.0054, 1.0035, 1.0134, 1.0243, 1.0252, 1.0054, 1.0267,\n",
      "         1.0278, 1.0039, 1.0214, 1.0198, 1.0090, 1.0014, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 21, 21])  attentions_grads shape: torch.Size([3, 2, 12, 21, 21])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 21, 21])\n",
      "joint_attentions shape: torch.Size([2, 21, 21])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0004, 1.0081, 1.0063, 1.0038, 1.0077, 1.0068, 1.0019, 1.0017,\n",
      "         1.0261, 1.0037, 1.0161, 1.0015, 1.0094, 1.0040, 1.0066, 1.0015, 1.0099,\n",
      "         1.0042, 1.0058, 1.0000],\n",
      "        [0.0000, 1.0043, 1.0018, 1.0228, 1.0067, 1.0071, 1.0158, 1.0033, 1.0010,\n",
      "         1.0197, 1.0151, 1.0021, 1.0022, 1.0047, 1.0039, 1.0010, 1.0045, 1.0044,\n",
      "         1.0005, 1.0048, 1.0208]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0069, 1.0134, 1.0113, 1.0271, 1.0044, 1.0233, 1.0087, 1.0167,\n",
      "         1.0110, 1.0318, 1.0200, 1.0023, 1.0011, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0010, 1.0206, 1.0018, 1.0028, 1.0054, 1.0076, 1.0050, 1.0013,\n",
      "         1.0030, 1.0058, 1.0138, 1.0093, 1.0009, 1.0058, 1.0138, 1.0030, 1.0008,\n",
      "         1.0037, 1.0013, 1.0026, 1.0006, 1.0183, 1.0055, 1.0055, 1.0133, 1.0079,\n",
      "         1.0033, 1.0014, 1.0181, 1.0010, 1.0004]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 27, 27])  attentions_grads shape: torch.Size([3, 2, 12, 27, 27])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 27, 27])\n",
      "joint_attentions shape: torch.Size([2, 27, 27])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0086, 1.0026, 1.0045, 1.0450, 1.0212, 1.0088, 1.0001, 1.0007,\n",
      "         1.0088, 1.0011, 1.0036, 1.0043, 1.0003, 1.0097, 1.0012, 1.0010, 1.0289,\n",
      "         1.0111, 1.0083, 1.0045, 1.0009, 1.0013, 1.0001, 1.0118, 1.0038, 1.0043],\n",
      "        [0.0000, 1.0116, 1.0103, 1.0082, 1.0261, 1.0515, 1.0272, 1.0248, 1.0038,\n",
      "         1.0099, 1.0575, 1.0030, 1.0019, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 20, 20])  attentions_grads shape: torch.Size([3, 2, 12, 20, 20])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 20, 20])\n",
      "joint_attentions shape: torch.Size([2, 20, 20])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0025, 1.0114, 1.0253, 1.0344, 1.0010, 1.0154, 1.0006, 1.0008,\n",
      "         1.0003, 1.0000, 1.0018, 1.0002, 1.0016, 1.0143, 1.0122, 1.0023, 1.0023,\n",
      "         1.0038, 1.0051],\n",
      "        [0.0000, 1.0099, 1.0012, 1.0050, 1.0525, 1.0022, 1.0058, 1.0098, 1.0037,\n",
      "         1.0058, 1.0021, 1.0022, 1.0256, 1.0102, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 30, 30])  attentions_grads shape: torch.Size([3, 2, 12, 30, 30])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 30, 30])\n",
      "joint_attentions shape: torch.Size([2, 30, 30])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0018, 1.0013, 1.0032, 1.0014, 1.0488, 1.0059, 1.0025, 1.0019,\n",
      "         1.0026, 1.0031, 1.0041, 1.0027, 1.0008, 1.0023, 1.0010, 1.0039, 1.0007,\n",
      "         1.0002, 1.0072, 1.0012, 1.0013, 1.0007, 1.0024, 1.0019, 1.0017, 1.0010,\n",
      "         1.0030, 1.0048, 1.0234],\n",
      "        [0.0000, 1.0204, 1.0015, 1.0015, 1.0018, 1.0145, 1.0056, 1.0019, 1.0216,\n",
      "         1.0155, 1.0095, 1.0019, 1.0194, 1.0581, 1.0068, 1.0014, 1.0086, 1.0112,\n",
      "         1.0041, 1.0041, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 17, 17])  attentions_grads shape: torch.Size([3, 2, 12, 17, 17])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 17, 17])\n",
      "joint_attentions shape: torch.Size([2, 17, 17])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0013, 1.0214, 1.0146, 1.0028, 1.0082, 1.0335, 1.0278, 1.0026,\n",
      "         1.0177, 1.0032, 1.0057, 1.0113, 1.0009, 1.0026, 1.0026, 1.0053],\n",
      "        [0.0000, 1.0160, 1.0127, 1.0384, 1.0112, 1.0017, 1.0307, 1.0525, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 33, 33])  attentions_grads shape: torch.Size([3, 2, 12, 33, 33])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 33, 33])\n",
      "joint_attentions shape: torch.Size([2, 33, 33])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0044, 1.0078, 1.0190, 1.0035, 1.0023, 1.0018, 1.0063, 1.0009,\n",
      "         1.0043, 1.0120, 1.0008, 1.0036, 1.0030, 1.0006, 1.0021, 1.0010, 1.0009,\n",
      "         1.0353, 1.0058, 1.0063, 1.0015, 1.0020, 1.0006, 1.0094, 1.0009, 1.0031,\n",
      "         1.0022, 1.0305, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0072, 1.0046, 1.0001, 1.0048, 1.0024, 1.0004, 1.0002, 1.0006,\n",
      "         1.0008, 1.0023, 1.0537, 1.0006, 1.0011, 1.0002, 1.0015, 1.0039, 1.0038,\n",
      "         1.0029, 1.0005, 1.0476, 1.0005, 1.0022, 1.0002, 1.0016, 1.0012, 1.0022,\n",
      "         1.0029, 1.0007, 1.0002, 1.0009, 1.0141, 1.0136]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0016, 1.0247, 1.0197, 1.0030, 1.0084, 1.0273, 1.0076, 1.0052,\n",
      "         1.0120, 1.0076, 1.0261, 1.0110, 1.0323, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0007, 1.0013, 1.0009, 1.0007, 1.0011, 1.0024, 1.0113, 1.0010,\n",
      "         1.0015, 1.0009, 1.0036, 1.0012, 1.0021, 1.0036, 1.0007, 1.0003, 1.0017,\n",
      "         1.0018, 1.0016, 1.0004, 1.0045, 1.0015, 1.0009, 1.0077, 1.0015, 1.0025,\n",
      "         1.0008, 1.0007, 1.0061, 1.0027, 1.0082, 1.0023, 1.0059, 1.0031, 1.0009,\n",
      "         1.0024, 1.0003, 1.0004, 1.0001]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 32, 32])  attentions_grads shape: torch.Size([3, 2, 12, 32, 32])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 32, 32])\n",
      "joint_attentions shape: torch.Size([2, 32, 32])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0018, 1.0423, 1.0284, 1.0034, 1.0239, 1.0025, 1.0141, 1.0293,\n",
      "         1.0481, 1.0027, 1.0038, 1.0039, 1.0023, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0171, 1.0024, 1.0035, 1.0024, 1.0011, 1.0034, 1.0073, 1.0015,\n",
      "         1.0070, 1.0022, 1.0076, 1.0063, 1.0130, 1.0017, 1.0136, 1.0187, 1.0063,\n",
      "         1.0048, 1.0176, 1.0047, 1.0254, 1.0040, 1.0143, 1.0085, 1.0049, 1.0010,\n",
      "         1.0101, 1.0068, 1.0058, 1.0008, 1.0003]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 36, 36])  attentions_grads shape: torch.Size([3, 2, 12, 36, 36])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 36, 36])\n",
      "joint_attentions shape: torch.Size([2, 36, 36])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0163, 1.0018, 1.0162, 1.0017, 1.0196, 1.0022, 1.0100, 1.0038,\n",
      "         1.0051, 1.0090, 1.0121, 1.0011, 1.0068, 1.0018, 1.0137, 1.0008, 1.0064,\n",
      "         1.0033, 1.0010, 1.0019, 1.0030, 1.0019, 1.0143, 1.0467, 1.0018, 1.0054,\n",
      "         1.0253, 1.0272, 1.0023, 1.0073, 1.0010, 1.0005, 1.0008, 1.0005, 1.0002],\n",
      "        [0.0000, 1.0047, 1.0209, 1.0035, 1.0038, 1.0019, 1.0006, 1.0015, 1.0004,\n",
      "         1.0077, 1.0013, 1.0017, 1.0031, 1.0013, 1.0017, 1.0002, 1.0021, 1.0030,\n",
      "         1.0044, 1.0146, 1.0058, 1.0012, 1.0024, 1.0316, 1.0027, 1.0019, 1.0039,\n",
      "         1.0017, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 19, 19])  attentions_grads shape: torch.Size([3, 2, 12, 19, 19])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 19, 19])\n",
      "joint_attentions shape: torch.Size([2, 19, 19])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0065, 1.0721, 1.0186, 1.0081, 1.0261, 1.0308, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000],\n",
      "        [0.0000, 1.0008, 1.0160, 1.0056, 1.0037, 1.0011, 1.0018, 1.0085, 1.0135,\n",
      "         1.0056, 1.0178, 1.0020, 1.0065, 1.0223, 1.0386, 1.0215, 1.0055, 1.0016,\n",
      "         1.0006]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 35, 35])  attentions_grads shape: torch.Size([3, 2, 12, 35, 35])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 35, 35])\n",
      "joint_attentions shape: torch.Size([2, 35, 35])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0081, 1.0096, 1.0014, 1.0016, 1.0038, 1.0493, 1.0031, 1.0036,\n",
      "         1.0014, 1.0074, 1.0018, 1.0047, 1.0008, 1.0025, 1.0011, 1.0025, 1.0010,\n",
      "         1.0016, 1.0026, 1.0566, 1.0014, 1.0005, 1.0000, 1.0001, 1.0006, 1.0020,\n",
      "         1.0015, 1.0036, 1.0012, 1.0010, 1.0014, 1.0283, 1.0041, 1.0075],\n",
      "        [0.0000, 1.0045, 1.0018, 1.0022, 1.0553, 1.0017, 1.0360, 1.0010, 1.0167,\n",
      "         1.0245, 1.0063, 1.0227, 1.0197, 1.0106, 1.0065, 1.0112, 1.0062, 1.0016,\n",
      "         1.0028, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 41, 41])  attentions_grads shape: torch.Size([3, 2, 12, 41, 41])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 41, 41])\n",
      "joint_attentions shape: torch.Size([2, 41, 41])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0022, 1.0022, 1.0134, 1.0021, 1.0037, 1.0047, 1.0204, 1.0044,\n",
      "         1.0013, 1.0379, 1.0012, 1.0788, 1.0080, 1.0030, 1.0228, 1.0166, 1.0014,\n",
      "         1.0042, 1.0019, 1.0044, 1.0036, 1.0027, 1.0011, 1.0006, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0102, 1.0253, 1.0017, 1.0026, 1.0032, 1.0131, 1.0086, 1.0151,\n",
      "         1.0028, 1.0015, 1.0279, 1.0021, 1.0028, 1.0067, 1.0033, 1.0006, 1.0007,\n",
      "         1.0131, 1.0009, 1.0056, 1.0096, 1.0004, 1.0093, 1.0008, 1.0019, 1.0167,\n",
      "         1.0017, 1.0007, 1.0174, 1.0005, 1.0011, 1.0053, 1.0096, 1.0020, 1.0006,\n",
      "         1.0056, 1.0044, 1.0015, 1.0005, 1.0002]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 40, 40])  attentions_grads shape: torch.Size([3, 2, 12, 40, 40])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 40, 40])\n",
      "joint_attentions shape: torch.Size([2, 40, 40])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0017, 1.0017, 1.0011, 1.0007, 1.0287, 1.0023, 1.0060, 1.0022,\n",
      "         1.0022, 1.0038, 1.0145, 1.0166, 1.0027, 1.0044, 1.0083, 1.0455, 1.0016,\n",
      "         1.0005, 1.0006, 1.0054, 1.0592, 1.0017, 1.0003, 1.0138, 1.0133, 1.0005,\n",
      "         1.0015, 1.0026, 1.0021, 1.0009, 1.0007, 1.0005, 1.0015, 1.0019, 1.0066,\n",
      "         1.0016, 1.0085, 1.0006, 1.0003],\n",
      "        [0.0000, 1.0044, 1.0190, 1.0033, 1.0012, 1.0020, 1.0015, 1.0129, 1.0119,\n",
      "         1.0111, 1.0091, 1.0160, 1.0072, 1.0009, 1.0014, 1.0009, 1.0706, 1.0058,\n",
      "         1.0015, 1.0014, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 49, 49])  attentions_grads shape: torch.Size([3, 2, 12, 49, 49])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 49, 49])\n",
      "joint_attentions shape: torch.Size([2, 49, 49])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0245, 1.0086, 1.0073, 1.0015, 1.0031, 1.0078, 1.0011, 1.0044,\n",
      "         1.0026, 1.0044, 1.0016, 1.0112, 1.0005, 1.0005, 1.0069, 1.0148, 1.0013,\n",
      "         1.0038, 1.0012, 1.0010, 1.0002, 1.0015, 1.0036, 1.0021, 1.0029, 1.0071,\n",
      "         1.0005, 1.0009, 1.0052, 1.0007, 1.0071, 1.0087, 1.0018, 1.0066, 1.0008,\n",
      "         1.0021, 1.0044, 1.0009, 1.0009, 1.0004, 1.0392, 1.0059, 1.0005, 1.0019,\n",
      "         1.0001, 1.0032, 1.0004, 1.0002],\n",
      "        [0.0000, 1.0174, 1.0006, 1.0012, 1.0050, 1.0216, 1.0053, 1.0062, 1.0134,\n",
      "         1.0024, 1.0217, 1.0012, 1.0016, 1.0004, 1.0170, 1.0004, 1.0081, 1.0062,\n",
      "         1.0082, 1.0032, 1.0017, 1.0079, 1.0022, 1.0032, 1.0116, 1.0205, 1.0049,\n",
      "         1.0104, 1.0025, 1.0024, 1.0047, 1.0009, 1.0003, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 52, 52])  attentions_grads shape: torch.Size([3, 2, 12, 52, 52])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 52, 52])\n",
      "joint_attentions shape: torch.Size([2, 52, 52])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0021, 1.0013, 1.0032, 1.0048, 1.0077, 1.0076, 1.0041, 1.0014,\n",
      "         1.0012, 1.0014, 1.0010, 1.0014, 1.0010, 1.0009, 1.0005, 1.0004, 1.0014,\n",
      "         1.0008, 1.0011, 1.0005, 1.0013, 1.0004, 1.0002, 1.0004, 1.0013, 1.0062,\n",
      "         1.0004, 1.0156, 1.0018, 1.0028, 1.0022, 1.0013, 1.0045, 1.0001, 1.0007,\n",
      "         1.0014, 1.0000, 1.0001, 1.0014, 1.0003, 1.0009, 1.0013, 1.0038, 1.0101,\n",
      "         1.0019, 1.0046, 1.0007, 1.0004, 1.0217, 1.0021, 1.0014],\n",
      "        [0.0000, 1.0035, 1.0014, 1.0020, 1.0025, 1.0000, 1.0005, 1.0018, 1.0000,\n",
      "         1.0008, 1.0001, 1.0055, 1.0014, 1.0008, 1.0007, 1.0026, 1.0054, 1.0111,\n",
      "         1.0132, 1.0013, 1.0031, 1.0016, 1.0013, 1.0011, 1.0011, 1.0007, 1.0004,\n",
      "         1.0004, 1.0043, 1.0012, 1.0049, 1.0011, 1.0000, 1.0000, 1.0035, 1.0011,\n",
      "         1.0026, 1.0009, 1.0000, 1.0011, 1.0037, 1.0045, 1.0006, 1.0020, 1.0010,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 24, 24])  attentions_grads shape: torch.Size([3, 2, 12, 24, 24])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 24, 24])\n",
      "joint_attentions shape: torch.Size([2, 24, 24])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0310, 1.0058, 1.0178, 1.0046, 1.0117, 1.0433, 1.0117, 1.0035,\n",
      "         1.0100, 1.0338, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0171, 1.0063, 1.0152, 1.0079, 1.0015, 1.0019, 1.0272, 1.0014,\n",
      "         1.0049, 1.0033, 1.0228, 1.0059, 1.0019, 1.0074, 1.0049, 1.0155, 1.0091,\n",
      "         1.0112, 1.0016, 1.0071, 1.0470, 1.0022, 1.0005]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 34, 34])  attentions_grads shape: torch.Size([3, 2, 12, 34, 34])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 34, 34])\n",
      "joint_attentions shape: torch.Size([2, 34, 34])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0031, 1.0074, 1.0058, 1.0032, 1.0035, 1.0268, 1.0011, 1.0129,\n",
      "         1.0155, 1.0087, 1.0006, 1.0100, 1.0108, 1.0010, 1.0233, 1.0005, 1.0022,\n",
      "         1.0601, 1.0150, 1.0010, 1.0021, 1.0007, 1.0060, 1.0069, 1.0036, 1.0030,\n",
      "         1.0099, 1.0042, 1.0014, 1.0028, 1.0047, 1.0007, 1.0002],\n",
      "        [0.0000, 1.0078, 1.0300, 1.0062, 1.0021, 1.0057, 1.0077, 1.0134, 1.0038,\n",
      "         1.0157, 1.0194, 1.0424, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 18, 18])  attentions_grads shape: torch.Size([3, 2, 12, 18, 18])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 18, 18])\n",
      "joint_attentions shape: torch.Size([2, 18, 18])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0078, 1.0134, 1.0056, 1.0043, 1.0353, 1.0117, 1.0572, 1.0485,\n",
      "         1.0292, 1.0107, 1.0018, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0011, 1.0159, 1.0294, 1.0010, 1.0145, 1.0046, 1.0071, 1.0010,\n",
      "         1.0004, 1.0012, 1.0125, 1.0007, 1.0000, 1.0136, 1.0620, 1.0042, 1.0017]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "attentions_mat shape: torch.Size([3, 2, 12, 31, 31])  attentions_grads shape: torch.Size([3, 2, 12, 31, 31])\n",
      "attentions_mat after mean over heads shape: torch.Size([3, 2, 31, 31])\n",
      "joint_attentions shape: torch.Size([2, 31, 31])\n",
      "explanation:\n",
      "tensor([[0.0000, 1.0169, 1.0086, 1.0097, 1.0037, 1.0012, 1.0036, 1.0061, 1.0085,\n",
      "         1.0383, 1.0033, 1.0101, 1.0017, 1.0024, 1.0021, 1.0009, 1.0050, 1.0052,\n",
      "         1.0249, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "         1.0000, 1.0000, 1.0000, 1.0000],\n",
      "        [0.0000, 1.0069, 1.0043, 1.0022, 1.0130, 1.0084, 1.0050, 1.0079, 1.0164,\n",
      "         1.0011, 1.0122, 1.0051, 1.0008, 1.0010, 1.0225, 1.0185, 1.0015, 1.0040,\n",
      "         1.0042, 1.0050, 1.0010, 1.0116, 1.0014, 1.0041, 1.0161, 1.0031, 1.0387,\n",
      "         1.0064, 1.0005, 1.0007, 1.0008]], grad_fn=<SumBackward1>)\n",
      "Validation set results: {'accuracy': 0.8130733944954128, 'rank_loss': 0.47742805887844797, 'base_loss': 0.20067952573299408, 'expl_mse_micro': 0.9349995749436673, 'expl_mse_macro': 0.9196119329661404}\n",
      "Saved val attributions to /home/tromanski/thesis/results/results_val_bert_sst2_eval_only_121958.json\n"
     ]
    }
   ],
   "source": [
    "!jobs/single_attack.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
